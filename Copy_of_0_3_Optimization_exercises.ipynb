{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moribots/tensors/blob/main/Copy_of_0_3_Optimization_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB06BSLwIT05"
      },
      "source": [
        "# [0.3] Optimization & Hyperparameters (exercises)\n",
        "\n",
        "> **ARENA [Streamlit Page](https://arena-chapter0-fundamentals.streamlit.app/03_[0.3]_Optimization)**\n",
        ">\n",
        "> **Colab: [exercises](https://colab.research.google.com/github/callummcdougall/ARENA_3.0/blob/main/chapter0_fundamentals/exercises/part3_optimization/0.3_Optimization_exercises.ipynb?t=20250126) | [solutions](https://colab.research.google.com/github/callummcdougall/ARENA_3.0/blob/main/chapter0_fundamentals/exercises/part3_optimization/0.3_Optimization_solutions.ipynb?t=20250126)**\n",
        "\n",
        "Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-uk/shared_invite/zt-2noug8mpy-TRYbCnc3pzj7ITNrZIjKww), and ask any questions on the dedicated channels for this chapter of material.\n",
        "\n",
        "You can collapse each section so only the headers are visible, by clicking the arrow symbol on the left hand side of the markdown header cells.\n",
        "\n",
        "Links to all other chapters: [(0) Fundamentals](https://arena-chapter0-fundamentals.streamlit.app/), [(1) Transformer Interpretability](https://arena-chapter1-transformer-interp.streamlit.app/), [(2) RL](https://arena-chapter2-rl.streamlit.app/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ttPnkhyIT0-"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-03.png\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E8aD9fbIT1A"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUfCeXx7IT1A"
      },
      "source": [
        "In today's exercises, we will explore various optimization algorithms and their roles in training deep learning models. We will delve into the inner workings of different optimization techniques such as Stochastic Gradient Descent (SGD), RMSprop, and Adam, and learn how to implement them using code. Additionally, we will discuss the concept of loss landscapes and their significance in visualizing the challenges faced during the optimization process. By the end of this set of exercises, you will have a solid understanding of optimization algorithms and their impact on model performance. We'll also take a look at Weights and Biases, a tool that can be used to track and visualize the training process, and test different values of hyperparameters to find the most effective ones.\n",
        "\n",
        "> Note - the third set of exercises in this section are on distributed training, and have different requirements: specifically, you'll need to SSH into a virtual machine which has multiple GPUs, and run the exercises from a Python file (not notebook or Colab). However you can still treat the first 2 sections as normal and then make this switch for the third section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbhSsCf9IT1B"
      },
      "source": [
        "## Content & Learning Objectives\n",
        "\n",
        "### 1️⃣ Optimizers\n",
        "\n",
        "These exercises will take you through how different optimization algorithms work (specifically SGD, RMSprop and Adam). You'll write your own optimisers, and use plotting functions to visualise gradient descent on loss landscapes.\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Understand how different optimization algorithms work\n",
        "> * Translate pseudocode for these algorithms into code\n",
        "> * Understand the idea of loss landscapes, and how they can visualize specific challenges in the optimization process\n",
        "\n",
        "### 2️⃣ Weights and Biases\n",
        "\n",
        "In this section, we'll look at methods for choosing hyperparameters effectively. You'll learn how to use **Weights and Biases**, a useful tool for hyperparameter search. By the end of today, you should be able to use Weights and Biases to train the ResNet you created in the last set of exercises.\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Write modular, extensible code for training models\n",
        "> * Learn what the most important hyperparameters are, and methods for efficiently searching over hyperparameter space\n",
        "> * Learn how to use Weights & Biases for logging your runs\n",
        "> * Adapt your code from yesterday to log training runs to Weights & Biases, and use this service to run **hyperparameter sweeps**\n",
        "\n",
        "### 3️⃣ Distributed Training\n",
        "\n",
        "In this section, we'll take you through the basics of distributed training, which is the process via which training is split over multiple separate GPUs to improve efficiency and capacity.\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Understand the different kinds of parallelization used in deep learning (data, pipeline, tensor)\n",
        "> * Understand how primitive operations in `torch.distributed` work, and how they come together to enable distributed training\n",
        "> * Launch and benchmark your own distributed training runs, to train your implementation of `ResNet34` from scratch\n",
        "\n",
        "### 4️⃣ Bonus\n",
        "\n",
        "This section gives you suggestions for further exploration of optimizers, and Weights & Biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRVEanikIT1E"
      },
      "source": [
        "## Setup code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "okIctbc2IT1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7baaf537-ef53-4f1d-e6e7-ebddf0f6a9eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Collecting jaxtyping\n",
            "  Downloading jaxtyping-0.2.37-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.5)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping)\n",
            "  Downloading wadler_lindig-0.1.3-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.20.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading jaxtyping-0.2.37-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading wadler_lindig-0.1.3-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, torchinfo, jaxtyping\n",
            "Successfully installed jaxtyping-0.2.37 torchinfo-1.8.0 wadler-lindig-0.1.3\n",
            "--2025-02-03 04:46:44--  https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/main.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/callummcdougall/ARENA_3.0/zip/refs/heads/main [following]\n",
            "--2025-02-03 04:46:44--  https://codeload.github.com/callummcdougall/ARENA_3.0/zip/refs/heads/main\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.114.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.114.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21977541 (21M) [application/zip]\n",
            "Saving to: ‘/content/main.zip’\n",
            "\n",
            "main.zip            100%[===================>]  20.96M  39.7MB/s    in 0.5s    \n",
            "\n",
            "2025-02-03 04:46:44 (39.7 MB/s) - ‘/content/main.zip’ saved [21977541/21977541]\n",
            "\n",
            "Archive:  /content/main.zip\n",
            "b6e030f1358727802a42489579a8449f4cc9e935\n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/\n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part0_prereqs/\n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part0_prereqs/0.0_Prerequisites_exercises.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part0_prereqs/0.0_Prerequisites_solutions.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part0_prereqs/numbers.npy  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part0_prereqs/solutions.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part0_prereqs/tests.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part0_prereqs/utils.py  \n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/\n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/0.1_Ray_Tracing_exercises.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/0.1_Ray_Tracing_solutions.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/pikachu.pt  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/pikachu.stl  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/solutions.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/test_with_pytest.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/tests.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/utils.py  \n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/\n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/0.2_CNNs_&_ResNets_exercises.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/0.2_CNNs_&_ResNets_solutions.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/imagenet_labels.json  \n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/\n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/astronaut.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/chimpanzee.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/dragonfly.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/fireworks.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/frogs.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/golden_retriever.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/goofy.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/hourglass.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/iguana.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/platypus.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/volcano.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/solutions.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/tests.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/utils.py  \n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part3_optimization/\n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part3_optimization/0.3_Optimization_exercises.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part3_optimization/0.3_Optimization_solutions.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part3_optimization/solutions.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part3_optimization/tests.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part3_optimization/utils.py  \n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part4_backprop/\n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part4_backprop/0.4_Backprop_exercises.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part4_backprop/0.4_Backprop_solutions.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part4_backprop/solutions.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part4_backprop/tests.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part4_backprop/utils.py  \n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part5_vaes_and_gans/\n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part5_vaes_and_gans/0.5_VAEs_&_GANs_exercises.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part5_vaes_and_gans/0.5_VAEs_&_GANs_solutions.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part5_vaes_and_gans/solutions.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part5_vaes_and_gans/tests.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part5_vaes_and_gans/utils.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/plotly_utils.py  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "chapter = \"chapter0_fundamentals\"\n",
        "repo = \"ARENA_3.0\"\n",
        "branch = \"main\"\n",
        "\n",
        "# Install dependencies\n",
        "try:\n",
        "    import jaxtyping\n",
        "except:\n",
        "    %pip install einops jaxtyping torchinfo wandb\n",
        "\n",
        "# Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo\n",
        "root = (\n",
        "    \"/content\"\n",
        "    if IN_COLAB\n",
        "    else \"/root\"\n",
        "    if repo not in os.getcwd()\n",
        "    else str(next(p for p in Path.cwd().parents if p.name == repo))\n",
        ")\n",
        "\n",
        "if Path(root).exists() and not Path(f\"{root}/{chapter}\").exists():\n",
        "    if not IN_COLAB:\n",
        "        !sudo apt-get install unzip\n",
        "        %pip install jupyter ipython --upgrade\n",
        "\n",
        "    if not os.path.exists(f\"{root}/{chapter}\"):\n",
        "        !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip\n",
        "        !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}\n",
        "        !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}\n",
        "        !rm {root}/{branch}.zip\n",
        "        !rmdir {root}/{repo}-{branch}\n",
        "\n",
        "\n",
        "assert Path(f\"{root}/{chapter}/exercises\").exists(), \"Unexpected error: please manually clone ARENA repo into `root`\"\n",
        "\n",
        "if f\"{root}/{chapter}/exercises\" not in sys.path:\n",
        "    sys.path.append(f\"{root}/{chapter}/exercises\")\n",
        "\n",
        "os.chdir(f\"{root}/{chapter}/exercises\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EpDSU7K9IT1H"
      },
      "outputs": [],
      "source": [
        "import importlib\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from dataclasses import dataclass, replace\n",
        "from pathlib import Path\n",
        "from typing import Callable, Iterable, Literal\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch as t\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "from IPython.core.display import HTML\n",
        "from IPython.display import display\n",
        "from jaxtyping import Float, Int\n",
        "from torch import Tensor, optim\n",
        "from torch.utils.data import DataLoader, DistributedSampler, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Make sure exercises are in the path\n",
        "chapter = \"chapter0_fundamentals\"\n",
        "section = \"part3_optimization\"\n",
        "root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())\n",
        "exercises_dir = root_dir / chapter / \"exercises\"\n",
        "section_dir = exercises_dir / section\n",
        "if str(exercises_dir) not in sys.path:\n",
        "    sys.path.append(str(exercises_dir))\n",
        "\n",
        "\n",
        "import part3_optimization.tests as tests\n",
        "from part2_cnns.solutions import Linear, ResNet34, get_resnet_for_feature_extraction\n",
        "from part3_optimization.utils import plot_fn, plot_fn_with_points\n",
        "from plotly_utils import bar, imshow, line\n",
        "\n",
        "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MAIN = __name__ == \"__main__\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGONHnKAIT1I"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I get a NumPy-related error</summary>\n",
        "\n",
        "This is an annoying colab-related issue which I haven't been able to find a satisfying fix for. If you restart runtime (but don't delete runtime), and run just the imports cell above again (but not the `%pip install` cell), the problem should go away.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL_TmZviIT1J"
      },
      "source": [
        "# 1️⃣ Optimizers\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Understand how different optimization algorithms work\n",
        "> * Translate pseudocode for these algorithms into code\n",
        "> * Understand the idea of loss landscapes, and how they can visualize specific challenges in the optimization process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCKzffm0IT1K"
      },
      "source": [
        "## Reading\n",
        "\n",
        "Some of these are strongly recommended, while others are optional. If you like, you can jump back to some of these videos while you're going through the material, if you feel like you need to.\n",
        "\n",
        "* Andrew Ng's video series on gradient descent variants: [Gradient Descent With Momentum](https://www.youtube.com/watch?v=k8fTYJPd3_I) (9 mins), [RMSProp](https://www.youtube.com/watch?v=_e-LFe_igno) (7 mins), [Adam](https://www.youtube.com/watch?v=JXQT_vxqwIs&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=23) (7 mins)\n",
        "    * These videos are strongly recommended, especially the RMSProp video\n",
        "* [A Visual Explanation of Gradient Descent Methods](https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c)\n",
        "    * This is also strongly recommended; if you only want to read/watch one thing, make it this\n",
        "* [Why Momentum Really Works (distill.pub)](https://distill.pub/2017/momentum/)\n",
        "    * This is optional, but a fascinating read if you have time and are interested in engaging with the mathematical details of optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCYuwQ6iIT1L"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "Tomorrow, we'll look in detail about how the backpropagation algorithm works. But for now, let's take it as read that calling `loss.backward()` on a scalar `loss` will result in the computation of the gradients $\\frac{\\partial loss}{\\partial w}$ for every parameter `w` in the model, and store these values in `w.grad`. How do we use these gradients to update our parameters in a way which decreases loss?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y08mhphtIT1L"
      },
      "source": [
        "A loss function can be any differentiable function such that we prefer a lower value. To apply gradient descent, we start by initializing the parameters to random values (the details of this are subtle), and then repeatedly compute the gradient of the loss with respect to the model parameters. It [can be proven](https://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx) that for an infinitesimal step, moving in the direction of the gradient would increase the loss by the largest amount out of all possible directions.\n",
        "\n",
        "We actually want to decrease the loss, so we subtract the gradient to go in the opposite direction. Taking infinitesimal steps is no good, so we pick some learning rate $\\lambda$ (also called the step size) and scale our step by that amount to obtain the update rule for gradient descent:\n",
        "\n",
        "$$\\theta_t \\leftarrow \\theta_{t-1} - \\lambda \\nabla L(\\theta_{t-1})$$\n",
        "\n",
        "We know that an infinitesimal step will decrease the loss, but a finite step will only do so if the loss function is linear enough in the neighbourhood of the current parameters. If the loss function is too curved, we might actually increase our loss.\n",
        "\n",
        "The biggest advantage of this algorithm is that for N bytes of parameters, you only need N additional bytes of memory to store the gradients, which are of the same shape as the parameters. GPU memory is very limited, so this is an extremely relevant consideration. The amount of computation needed is also minimal: one multiply and one add per parameter.\n",
        "\n",
        "The biggest disadvantage is that we're completely ignoring the curvature of the loss function, not captured by the gradient consisting of partial derivatives. Intuitively, we can take a larger step if the loss function is flat in some direction or a smaller step if it is very curved. Generally, you could represent this by some matrix P that pre-multiplies the gradients to rescale them to account for the curvature. $P$ is called a preconditioner, and gradient descent is equivalent to approximating $P$ by an identity matrix, which is a very bad approximation.\n",
        "\n",
        "Most competing optimizers can be interpreted as trying to do something more sensible for $P$, subject to the constraint that GPU memory is at a premium. In particular, constructing $P$ explicitly is infeasible, since it's an $N \\times N$ matrix and N can be hundreds of billions. One idea is to use a diagonal $P$, which only requires N additional memory. An example of a more sophisticated scheme is [Shampoo](https://arxiv.org/pdf/1802.09568.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW2rn447IT1M"
      },
      "source": [
        "> The algorithm is called **Shampoo** because you put shampoo on your hair before using conditioner, and this method is a pre-conditioner.\n",
        ">     \n",
        "> If you take away just one thing from this entire curriculum, please don't let it be this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCXgg-hvIT1N"
      },
      "source": [
        "## Stochastic Gradient Descent\n",
        "\n",
        "The terms gradient descent and SGD are used loosely in deep learning. To be technical, there are three variations:\n",
        "\n",
        "- Batch gradient descent - the loss function is the loss over the entire dataset. This requires too much computation unless the dataset is small, so it is rarely used in deep learning.\n",
        "- Stochastic gradient descent - the loss function is the loss on a randomly selected example. Any particular loss may be completely in the wrong direction of the loss on the entire dataset, but in expectation it's in the right direction. This has some nice properties but doesn't parallelize well, so it is rarely used in deep learning.\n",
        "- Mini-batch gradient descent - the loss function is the loss on a batch of examples of size `batch_size`. This is the standard in deep learning.\n",
        "\n",
        "The class `torch.optim.SGD` can be used for any of these by varying the number of examples passed in. We will be using only mini-batch gradient descent in this course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5moNtTs8IT1N"
      },
      "source": [
        "## Batch Size\n",
        "\n",
        "In addition to choosing a learning rate or learning rate schedule, we need to choose the batch size or batch size schedule as well. Intuitively, using a larger batch means that the estimate of the gradient is closer to that of the true gradient over the entire dataset, but this requires more compute. Each element of the batch can be computed in parallel so with sufficient compute, one can increase the batch size without increasing wall-clock time. For small-scale experiments, a good heuristic is thus \"fill up all of your GPU memory\".\n",
        "\n",
        "At a larger scale, we would expect diminishing returns of increasing the batch size, but empirically it's worse than that - a batch size that is too large generalizes more poorly in many scenarios. The intuition that a closer approximation to the true gradient is always better is therefore incorrect. See [this paper](https://arxiv.org/pdf/1706.02677.pdf) for one discussion of this.\n",
        "\n",
        "For a batch size schedule, most commonly you'll see batch sizes increase over the course of training. The intuition is that a rough estimate of the proper direction is good enough early in training, but later in training it's important to preserve our progress and not \"bounce around\" too much.\n",
        "\n",
        "You will commonly see batch sizes that are a multiple of 32. One motivation for this is that when using CUDA, threads are grouped into \"warps\" of 32 threads which execute the same instructions in parallel. So a batch size of 64 would allow two warps to be fully utilized, whereas a size of 65 would require waiting for a third warp to finish. As batch sizes become larger, this wastage becomes less important.\n",
        "\n",
        "Powers of two are also common - the idea here is that work can be recursively divided up among different GPUs or within a GPU. For example, a matrix multiplication can be expressed by recursively dividing each matrix into four equal blocks and performing eight smaller matrix multiplications between the blocks.\n",
        "\n",
        "In tomorrow's exercises, you'll have the option to expore batch sizes in more detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvPxVSq-IT1O"
      },
      "source": [
        "## Common Themes in Gradient-Based Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p_2TcnCIT1O"
      },
      "source": [
        "### Weight Decay\n",
        "\n",
        "Weight decay means that on each iteration, in addition to a regular step, we also shrink each parameter very slightly towards 0 by multiplying a scaling factor close to 1, e.g. 0.9999. Empirically, this seems to help but there are no proofs that apply to deep neural networks.\n",
        "\n",
        "In the case of linear regression, weight decay is mathematically equivalent to having a prior that each parameter is Gaussian distributed - in other words it's very unlikely that the true parameter values are very positive or very negative. This is an example of \"**inductive bias**\" - we make an assumption that helps us in the case where it's justified, and hurts us in the case where it's not justified.\n",
        "\n",
        "For a `Linear` layer, it's common practice to apply weight decay only to the weight and not the bias. It's also common to not apply weight decay to the parameters of a batch normalization layer. Again, there is empirical evidence (such as [Jai et al 2018](https://arxiv.org/pdf/1807.11205.pdf)) and there are heuristic arguments to justify these choices, but no rigorous proofs. Note that PyTorch will implement weight decay on the weights *and* biases of linear layers by default - see the bonus exercises tomorrow for more on this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWEEokSRIT1P"
      },
      "source": [
        "### Momentum\n",
        "\n",
        "Momentum means that the step includes a term proportional to a moving average of past gradients. [Distill.pub](https://distill.pub/2017/momentum/) has a great article on momentum, which you should definitely read if you have time. Don't worry if you don't understand all of it; skimming parts of it can be very informative. For instance, the first half discusses the **conditioning number** (a very important concept to understand in optimisation), and concludes by giving an intuitive argument for why we generally set the momentum parameter close to 1 for ill-conditioned problems (those with a very large conditioning number)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iq9U1esIT1Q"
      },
      "source": [
        "## Visualising optimization with pathological curvatures\n",
        "\n",
        "A pathological curvature is a type of surface that is similar to ravines and is particularly tricky for plain SGD optimization. In words, pathological curvatures typically have a steep gradient in one direction with an optimum at the center, while in a second direction we have a slower gradient towards a (global) optimum. Let’s first create an example surface of this and visualize it. The code below creates 2 visualizations (3D and 2D) and also adds the minimum point to the plot (note this is the min in the visible region, not the global minimum)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PEugGnYgIT1Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "f499cc96-ada2-4ea8-f877-8343ad364a34"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"c4ea8d13-cbd7-4747-9782-82f3271767e3\" class=\"plotly-graph-div\" style=\"height:450px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c4ea8d13-cbd7-4747-9782-82f3271767e3\")) {                    Plotly.newPlot(                        \"c4ea8d13-cbd7-4747-9782-82f3271767e3\",                        [{\"colorscale\":[[0.0,\"rgb(255,255,255)\"],[0.125,\"rgb(240,240,240)\"],[0.25,\"rgb(217,217,217)\"],[0.375,\"rgb(189,189,189)\"],[0.5,\"rgb(150,150,150)\"],[0.625,\"rgb(115,115,115)\"],[0.75,\"rgb(82,82,82)\"],[0.875,\"rgb(37,37,37)\"],[1.0,\"rgb(0,0,0)\"]],\"contours\":{\"x\":{\"color\":\"grey\",\"end\":6,\"show\":true,\"size\":0.2,\"start\":-6},\"y\":{\"color\":\"grey\",\"end\":6,\"show\":true,\"size\":0.2,\"start\":-10}},\"hovertemplate\":\"\\u003cb\\u003ex\\u003c\\u002fb\\u003e = %{x:.2f}\\u003cbr\\u003e\\u003cb\\u003ey\\u003c\\u002fb\\u003e = %{y:.2f}\\u003cbr\\u003e\\u003cb\\u003ez\\u003c\\u002fb\\u003e = %{z:.2f}\\u003c\\u002fb\\u003e\",\"showscale\":false,\"x\":[-6.0,-5.878788,-5.757576,-5.6363635,-5.5151515,-5.3939395,-5.272727,-5.151515,-5.030303,-4.909091,-4.787879,-4.6666665,-4.5454545,-4.4242425,-4.30303,-4.181818,-4.060606,-3.939394,-3.8181818,-3.6969695,-3.5757575,-3.4545455,-3.3333333,-3.212121,-3.090909,-2.969697,-2.8484848,-2.7272725,-2.6060605,-2.4848485,-2.3636363,-2.242424,-2.121212,-1.9999999,-1.8787878,-1.7575756,-1.6363635,-1.5151514,-1.3939393,-1.2727271,-1.151515,-1.0303029,-0.90909076,-0.78787863,-0.6666665,-0.5454544,-0.42424226,-0.30303013,-0.18181801,-0.060605884,0.060605884,0.18181801,0.30303013,0.42424226,0.5454544,0.6666665,0.78787863,0.90909076,1.0303029,1.151515,1.2727271,1.3939393,1.5151514,1.6363635,1.7575756,1.8787878,1.9999999,2.121212,2.242424,2.3636363,2.4848485,2.6060605,2.7272725,2.8484848,2.969697,3.090909,3.212121,3.3333333,3.4545455,3.5757575,3.6969695,3.8181818,3.939394,4.060606,4.181818,4.30303,4.4242425,4.5454545,4.6666665,4.787879,4.909091,5.030303,5.151515,5.272727,5.3939395,5.5151515,5.6363635,5.757576,5.878788,6.0],\"y\":[-10.0,-9.838384,-9.676767,-9.515152,-9.353536,-9.191919,-9.030303,-8.868687,-8.70707,-8.545455,-8.383839,-8.222222,-8.060606,-7.8989897,-7.737374,-7.5757575,-7.4141417,-7.2525253,-7.090909,-6.929293,-6.767677,-6.6060605,-6.4444447,-6.2828283,-6.121212,-5.959596,-5.79798,-5.6363635,-5.4747477,-5.3131313,-5.151515,-4.989899,-4.828283,-4.6666665,-4.5050507,-4.3434343,-4.181818,-4.020202,-3.8585858,-3.6969697,-3.5353537,-3.3737373,-3.2121212,-3.0505052,-2.8888888,-2.7272727,-2.5656567,-2.4040403,-2.2424242,-2.0808082,-1.919192,-1.7575758,-1.5959595,-1.4343435,-1.2727273,-1.1111112,-0.94949496,-0.78787875,-0.6262626,-0.46464646,-0.3030303,-0.14141414,0.020202026,0.18181819,0.34343433,0.50505054,0.6666667,0.82828283,0.989899,1.1515151,1.3131313,1.4747474,1.6363636,1.7979798,1.9595959,2.121212,2.2828283,2.4444444,2.6060605,2.7676768,2.929293,3.090909,3.2525253,3.4141414,3.5757575,3.7373738,3.89899,4.060606,4.2222223,4.383838,4.5454545,4.707071,4.8686867,5.030303,5.1919193,5.353535,5.5151515,5.676768,5.8383837,6.0],\"z\":[[1.0600208,1.0588019,1.0575812,1.0563581,1.055132,1.0539021,1.0526674,1.0514264,1.0501775,1.0489185,1.0476466,1.0463585,1.0450493,1.0437136,1.042344,1.0409312,1.0394635,1.0379257,1.0362988,1.0345583,1.0326732,1.0306042,1.0283011,1.0257006,1.0227219,1.0192631,1.0151955,1.0103563,1.0045413,0.99749476,0.98889786,0.97835594,0.9653843,0.9493946,0.9296824,0.90542156,0.8756685,0.83938664,0.79550195,0.74300283,0.6811024,0.60947055,0.5285384,0.43984336,0.34634793,0.25261393,0.1646867,0.0895606,0.034206238,0.0043155537,0.0043155537,0.034206238,0.0895606,0.1646867,0.25261393,0.34634793,0.43984336,0.5285384,0.60947055,0.6811024,0.74300283,0.79550195,0.83938664,0.8756685,0.90542156,0.9296824,0.9493946,0.9653843,0.97835594,0.98889786,0.99749476,1.0045413,1.0103563,1.0151955,1.0192631,1.0227219,1.0257006,1.0283011,1.0306042,1.0326732,1.0345583,1.0362988,1.0379257,1.0394635,1.0409312,1.042344,1.0437136,1.0450493,1.0463585,1.0476466,1.0489185,1.0501775,1.0514264,1.0526674,1.0539021,1.055132,1.0563581,1.0575812,1.0588019,1.0600208],[1.0600288,1.0588099,1.0575892,1.0563661,1.05514,1.0539101,1.0526754,1.0514344,1.0501854,1.0489265,1.0476546,1.0463665,1.0450573,1.0437216,1.042352,1.0409392,1.0394715,1.0379337,1.0363067,1.0345663,1.0326812,1.0306122,1.0283091,1.0257086,1.0227299,1.0192711,1.0152035,1.0103643,1.0045493,0.9975027,0.9889058,0.9783639,0.96539223,0.9494025,0.9296903,0.9054295,0.87567645,0.83939457,0.7955099,0.74301076,0.6811103,0.6094785,0.52854633,0.4398513,0.3463559,0.2526219,0.16469465,0.08956856,0.034214202,0.0043235165,0.0043235165,0.034214202,0.08956856,0.16469465,0.2526219,0.3463559,0.4398513,0.52854633,0.6094785,0.6811103,0.74301076,0.7955099,0.83939457,0.87567645,0.9054295,0.9296903,0.9494025,0.96539223,0.9783639,0.9889058,0.9975027,1.0045493,1.0103643,1.0152035,1.0192711,1.0227299,1.0257086,1.0283091,1.0306122,1.0326812,1.0345663,1.0363067,1.0379337,1.0394715,1.0409392,1.042352,1.0437216,1.0450573,1.0463665,1.0476546,1.0489265,1.0501854,1.0514344,1.0526754,1.0539101,1.05514,1.0563661,1.0575892,1.0588099,1.0600288],[1.0600381,1.0588192,1.0575985,1.0563754,1.0551493,1.0539194,1.0526847,1.0514437,1.0501947,1.0489358,1.0476639,1.0463758,1.0450666,1.0437309,1.0423613,1.0409485,1.0394808,1.037943,1.036316,1.0345756,1.0326905,1.0306215,1.0283184,1.0257179,1.0227392,1.0192804,1.0152128,1.0103736,1.0045586,0.99751204,0.98891515,0.9783732,0.9654016,0.94941187,0.92969966,0.90543884,0.8756858,0.8394039,0.79551923,0.7430201,0.6811197,0.60948783,0.5285557,0.4398607,0.34636527,0.25263128,0.16470401,0.08957792,0.03422356,0.004332876,0.004332876,0.03422356,0.08957792,0.16470401,0.25263128,0.34636527,0.4398607,0.5285557,0.60948783,0.6811197,0.7430201,0.79551923,0.8394039,0.8756858,0.90543884,0.92969966,0.94941187,0.9654016,0.9783732,0.98891515,0.99751204,1.0045586,1.0103736,1.0152128,1.0192804,1.0227392,1.0257179,1.0283184,1.0306215,1.0326905,1.0345756,1.036316,1.037943,1.0394808,1.0409485,1.0423613,1.0437309,1.0450666,1.0463758,1.0476639,1.0489358,1.0501947,1.0514437,1.0526847,1.0539194,1.0551493,1.0563754,1.0575985,1.0588192,1.0600381],[1.060049,1.0588301,1.0576094,1.0563864,1.0551603,1.0539304,1.0526956,1.0514547,1.0502057,1.0489467,1.0476749,1.0463867,1.0450776,1.0437418,1.0423722,1.0409595,1.0394918,1.037954,1.036327,1.0345865,1.0327015,1.0306325,1.0283294,1.0257288,1.0227501,1.0192914,1.0152237,1.0103846,1.0045695,0.99752307,0.9889262,0.97838426,0.9654126,0.9494229,0.9297107,0.90544987,0.87569684,0.83941495,0.79553026,0.74303114,0.6811307,0.60949886,0.5285667,0.4398717,0.34637627,0.25264227,0.164715,0.089588925,0.03423456,0.0043438766,0.0043438766,0.03423456,0.089588925,0.164715,0.25264227,0.34637627,0.4398717,0.5285667,0.60949886,0.6811307,0.74303114,0.79553026,0.83941495,0.87569684,0.90544987,0.9297107,0.9494229,0.9654126,0.97838426,0.9889262,0.99752307,1.0045695,1.0103846,1.0152237,1.0192914,1.0227501,1.0257288,1.0283294,1.0306325,1.0327015,1.0345865,1.036327,1.037954,1.0394918,1.0409595,1.0423722,1.0437418,1.0450776,1.0463867,1.0476749,1.0489467,1.0502057,1.0514547,1.0526956,1.0539304,1.0551603,1.0563864,1.0576094,1.0588301,1.060049],[1.060062,1.0588431,1.0576224,1.0563993,1.0551733,1.0539434,1.0527086,1.0514677,1.0502187,1.0489597,1.0476879,1.0463997,1.0450906,1.0437548,1.0423852,1.0409725,1.0395048,1.037967,1.03634,1.0345995,1.0327145,1.0306455,1.0283424,1.0257418,1.0227631,1.0193044,1.0152367,1.0103976,1.0045825,0.997536,0.9889391,0.9783972,0.96542555,0.94943583,0.9297236,0.9054628,0.8757098,0.8394279,0.7955432,0.7430441,0.68114364,0.6095118,0.52857965,0.43988463,0.3463892,0.2526552,0.16472794,0.08960185,0.03424749,0.004356807,0.004356807,0.03424749,0.08960185,0.16472794,0.2526552,0.3463892,0.43988463,0.52857965,0.6095118,0.68114364,0.7430441,0.7955432,0.8394279,0.8757098,0.9054628,0.9297236,0.94943583,0.96542555,0.9783972,0.9889391,0.997536,1.0045825,1.0103976,1.0152367,1.0193044,1.0227631,1.0257418,1.0283424,1.0306455,1.0327145,1.0345995,1.03634,1.037967,1.0395048,1.0409725,1.0423852,1.0437548,1.0450906,1.0463997,1.0476879,1.0489597,1.0502187,1.0514677,1.0527086,1.0539434,1.0551733,1.0563993,1.0576224,1.0588431,1.060062],[1.0600772,1.0588583,1.0576376,1.0564145,1.0551884,1.0539585,1.0527238,1.0514828,1.0502338,1.0489749,1.047703,1.0464149,1.0451057,1.04377,1.0424004,1.0409876,1.0395199,1.0379821,1.0363551,1.0346147,1.0327296,1.0306606,1.0283575,1.025757,1.0227783,1.0193195,1.0152519,1.0104127,1.0045977,0.9975512,0.9889543,0.9784124,0.96544075,0.949451,0.9297388,0.905478,0.875725,0.8394431,0.7955584,0.7430593,0.68115884,0.609527,0.52859485,0.4398998,0.34640437,0.25267038,0.16474314,0.08961705,0.03426269,0.004372005,0.004372005,0.03426269,0.08961705,0.16474314,0.25267038,0.34640437,0.4398998,0.52859485,0.609527,0.68115884,0.7430593,0.7955584,0.8394431,0.875725,0.905478,0.9297388,0.949451,0.96544075,0.9784124,0.9889543,0.9975512,1.0045977,1.0104127,1.0152519,1.0193195,1.0227783,1.025757,1.0283575,1.0306606,1.0327296,1.0346147,1.0363551,1.0379821,1.0395199,1.0409876,1.0424004,1.04377,1.0451057,1.0464149,1.047703,1.0489749,1.0502338,1.0514828,1.0527238,1.0539585,1.0551884,1.0564145,1.0576376,1.0588583,1.0600772],[1.0600951,1.0588762,1.0576555,1.0564324,1.0552063,1.0539764,1.0527416,1.0515007,1.0502517,1.0489928,1.0477209,1.0464327,1.0451236,1.0437878,1.0424182,1.0410055,1.0395378,1.038,1.036373,1.0346326,1.0327475,1.0306785,1.0283754,1.0257748,1.0227962,1.0193374,1.0152698,1.0104306,1.0046155,0.997569,0.9889721,0.9784302,0.9654586,0.94946885,0.92975664,0.9054958,0.8757428,0.8394609,0.7955762,0.7430771,0.68117666,0.6095448,0.5286127,0.43991768,0.34642226,0.25268826,0.164761,0.08963491,0.034280553,0.0043898677,0.0043898677,0.034280553,0.08963491,0.164761,0.25268826,0.34642226,0.43991768,0.5286127,0.6095448,0.68117666,0.7430771,0.7955762,0.8394609,0.8757428,0.9054958,0.92975664,0.94946885,0.9654586,0.9784302,0.9889721,0.997569,1.0046155,1.0104306,1.0152698,1.0193374,1.0227962,1.0257748,1.0283754,1.0306785,1.0327475,1.0346326,1.036373,1.038,1.0395378,1.0410055,1.0424182,1.0437878,1.0451236,1.0464327,1.0477209,1.0489928,1.0502517,1.0515007,1.0527416,1.0539764,1.0552063,1.0564324,1.0576555,1.0588762,1.0600951],[1.060116,1.0588971,1.0576764,1.0564533,1.0552273,1.0539974,1.0527626,1.0515217,1.0502727,1.0490137,1.0477419,1.0464537,1.0451446,1.0438088,1.0424392,1.0410265,1.0395588,1.038021,1.036394,1.0346535,1.0327685,1.0306995,1.0283964,1.0257958,1.0228171,1.0193584,1.0152907,1.0104516,1.0046365,0.99759007,0.98899317,0.97845125,0.9654796,0.9494899,0.9297777,0.90551686,0.87576383,0.83948195,0.79559726,0.74309814,0.6811977,0.60956585,0.5286337,0.43993866,0.34644324,0.25270924,0.164782,0.089655906,0.03430155,0.0044108634,0.0044108634,0.03430155,0.089655906,0.164782,0.25270924,0.34644324,0.43993866,0.5286337,0.60956585,0.6811977,0.74309814,0.79559726,0.83948195,0.87576383,0.90551686,0.9297777,0.9494899,0.9654796,0.97845125,0.98899317,0.99759007,1.0046365,1.0104516,1.0152907,1.0193584,1.0228171,1.0257958,1.0283964,1.0306995,1.0327685,1.0346535,1.036394,1.038021,1.0395588,1.0410265,1.0424392,1.0438088,1.0451446,1.0464537,1.0477419,1.0490137,1.0502727,1.0515217,1.0527626,1.0539974,1.0552273,1.0564533,1.0576764,1.0588971,1.060116],[1.0601407,1.0589218,1.0577011,1.056478,1.055252,1.0540221,1.0527873,1.0515463,1.0502974,1.0490384,1.0477666,1.0464784,1.0451692,1.0438335,1.0424639,1.0410511,1.0395834,1.0380456,1.0364187,1.0346782,1.0327932,1.0307242,1.028421,1.0258205,1.0228418,1.0193831,1.0153154,1.0104762,1.0046612,0.99761474,0.98901784,0.9784759,0.9655043,0.94951457,0.92980236,0.90554154,0.8757885,0.8395066,0.79562193,0.7431228,0.6812224,0.60959053,0.5286584,0.43996334,0.3464679,0.25273392,0.16480668,0.08968059,0.034326226,0.0044355406,0.0044355406,0.034326226,0.08968059,0.16480668,0.25273392,0.3464679,0.43996334,0.5286584,0.60959053,0.6812224,0.7431228,0.79562193,0.8395066,0.8757885,0.90554154,0.92980236,0.94951457,0.9655043,0.9784759,0.98901784,0.99761474,1.0046612,1.0104762,1.0153154,1.0193831,1.0228418,1.0258205,1.028421,1.0307242,1.0327932,1.0346782,1.0364187,1.0380456,1.0395834,1.0410511,1.0424639,1.0438335,1.0451692,1.0464784,1.0477666,1.0490384,1.0502974,1.0515463,1.0527873,1.0540221,1.055252,1.056478,1.0577011,1.0589218,1.0601407],[1.0601698,1.0589509,1.0577302,1.0565071,1.055281,1.0540512,1.0528164,1.0515754,1.0503265,1.0490675,1.0477957,1.0465075,1.0451983,1.0438626,1.042493,1.0410802,1.0396125,1.0380747,1.0364478,1.0347073,1.0328223,1.0307533,1.0284501,1.0258496,1.0228709,1.0194122,1.0153445,1.0105053,1.0046903,0.9976437,0.9890468,0.9785049,0.96553326,0.94954354,0.9298313,0.9055705,0.8758175,0.8395356,0.7956509,0.7431518,0.68125135,0.6096195,0.52868736,0.43999237,0.34649694,0.25276294,0.16483568,0.08970959,0.03435523,0.004464545,0.004464545,0.03435523,0.08970959,0.16483568,0.25276294,0.34649694,0.43999237,0.52868736,0.6096195,0.68125135,0.7431518,0.7956509,0.8395356,0.8758175,0.9055705,0.9298313,0.94954354,0.96553326,0.9785049,0.9890468,0.9976437,1.0046903,1.0105053,1.0153445,1.0194122,1.0228709,1.0258496,1.0284501,1.0307533,1.0328223,1.0347073,1.0364478,1.0380747,1.0396125,1.0410802,1.042493,1.0438626,1.0451983,1.0465075,1.0477957,1.0490675,1.0503265,1.0515754,1.0528164,1.0540512,1.055281,1.0565071,1.0577302,1.0589509,1.0601698],[1.0602039,1.058985,1.0577643,1.0565412,1.0553151,1.0540853,1.0528505,1.0516095,1.0503606,1.0491016,1.0478297,1.0465416,1.0452324,1.0438967,1.0425271,1.0411143,1.0396466,1.0381088,1.0364819,1.0347414,1.0328563,1.0307873,1.0284842,1.0258837,1.022905,1.0194463,1.0153786,1.0105394,1.0047244,0.9976778,0.9890809,0.978539,0.96556735,0.9495776,0.9298654,0.9056046,0.8758516,0.8395697,0.795685,0.7431859,0.68128544,0.6096536,0.52872145,0.44002643,0.346531,0.252797,0.16486977,0.08974368,0.03438932,0.0044986345,0.0044986345,0.03438932,0.08974368,0.16486977,0.252797,0.346531,0.44002643,0.52872145,0.6096536,0.68128544,0.7431859,0.795685,0.8395697,0.8758516,0.9056046,0.9298654,0.9495776,0.96556735,0.978539,0.9890809,0.9976778,1.0047244,1.0105394,1.0153786,1.0194463,1.022905,1.0258837,1.0284842,1.0307873,1.0328563,1.0347414,1.0364819,1.0381088,1.0396466,1.0411143,1.0425271,1.0438967,1.0452324,1.0465416,1.0478297,1.0491016,1.0503606,1.0516095,1.0528505,1.0540853,1.0553151,1.0565412,1.0577643,1.058985,1.0602039],[1.060244,1.059025,1.0578043,1.0565813,1.0553552,1.0541253,1.0528905,1.0516496,1.0504006,1.0491416,1.0478698,1.0465816,1.0452725,1.0439367,1.0425671,1.0411544,1.0396867,1.0381489,1.0365219,1.0347815,1.0328964,1.0308274,1.0285243,1.0259237,1.022945,1.0194863,1.0154186,1.0105795,1.0047644,0.99771786,0.98912096,0.97857904,0.9656074,0.9496177,0.9299055,0.90564466,0.8758916,0.83960974,0.79572505,0.74322593,0.6813255,0.60969365,0.5287615,0.44006652,0.3465711,0.2528371,0.16490984,0.08978375,0.034429386,0.0045387014,0.0045387014,0.034429386,0.08978375,0.16490984,0.2528371,0.3465711,0.44006652,0.5287615,0.60969365,0.6813255,0.74322593,0.79572505,0.83960974,0.8758916,0.90564466,0.9299055,0.9496177,0.9656074,0.97857904,0.98912096,0.99771786,1.0047644,1.0105795,1.0154186,1.0194863,1.022945,1.0259237,1.0285243,1.0308274,1.0328964,1.0347815,1.0365219,1.0381489,1.0396867,1.0411544,1.0425671,1.0439367,1.0452725,1.0465816,1.0478698,1.0491416,1.0504006,1.0516496,1.0528905,1.0541253,1.0553552,1.0565813,1.0578043,1.059025,1.060244],[1.060291,1.0590721,1.0578514,1.0566283,1.0554023,1.0541724,1.0529376,1.0516967,1.0504477,1.0491887,1.0479169,1.0466287,1.0453196,1.0439838,1.0426142,1.0412015,1.0397338,1.038196,1.036569,1.0348285,1.0329435,1.0308745,1.0285714,1.0259708,1.0229921,1.0195334,1.0154657,1.0106266,1.0048115,0.99776495,0.98916805,0.97862613,0.9656545,0.9496648,0.92995256,0.90569174,0.8759387,0.83965683,0.79577214,0.743273,0.6813726,0.60974073,0.5288086,0.4401136,0.34661818,0.25288418,0.16495693,0.08983084,0.034476478,0.004585792,0.004585792,0.034476478,0.08983084,0.16495693,0.25288418,0.34661818,0.4401136,0.5288086,0.60974073,0.6813726,0.743273,0.79577214,0.83965683,0.8759387,0.90569174,0.92995256,0.9496648,0.9656545,0.97862613,0.98916805,0.99776495,1.0048115,1.0106266,1.0154657,1.0195334,1.0229921,1.0259708,1.0285714,1.0308745,1.0329435,1.0348285,1.036569,1.038196,1.0397338,1.0412015,1.0426142,1.0439838,1.0453196,1.0466287,1.0479169,1.0491887,1.0504477,1.0516967,1.0529376,1.0541724,1.0554023,1.0566283,1.0578514,1.0590721,1.060291],[1.0603464,1.0591274,1.0579067,1.0566837,1.0554576,1.0542277,1.0529929,1.051752,1.050503,1.049244,1.0479722,1.046684,1.0453749,1.0440391,1.0426695,1.0412568,1.0397891,1.0382513,1.0366243,1.0348839,1.0329988,1.0309298,1.0286267,1.0260261,1.0230474,1.0195887,1.015521,1.0106819,1.0048668,0.9978203,0.9892234,0.9786815,0.96570987,0.94972014,0.93000793,0.9057471,0.8759941,0.8397122,0.7958275,0.7433284,0.68142796,0.6097961,0.52886397,0.44016895,0.34667352,0.25293952,0.16501227,0.08988618,0.03453182,0.0046411366,0.0046411366,0.03453182,0.08988618,0.16501227,0.25293952,0.34667352,0.44016895,0.52886397,0.6097961,0.68142796,0.7433284,0.7958275,0.8397122,0.8759941,0.9057471,0.93000793,0.94972014,0.96570987,0.9786815,0.9892234,0.9978203,1.0048668,1.0106819,1.015521,1.0195887,1.0230474,1.0260261,1.0286267,1.0309298,1.0329988,1.0348839,1.0366243,1.0382513,1.0397891,1.0412568,1.0426695,1.0440391,1.0453749,1.046684,1.0479722,1.049244,1.050503,1.051752,1.0529929,1.0542277,1.0554576,1.0566837,1.0579067,1.0591274,1.0603464],[1.0604115,1.0591925,1.0579718,1.0567487,1.0555227,1.0542928,1.053058,1.051817,1.0505681,1.0493091,1.0480373,1.0467491,1.04544,1.0441042,1.0427346,1.0413219,1.0398542,1.0383164,1.0366894,1.034949,1.0330639,1.0309949,1.0286918,1.0260912,1.0231125,1.0196538,1.0155861,1.010747,1.0049319,0.99788535,0.98928845,0.97874653,0.9657749,0.9497852,0.93007296,0.90581214,0.8760591,0.83977723,0.79589254,0.7433934,0.681493,0.60986114,0.528929,0.440234,0.34673858,0.25300458,0.16507731,0.089951225,0.034596868,0.0047061816,0.0047061816,0.034596868,0.089951225,0.16507731,0.25300458,0.34673858,0.440234,0.528929,0.60986114,0.681493,0.7433934,0.79589254,0.83977723,0.8760591,0.90581214,0.93007296,0.9497852,0.9657749,0.97874653,0.98928845,0.99788535,1.0049319,1.010747,1.0155861,1.0196538,1.0231125,1.0260912,1.0286918,1.0309949,1.0330639,1.034949,1.0366894,1.0383164,1.0398542,1.0413219,1.0427346,1.0441042,1.04544,1.0467491,1.0480373,1.0493091,1.0505681,1.051817,1.053058,1.0542928,1.0555227,1.0567487,1.0579718,1.0591925,1.0604115],[1.0604879,1.059269,1.0580482,1.0568252,1.0555991,1.0543692,1.0531344,1.0518935,1.0506445,1.0493855,1.0481137,1.0468255,1.0455164,1.0441806,1.042811,1.0413983,1.0399306,1.0383928,1.0367658,1.0350254,1.0331403,1.0310713,1.0287682,1.0261676,1.023189,1.0197302,1.0156626,1.0108234,1.0050083,0.9979618,0.9893649,0.978823,0.96585137,0.94986165,0.93014944,0.9058886,0.8761356,0.8398537,0.795969,0.7434699,0.68156946,0.6099376,0.52900547,0.44031045,0.34681502,0.25308102,0.16515376,0.09002767,0.03467331,0.004782625,0.004782625,0.03467331,0.09002767,0.16515376,0.25308102,0.34681502,0.44031045,0.52900547,0.6099376,0.68156946,0.7434699,0.795969,0.8398537,0.8761356,0.9058886,0.93014944,0.94986165,0.96585137,0.978823,0.9893649,0.9979618,1.0050083,1.0108234,1.0156626,1.0197302,1.023189,1.0261676,1.0287682,1.0310713,1.0331403,1.0350254,1.0367658,1.0383928,1.0399306,1.0413983,1.042811,1.0441806,1.0455164,1.0468255,1.0481137,1.0493855,1.0506445,1.0518935,1.0531344,1.0543692,1.0555991,1.0568252,1.0580482,1.059269,1.0604879],[1.0605778,1.0593588,1.0581381,1.056915,1.055689,1.0544591,1.0532243,1.0519834,1.0507344,1.0494754,1.0482036,1.0469154,1.0456063,1.0442705,1.0429009,1.0414882,1.0400205,1.0384827,1.0368557,1.0351152,1.0332302,1.0311612,1.0288581,1.0262575,1.0232788,1.0198201,1.0157524,1.0109133,1.0050982,0.99805164,0.98945475,0.97891283,0.9659412,0.94995147,0.93023926,0.90597844,0.8762254,0.8399435,0.79605883,0.7435597,0.6816593,0.61002743,0.5290953,0.44040027,0.34690484,0.25317085,0.1652436,0.09011751,0.034763146,0.0048724622,0.0048724622,0.034763146,0.09011751,0.1652436,0.25317085,0.34690484,0.44040027,0.5290953,0.61002743,0.6816593,0.7435597,0.79605883,0.8399435,0.8762254,0.90597844,0.93023926,0.94995147,0.9659412,0.97891283,0.98945475,0.99805164,1.0050982,1.0109133,1.0157524,1.0198201,1.0232788,1.0262575,1.0288581,1.0311612,1.0332302,1.0351152,1.0368557,1.0384827,1.0400205,1.0414882,1.0429009,1.0442705,1.0456063,1.0469154,1.0482036,1.0494754,1.0507344,1.0519834,1.0532243,1.0544591,1.055689,1.056915,1.0581381,1.0593588,1.0605778],[1.0606833,1.0594643,1.0582436,1.0570205,1.0557945,1.0545646,1.0533298,1.0520889,1.0508399,1.0495809,1.0483091,1.0470209,1.0457118,1.044376,1.0430064,1.0415937,1.040126,1.0385882,1.0369612,1.0352207,1.0333357,1.0312667,1.0289636,1.026363,1.0233843,1.0199256,1.0158579,1.0110188,1.0052037,0.9981572,0.9895603,0.9790184,0.96604675,0.950057,0.9303448,0.906084,0.876331,0.8400491,0.7961644,0.7436653,0.68176484,0.610133,0.52920085,0.44050586,0.34701043,0.25327644,0.16534917,0.09022308,0.034868725,0.0049780374,0.0049780374,0.034868725,0.09022308,0.16534917,0.25327644,0.34701043,0.44050586,0.52920085,0.610133,0.68176484,0.7436653,0.7961644,0.8400491,0.876331,0.906084,0.9303448,0.950057,0.96604675,0.9790184,0.9895603,0.9981572,1.0052037,1.0110188,1.0158579,1.0199256,1.0233843,1.026363,1.0289636,1.0312667,1.0333357,1.0352207,1.0369612,1.0385882,1.040126,1.0415937,1.0430064,1.044376,1.0457118,1.0470209,1.0483091,1.0495809,1.0508399,1.0520889,1.0533298,1.0545646,1.0557945,1.0570205,1.0582436,1.0594643,1.0606833],[1.0608073,1.0595884,1.0583677,1.0571446,1.0559186,1.0546887,1.0534539,1.052213,1.050964,1.049705,1.0484332,1.047145,1.0458359,1.0445001,1.0431305,1.0417178,1.0402501,1.0387123,1.0370853,1.0353448,1.0334598,1.0313908,1.0290877,1.0264871,1.0235084,1.0200497,1.015982,1.0111428,1.0053278,0.9982813,0.9896844,0.9791425,0.96617085,0.9501811,0.9304689,0.9062081,0.87645507,0.8401732,0.7962885,0.7437894,0.68188894,0.6102571,0.52932495,0.44062993,0.3471345,0.2534005,0.16547324,0.09034715,0.034992788,0.0051021036,0.0051021036,0.034992788,0.09034715,0.16547324,0.2534005,0.3471345,0.44062993,0.52932495,0.6102571,0.68188894,0.7437894,0.7962885,0.8401732,0.87645507,0.9062081,0.9304689,0.9501811,0.96617085,0.9791425,0.9896844,0.9982813,1.0053278,1.0111428,1.015982,1.0200497,1.0235084,1.0264871,1.0290877,1.0313908,1.0334598,1.0353448,1.0370853,1.0387123,1.0402501,1.0417178,1.0431305,1.0445001,1.0458359,1.047145,1.0484332,1.049705,1.050964,1.052213,1.0534539,1.0546887,1.0559186,1.0571446,1.0583677,1.0595884,1.0608073],[1.0609531,1.0597342,1.0585135,1.0572904,1.0560644,1.0548345,1.0535997,1.0523587,1.0511098,1.0498508,1.048579,1.0472908,1.0459816,1.0446459,1.0432763,1.0418636,1.0403959,1.038858,1.0372311,1.0354906,1.0336056,1.0315366,1.0292335,1.0266329,1.0236542,1.0201955,1.0161278,1.0112886,1.0054736,0.9984271,0.9898302,0.9792883,0.96631664,0.9503269,0.9306147,0.9063539,0.87660086,0.840319,0.7964343,0.74393517,0.68203473,0.6104029,0.52947074,0.4407757,0.34728026,0.25354627,0.16561903,0.090492934,0.035138577,0.0052478914,0.0052478914,0.035138577,0.090492934,0.16561903,0.25354627,0.34728026,0.4407757,0.52947074,0.6104029,0.68203473,0.74393517,0.7964343,0.840319,0.87660086,0.9063539,0.9306147,0.9503269,0.96631664,0.9792883,0.9898302,0.9984271,1.0054736,1.0112886,1.0161278,1.0201955,1.0236542,1.0266329,1.0292335,1.0315366,1.0336056,1.0354906,1.0372311,1.038858,1.0403959,1.0418636,1.0432763,1.0446459,1.0459816,1.0472908,1.048579,1.0498508,1.0511098,1.0523587,1.0535997,1.0548345,1.0560644,1.0572904,1.0585135,1.0597342,1.0609531],[1.0611244,1.0599055,1.0586848,1.0574617,1.0562357,1.0550058,1.053771,1.05253,1.0512811,1.0500221,1.0487503,1.0474621,1.046153,1.0448172,1.0434476,1.0420349,1.0405672,1.0390294,1.0374024,1.0356619,1.0337769,1.0317079,1.0294048,1.0268042,1.0238255,1.0203668,1.0162991,1.01146,1.0056449,0.9985984,0.9900015,0.9794596,0.96648794,0.9504982,0.930786,0.9065252,0.87677217,0.8404903,0.7966056,0.7441065,0.68220603,0.6105742,0.52964205,0.440947,0.34745157,0.25371757,0.16579033,0.090664245,0.035309885,0.0054191984,0.0054191984,0.035309885,0.090664245,0.16579033,0.25371757,0.34745157,0.440947,0.52964205,0.6105742,0.68220603,0.7441065,0.7966056,0.8404903,0.87677217,0.9065252,0.930786,0.9504982,0.96648794,0.9794596,0.9900015,0.9985984,1.0056449,1.01146,1.0162991,1.0203668,1.0238255,1.0268042,1.0294048,1.0317079,1.0337769,1.0356619,1.0374024,1.0390294,1.0405672,1.0420349,1.0434476,1.0448172,1.046153,1.0474621,1.0487503,1.0500221,1.0512811,1.05253,1.053771,1.0550058,1.0562357,1.0574617,1.0586848,1.0599055,1.0611244],[1.0613257,1.0601068,1.058886,1.057663,1.0564369,1.055207,1.0539722,1.0527313,1.0514823,1.0502234,1.0489515,1.0476633,1.0463542,1.0450184,1.0436488,1.0422361,1.0407684,1.0392306,1.0376036,1.0358632,1.0339781,1.0319091,1.029606,1.0270054,1.0240268,1.020568,1.0165004,1.0116612,1.0058461,0.9987997,0.9902028,0.97966087,0.9666892,0.9506995,0.9309873,0.9067265,0.87697345,0.84069157,0.7968069,0.74430776,0.6824073,0.6107755,0.52984333,0.44114828,0.34765285,0.25391886,0.16599162,0.09086552,0.035511162,0.0056204787,0.0056204787,0.035511162,0.09086552,0.16599162,0.25391886,0.34765285,0.44114828,0.52984333,0.6107755,0.6824073,0.74430776,0.7968069,0.84069157,0.87697345,0.9067265,0.9309873,0.9506995,0.9666892,0.97966087,0.9902028,0.9987997,1.0058461,1.0116612,1.0165004,1.020568,1.0240268,1.0270054,1.029606,1.0319091,1.0339781,1.0358632,1.0376036,1.0392306,1.0407684,1.0422361,1.0436488,1.0450184,1.0463542,1.0476633,1.0489515,1.0502234,1.0514823,1.0527313,1.0539722,1.055207,1.0564369,1.057663,1.058886,1.0601068,1.0613257],[1.0615622,1.0603433,1.0591226,1.0578995,1.0566734,1.0554435,1.0542088,1.0529678,1.0517188,1.0504599,1.049188,1.0478998,1.0465907,1.045255,1.0438854,1.0424726,1.0410049,1.0394671,1.0378401,1.0360997,1.0342146,1.0321456,1.0298425,1.027242,1.0242633,1.0208045,1.0167369,1.0118977,1.0060827,0.99903613,0.99043924,0.9798973,0.9669257,0.95093596,0.93122375,0.90696293,0.8772099,0.840928,0.7970433,0.7445442,0.6826438,0.6110119,0.5300798,0.44138476,0.34788933,0.25415534,0.1662281,0.091102004,0.035747647,0.005856961,0.005856961,0.035747647,0.091102004,0.1662281,0.25415534,0.34788933,0.44138476,0.5300798,0.6110119,0.6826438,0.7445442,0.7970433,0.840928,0.8772099,0.90696293,0.93122375,0.95093596,0.9669257,0.9798973,0.99043924,0.99903613,1.0060827,1.0118977,1.0167369,1.0208045,1.0242633,1.027242,1.0298425,1.0321456,1.0342146,1.0360997,1.0378401,1.0394671,1.0410049,1.0424726,1.0438854,1.045255,1.0465907,1.0478998,1.049188,1.0504599,1.0517188,1.0529678,1.0542088,1.0554435,1.0566734,1.0578995,1.0591226,1.0603433,1.0615622],[1.06184,1.0606211,1.0594004,1.0581774,1.0569513,1.0557214,1.0544866,1.0532457,1.0519967,1.0507377,1.0494659,1.0481777,1.0468686,1.0455328,1.0441632,1.0427505,1.0412828,1.039745,1.038118,1.0363775,1.0344925,1.0324235,1.0301204,1.0275198,1.0245411,1.0210824,1.0170147,1.0121756,1.0063605,0.99931395,0.99071705,0.98017514,0.9672035,0.9512138,0.93150157,0.90724075,0.8774877,0.84120584,0.79732114,0.744822,0.6829216,0.61128974,0.5303576,0.44166258,0.34816715,0.25443316,0.16650592,0.09137983,0.03602547,0.006134782,0.006134782,0.03602547,0.09137983,0.16650592,0.25443316,0.34816715,0.44166258,0.5303576,0.61128974,0.6829216,0.744822,0.79732114,0.84120584,0.8774877,0.90724075,0.93150157,0.9512138,0.9672035,0.98017514,0.99071705,0.99931395,1.0063605,1.0121756,1.0170147,1.0210824,1.0245411,1.0275198,1.0301204,1.0324235,1.0344925,1.0363775,1.038118,1.039745,1.0412828,1.0427505,1.0441632,1.0455328,1.0468686,1.0481777,1.0494659,1.0507377,1.0519967,1.0532457,1.0544866,1.0557214,1.0569513,1.0581774,1.0594004,1.0606211,1.06184],[1.0621663,1.0609474,1.0597267,1.0585036,1.0572776,1.0560477,1.0548129,1.0535719,1.052323,1.051064,1.0497922,1.048504,1.0471948,1.0458591,1.0444895,1.0430768,1.041609,1.0400712,1.0384443,1.0367038,1.0348188,1.0327498,1.0304466,1.0278461,1.0248674,1.0214087,1.017341,1.0125018,1.0066868,0.99964035,0.99104345,0.98050153,0.9675299,0.9515402,0.93182796,0.90756714,0.8778141,0.84153223,0.79764754,0.7451484,0.683248,0.61161613,0.530684,0.44198895,0.34849352,0.25475952,0.16683227,0.09170619,0.036351822,0.006461138,0.006461138,0.036351822,0.09170619,0.16683227,0.25475952,0.34849352,0.44198895,0.530684,0.61161613,0.683248,0.7451484,0.79764754,0.84153223,0.8778141,0.90756714,0.93182796,0.9515402,0.9675299,0.98050153,0.99104345,0.99964035,1.0066868,1.0125018,1.017341,1.0214087,1.0248674,1.0278461,1.0304466,1.0327498,1.0348188,1.0367038,1.0384443,1.0400712,1.041609,1.0430768,1.0444895,1.0458591,1.0471948,1.048504,1.0497922,1.051064,1.052323,1.0535719,1.0548129,1.0560477,1.0572776,1.0585036,1.0597267,1.0609474,1.0621663],[1.0625497,1.0613308,1.0601101,1.058887,1.0576609,1.056431,1.0551963,1.0539553,1.0527064,1.0514474,1.0501755,1.0488874,1.0475782,1.0462425,1.0448729,1.0434601,1.0419924,1.0404546,1.0388277,1.0370872,1.0352021,1.0331331,1.03083,1.0282295,1.0252508,1.021792,1.0177244,1.0128852,1.0070702,1.0000236,0.99142677,0.98088485,0.9679132,0.9519235,0.9322113,0.90795046,0.87819743,0.84191555,0.79803085,0.74553174,0.6836313,0.61199945,0.5310673,0.4423723,0.34887686,0.25514287,0.1672156,0.09208951,0.03673515,0.0068444656,0.0068444656,0.03673515,0.09208951,0.1672156,0.25514287,0.34887686,0.4423723,0.5310673,0.61199945,0.6836313,0.74553174,0.79803085,0.84191555,0.87819743,0.90795046,0.9322113,0.9519235,0.9679132,0.98088485,0.99142677,1.0000236,1.0070702,1.0128852,1.0177244,1.021792,1.0252508,1.0282295,1.03083,1.0331331,1.0352021,1.0370872,1.0388277,1.0404546,1.0419924,1.0434601,1.0448729,1.0462425,1.0475782,1.0488874,1.0501755,1.0514474,1.0527064,1.0539553,1.0551963,1.056431,1.0576609,1.058887,1.0601101,1.0613308,1.0625497],[1.0629998,1.0617809,1.0605602,1.0593371,1.0581111,1.0568812,1.0556464,1.0544055,1.0531565,1.0518975,1.0506257,1.0493375,1.0480283,1.0466926,1.045323,1.0439103,1.0424426,1.0409048,1.0392778,1.0375373,1.0356523,1.0335833,1.0312802,1.0286796,1.0257009,1.0222422,1.0181745,1.0133353,1.0075203,1.0004739,0.99187696,0.98133504,0.9683634,0.9523737,0.9326615,0.90840065,0.8786476,0.84236574,0.79848105,0.74598193,0.6840815,0.61244965,0.5315175,0.44282246,0.34932703,0.25559303,0.1676658,0.092539705,0.03718534,0.0072946576,0.0072946576,0.03718534,0.092539705,0.1676658,0.25559303,0.34932703,0.44282246,0.5315175,0.61244965,0.6840815,0.74598193,0.79848105,0.84236574,0.8786476,0.90840065,0.9326615,0.9523737,0.9683634,0.98133504,0.99187696,1.0004739,1.0075203,1.0133353,1.0181745,1.0222422,1.0257009,1.0286796,1.0312802,1.0335833,1.0356523,1.0375373,1.0392778,1.0409048,1.0424426,1.0439103,1.045323,1.0466926,1.0480283,1.0493375,1.0506257,1.0518975,1.0531565,1.0544055,1.0556464,1.0568812,1.0581111,1.0593371,1.0605602,1.0617809,1.0629998],[1.0635285,1.0623096,1.0610889,1.0598658,1.0586398,1.0574099,1.0561751,1.0549341,1.0536852,1.0524262,1.0511544,1.0498662,1.048557,1.0472213,1.0458517,1.044439,1.0429713,1.0414335,1.0398065,1.038066,1.036181,1.034112,1.0318089,1.0292083,1.0262296,1.0227709,1.0187032,1.013864,1.008049,1.0010024,0.9924056,0.9818637,0.96889204,0.9529023,0.9331901,0.9089293,0.87917626,0.8428944,0.7990097,0.74651057,0.6846101,0.6129783,0.53204614,0.44335112,0.3498557,0.2561217,0.16819443,0.09306835,0.037713982,0.007823298,0.007823298,0.037713982,0.09306835,0.16819443,0.2561217,0.3498557,0.44335112,0.53204614,0.6129783,0.6846101,0.74651057,0.7990097,0.8428944,0.87917626,0.9089293,0.9331901,0.9529023,0.96889204,0.9818637,0.9924056,1.0010024,1.008049,1.013864,1.0187032,1.0227709,1.0262296,1.0292083,1.0318089,1.034112,1.036181,1.038066,1.0398065,1.0414335,1.0429713,1.044439,1.0458517,1.0472213,1.048557,1.0498662,1.0511544,1.0524262,1.0536852,1.0549341,1.0561751,1.0574099,1.0586398,1.0598658,1.0610889,1.0623096,1.0635285],[1.0641491,1.0629302,1.0617095,1.0604864,1.0592604,1.0580305,1.0567957,1.0555547,1.0543058,1.0530468,1.051775,1.0504868,1.0491776,1.0478419,1.0464723,1.0450596,1.0435919,1.042054,1.0404271,1.0386866,1.0368016,1.0347326,1.0324295,1.0298289,1.0268502,1.0233915,1.0193238,1.0144846,1.0086696,1.0016232,0.99302626,0.98248434,0.9695127,0.953523,0.9338108,0.90954995,0.8797969,0.84351504,0.79963034,0.7471312,0.6852308,0.61359894,0.5326668,0.44397175,0.35047632,0.25674233,0.16881508,0.093688995,0.038334634,0.008443948,0.008443948,0.038334634,0.093688995,0.16881508,0.25674233,0.35047632,0.44397175,0.5326668,0.61359894,0.6852308,0.7471312,0.79963034,0.84351504,0.8797969,0.90954995,0.9338108,0.953523,0.9695127,0.98248434,0.99302626,1.0016232,1.0086696,1.0144846,1.0193238,1.0233915,1.0268502,1.0298289,1.0324295,1.0347326,1.0368016,1.0386866,1.0404271,1.042054,1.0435919,1.0450596,1.0464723,1.0478419,1.0491776,1.0504868,1.051775,1.0530468,1.0543058,1.0555547,1.0567957,1.0580305,1.0592604,1.0604864,1.0617095,1.0629302,1.0641491],[1.0648777,1.0636588,1.0624381,1.061215,1.059989,1.0587591,1.0575243,1.0562834,1.0550344,1.0537754,1.0525036,1.0512154,1.0499063,1.0485705,1.0472009,1.0457882,1.0443205,1.0427827,1.0411557,1.0394152,1.0375302,1.0354612,1.0331581,1.0305575,1.0275788,1.0241201,1.0200524,1.0152133,1.0093982,1.0023516,0.99375474,0.9832128,0.9702412,0.95425147,0.93453926,0.91027844,0.8805254,0.8442435,0.80035883,0.7478597,0.6859593,0.61432743,0.5333953,0.4447003,0.35120487,0.25747088,0.16954361,0.09441753,0.039063167,0.009172481,0.009172481,0.039063167,0.09441753,0.16954361,0.25747088,0.35120487,0.4447003,0.5333953,0.61432743,0.6859593,0.7478597,0.80035883,0.8442435,0.8805254,0.91027844,0.93453926,0.95425147,0.9702412,0.9832128,0.99375474,1.0023516,1.0093982,1.0152133,1.0200524,1.0241201,1.0275788,1.0305575,1.0331581,1.0354612,1.0375302,1.0394152,1.0411557,1.0427827,1.0443205,1.0457882,1.0472009,1.0485705,1.0499063,1.0512154,1.0525036,1.0537754,1.0550344,1.0562834,1.0575243,1.0587591,1.059989,1.061215,1.0624381,1.0636588,1.0648777],[1.0657327,1.0645138,1.0632931,1.06207,1.060844,1.0596141,1.0583793,1.0571383,1.0558894,1.0546304,1.0533586,1.0520704,1.0507612,1.0494255,1.0480559,1.0466431,1.0451754,1.0436376,1.0420107,1.0402702,1.0383852,1.0363162,1.034013,1.0314125,1.0284338,1.0249751,1.0209074,1.0160682,1.0102532,1.0032066,0.9946097,0.9840678,0.97109616,0.95510644,0.9353942,0.9111334,0.8813804,0.8450985,0.8012138,0.7487147,0.68681425,0.6151824,0.53425026,0.44555527,0.35205984,0.25832582,0.17039858,0.09527249,0.03991813,0.010027443,0.010027443,0.03991813,0.09527249,0.17039858,0.25832582,0.35205984,0.44555527,0.53425026,0.6151824,0.68681425,0.7487147,0.8012138,0.8450985,0.8813804,0.9111334,0.9353942,0.95510644,0.97109616,0.9840678,0.9946097,1.0032066,1.0102532,1.0160682,1.0209074,1.0249751,1.0284338,1.0314125,1.034013,1.0363162,1.0383852,1.0402702,1.0420107,1.0436376,1.0451754,1.0466431,1.0480559,1.0494255,1.0507612,1.0520704,1.0533586,1.0546304,1.0558894,1.0571383,1.0583793,1.0596141,1.060844,1.06207,1.0632931,1.0645138,1.0657327],[1.0667357,1.0655168,1.0642961,1.063073,1.061847,1.0606171,1.0593823,1.0581414,1.0568924,1.0556334,1.0543616,1.0530734,1.0517642,1.0504285,1.0490589,1.0476462,1.0461785,1.0446407,1.0430137,1.0412732,1.0393882,1.0373192,1.0350161,1.0324155,1.0294368,1.0259781,1.0219104,1.0170712,1.0112562,1.0042096,0.9956128,0.9850709,0.97209924,0.9561095,0.9363973,0.9121365,0.88238347,0.8461016,0.8022169,0.7497178,0.68781734,0.6161855,0.53525335,0.4465583,0.35306287,0.25932887,0.17140163,0.09627554,0.040921178,0.011030493,0.011030493,0.040921178,0.09627554,0.17140163,0.25932887,0.35306287,0.4465583,0.53525335,0.6161855,0.68781734,0.7497178,0.8022169,0.8461016,0.88238347,0.9121365,0.9363973,0.9561095,0.97209924,0.9850709,0.9956128,1.0042096,1.0112562,1.0170712,1.0219104,1.0259781,1.0294368,1.0324155,1.0350161,1.0373192,1.0393882,1.0412732,1.0430137,1.0446407,1.0461785,1.0476462,1.0490589,1.0504285,1.0517642,1.0530734,1.0543616,1.0556334,1.0568924,1.0581414,1.0593823,1.0606171,1.061847,1.063073,1.0642961,1.0655168,1.0667357],[1.0679121,1.0666932,1.0654725,1.0642494,1.0630233,1.0617934,1.0605587,1.0593177,1.0580688,1.0568098,1.0555379,1.0542498,1.0529406,1.0516049,1.0502353,1.0488225,1.0473548,1.045817,1.04419,1.0424496,1.0405645,1.0384955,1.0361924,1.0335919,1.0306132,1.0271544,1.0230868,1.0182476,1.0124326,1.0053861,0.9967892,0.9862473,0.97327566,0.95728594,0.93757373,0.9133129,0.8835599,0.847278,0.8033933,0.7508942,0.68899375,0.6173619,0.53642976,0.4477347,0.35423928,0.2605053,0.17257804,0.097451955,0.04209759,0.012206907,0.012206907,0.04209759,0.097451955,0.17257804,0.2605053,0.35423928,0.4477347,0.53642976,0.6173619,0.68899375,0.7508942,0.8033933,0.847278,0.8835599,0.9133129,0.93757373,0.95728594,0.97327566,0.9862473,0.9967892,1.0053861,1.0124326,1.0182476,1.0230868,1.0271544,1.0306132,1.0335919,1.0361924,1.0384955,1.0405645,1.0424496,1.04419,1.045817,1.0473548,1.0488225,1.0502353,1.0516049,1.0529406,1.0542498,1.0555379,1.0568098,1.0580688,1.0593177,1.0605587,1.0617934,1.0630233,1.0642494,1.0654725,1.0666932,1.0679121],[1.0692914,1.0680724,1.0668517,1.0656286,1.0644026,1.0631727,1.0619379,1.060697,1.059448,1.058189,1.0569172,1.055629,1.0543199,1.0529841,1.0516145,1.0502018,1.0487341,1.0471963,1.0455693,1.0438288,1.0419438,1.0398748,1.0375717,1.0349711,1.0319924,1.0285337,1.024466,1.0196269,1.0138118,1.0067652,0.9981684,0.9876265,0.97465485,0.95866513,0.9389529,0.9146921,0.8849391,0.8486572,0.8047725,0.7522734,0.69037294,0.6187411,0.53780895,0.44911394,0.3556185,0.2618845,0.17395726,0.09883116,0.0434768,0.013586117,0.013586117,0.0434768,0.09883116,0.17395726,0.2618845,0.3556185,0.44911394,0.53780895,0.6187411,0.69037294,0.7522734,0.8047725,0.8486572,0.8849391,0.9146921,0.9389529,0.95866513,0.97465485,0.9876265,0.9981684,1.0067652,1.0138118,1.0196269,1.024466,1.0285337,1.0319924,1.0349711,1.0375717,1.0398748,1.0419438,1.0438288,1.0455693,1.0471963,1.0487341,1.0502018,1.0516145,1.0529841,1.0543199,1.055629,1.0569172,1.058189,1.059448,1.060697,1.0619379,1.0631727,1.0644026,1.0656286,1.0668517,1.0680724,1.0692914],[1.0709076,1.0696887,1.068468,1.0672449,1.0660188,1.0647889,1.0635542,1.0623132,1.0610642,1.0598053,1.0585334,1.0572453,1.0559361,1.0546004,1.0532308,1.051818,1.0503503,1.0488125,1.0471855,1.0454451,1.04356,1.041491,1.0391879,1.0365874,1.0336087,1.0301499,1.0260823,1.0212431,1.0154281,1.0083815,0.99978465,0.98924273,0.9762711,0.9602814,0.94056916,0.91630834,0.8865553,0.85027343,0.80638874,0.7538896,0.6919892,0.62035733,0.5394252,0.45073017,0.35723475,0.26350075,0.17557348,0.1004474,0.045093037,0.015202352,0.015202352,0.045093037,0.1004474,0.17557348,0.26350075,0.35723475,0.45073017,0.5394252,0.62035733,0.6919892,0.7538896,0.80638874,0.85027343,0.8865553,0.91630834,0.94056916,0.9602814,0.9762711,0.98924273,0.99978465,1.0083815,1.0154281,1.0212431,1.0260823,1.0301499,1.0336087,1.0365874,1.0391879,1.041491,1.04356,1.0454451,1.0471855,1.0488125,1.0503503,1.051818,1.0532308,1.0546004,1.0559361,1.0572453,1.0585334,1.0598053,1.0610642,1.0623132,1.0635542,1.0647889,1.0660188,1.0672449,1.068468,1.0696887,1.0709076],[1.0728006,1.0715817,1.070361,1.0691379,1.0679119,1.066682,1.0654472,1.0642062,1.0629573,1.0616983,1.0604265,1.0591383,1.0578291,1.0564934,1.0551238,1.053711,1.0522434,1.0507056,1.0490786,1.0473381,1.0454531,1.0433841,1.041081,1.0384804,1.0355017,1.032043,1.0279753,1.0231361,1.0173211,1.0102745,1.0016776,0.9911357,0.9781641,0.96217436,0.94246215,0.9182013,0.8884483,0.8521664,0.8082817,0.7557826,0.69388217,0.6222503,0.5413182,0.4526232,0.35912776,0.26539376,0.1774665,0.102340415,0.04698605,0.017095365,0.017095365,0.04698605,0.102340415,0.1774665,0.26539376,0.35912776,0.4526232,0.5413182,0.6222503,0.69388217,0.7557826,0.8082817,0.8521664,0.8884483,0.9182013,0.94246215,0.96217436,0.9781641,0.9911357,1.0016776,1.0102745,1.0173211,1.0231361,1.0279753,1.032043,1.0355017,1.0384804,1.041081,1.0433841,1.0454531,1.0473381,1.0490786,1.0507056,1.0522434,1.053711,1.0551238,1.0564934,1.0578291,1.0591383,1.0604265,1.0616983,1.0629573,1.0642062,1.0654472,1.066682,1.0679119,1.0691379,1.070361,1.0715817,1.0728006],[1.0750164,1.0737975,1.0725768,1.0713537,1.0701276,1.0688977,1.067663,1.066422,1.065173,1.0639141,1.0626422,1.061354,1.0600449,1.0587091,1.0573395,1.0559268,1.0544591,1.0529213,1.0512943,1.0495539,1.0476688,1.0455998,1.0432967,1.0406961,1.0377175,1.0342587,1.0301911,1.0253519,1.0195369,1.0124904,1.0038935,0.9933516,0.98037994,0.9643902,0.944678,0.9204172,0.89066416,0.8543823,0.8104976,0.75799847,0.696098,0.6244662,0.54353404,0.454839,0.36134356,0.26760957,0.17968233,0.10455623,0.049201876,0.01931119,0.01931119,0.049201876,0.10455623,0.17968233,0.26760957,0.36134356,0.454839,0.54353404,0.6244662,0.696098,0.75799847,0.8104976,0.8543823,0.89066416,0.9204172,0.944678,0.9643902,0.98037994,0.9933516,1.0038935,1.0124904,1.0195369,1.0253519,1.0301911,1.0342587,1.0377175,1.0406961,1.0432967,1.0455998,1.0476688,1.0495539,1.0512943,1.0529213,1.0544591,1.0559268,1.0573395,1.0587091,1.0600449,1.061354,1.0626422,1.0639141,1.065173,1.066422,1.067663,1.0688977,1.0701276,1.0713537,1.0725768,1.0737975,1.0750164],[1.0776082,1.0763893,1.0751686,1.0739455,1.0727195,1.0714896,1.0702548,1.0690138,1.0677649,1.0665059,1.0652341,1.0639459,1.0626367,1.061301,1.0599314,1.0585186,1.057051,1.0555131,1.0538862,1.0521457,1.0502607,1.0481917,1.0458885,1.043288,1.0403093,1.0368506,1.0327829,1.0279437,1.0221287,1.0150821,1.0064852,0.99594337,0.9829717,0.966982,0.9472698,0.923009,0.89325595,0.85697407,0.8130894,0.76059026,0.6986898,0.62705797,0.5461258,0.4574308,0.36393538,0.27020139,0.18227413,0.10714804,0.05179368,0.021902993,0.021902993,0.05179368,0.10714804,0.18227413,0.27020139,0.36393538,0.4574308,0.5461258,0.62705797,0.6986898,0.76059026,0.8130894,0.85697407,0.89325595,0.923009,0.9472698,0.966982,0.9829717,0.99594337,1.0064852,1.0150821,1.0221287,1.0279437,1.0327829,1.0368506,1.0403093,1.043288,1.0458885,1.0481917,1.0502607,1.0521457,1.0538862,1.0555131,1.057051,1.0585186,1.0599314,1.061301,1.0626367,1.0639459,1.0652341,1.0665059,1.0677649,1.0690138,1.0702548,1.0714896,1.0727195,1.0739455,1.0751686,1.0763893,1.0776082],[1.0806373,1.0794184,1.0781977,1.0769746,1.0757486,1.0745187,1.0732839,1.072043,1.070794,1.069535,1.0682632,1.066975,1.0656658,1.0643301,1.0629605,1.0615478,1.06008,1.0585423,1.0569153,1.0551748,1.0532898,1.0512208,1.0489177,1.0463171,1.0433384,1.0398797,1.035812,1.0309728,1.0251578,1.0181112,1.0095143,0.9989724,0.9860008,0.97001106,0.95029885,0.926038,0.896285,0.8600031,0.8161184,0.7636193,0.70171887,0.630087,0.5491549,0.46045986,0.36696443,0.27323043,0.18530318,0.11017709,0.054822735,0.02493205,0.02493205,0.054822735,0.11017709,0.18530318,0.27323043,0.36696443,0.46045986,0.5491549,0.630087,0.70171887,0.7636193,0.8161184,0.8600031,0.896285,0.926038,0.95029885,0.97001106,0.9860008,0.9989724,1.0095143,1.0181112,1.0251578,1.0309728,1.035812,1.0398797,1.0433384,1.0463171,1.0489177,1.0512208,1.0532898,1.0551748,1.0569153,1.0585423,1.06008,1.0615478,1.0629605,1.0643301,1.0656658,1.066975,1.0682632,1.069535,1.070794,1.072043,1.0732839,1.0745187,1.0757486,1.0769746,1.0781977,1.0794184,1.0806373],[1.0841738,1.0829549,1.0817342,1.0805111,1.079285,1.0780551,1.0768204,1.0755794,1.0743304,1.0730715,1.0717996,1.0705115,1.0692023,1.0678666,1.066497,1.0650842,1.0636165,1.0620787,1.0604517,1.0587113,1.0568262,1.0547572,1.0524541,1.0498536,1.0468749,1.0434161,1.0393485,1.0345093,1.0286943,1.0216478,1.0130509,1.002509,0.98953736,0.97354764,0.9538354,0.9295746,0.8998216,0.8635397,0.819655,0.7671559,0.70525545,0.6336236,0.55269146,0.46399644,0.370501,0.27676702,0.18883976,0.113713674,0.05835931,0.028468627,0.028468627,0.05835931,0.113713674,0.18883976,0.27676702,0.370501,0.46399644,0.55269146,0.6336236,0.70525545,0.7671559,0.819655,0.8635397,0.8998216,0.9295746,0.9538354,0.97354764,0.98953736,1.002509,1.0130509,1.0216478,1.0286943,1.0345093,1.0393485,1.0434161,1.0468749,1.0498536,1.0524541,1.0547572,1.0568262,1.0587113,1.0604517,1.0620787,1.0636165,1.0650842,1.066497,1.0678666,1.0692023,1.0705115,1.0717996,1.0730715,1.0743304,1.0755794,1.0768204,1.0780551,1.079285,1.0805111,1.0817342,1.0829549,1.0841738],[1.0882983,1.0870794,1.0858587,1.0846356,1.0834095,1.0821797,1.0809449,1.0797039,1.078455,1.077196,1.0759242,1.074636,1.0733268,1.0719911,1.0706215,1.0692087,1.067741,1.0662032,1.0645763,1.0628358,1.0609508,1.0588818,1.0565786,1.0539781,1.0509994,1.0475407,1.043473,1.0386338,1.0328188,1.0257722,1.0171753,1.0066334,0.99366176,0.97767204,0.95795983,0.933699,0.903946,0.8676641,0.8237794,0.7712803,0.70937985,0.637748,0.55681586,0.46812084,0.3746254,0.28089142,0.19296417,0.117838085,0.06248372,0.032593034,0.032593034,0.06248372,0.117838085,0.19296417,0.28089142,0.3746254,0.46812084,0.55681586,0.637748,0.70937985,0.7712803,0.8237794,0.8676641,0.903946,0.933699,0.95795983,0.97767204,0.99366176,1.0066334,1.0171753,1.0257722,1.0328188,1.0386338,1.043473,1.0475407,1.0509994,1.0539781,1.0565786,1.0588818,1.0609508,1.0628358,1.0645763,1.0662032,1.067741,1.0692087,1.0706215,1.0719911,1.0733268,1.074636,1.0759242,1.077196,1.078455,1.0797039,1.0809449,1.0821797,1.0834095,1.0846356,1.0858587,1.0870794,1.0882983],[1.0931017,1.0918828,1.0906621,1.089439,1.088213,1.0869831,1.0857483,1.0845073,1.0832584,1.0819994,1.0807276,1.0794394,1.0781302,1.0767945,1.0754249,1.0740122,1.0725445,1.0710067,1.0693797,1.0676392,1.0657542,1.0636852,1.061382,1.0587815,1.0558028,1.0523441,1.0482764,1.0434372,1.0376222,1.0305758,1.0219789,1.0114369,0.9984653,0.9824756,0.96276337,0.93850255,0.9087495,0.87246764,0.82858294,0.7760838,0.7141834,0.64255154,0.5616194,0.47292435,0.37942892,0.28569493,0.19776769,0.12264159,0.06728724,0.03739655,0.03739655,0.06728724,0.12264159,0.19776769,0.28569493,0.37942892,0.47292435,0.5616194,0.64255154,0.7141834,0.7760838,0.82858294,0.87246764,0.9087495,0.93850255,0.96276337,0.9824756,0.9984653,1.0114369,1.0219789,1.0305758,1.0376222,1.0434372,1.0482764,1.0523441,1.0558028,1.0587815,1.061382,1.0636852,1.0657542,1.0676392,1.0693797,1.0710067,1.0725445,1.0740122,1.0754249,1.0767945,1.0781302,1.0794394,1.0807276,1.0819994,1.0832584,1.0845073,1.0857483,1.0869831,1.088213,1.089439,1.0906621,1.0918828,1.0931017],[1.0986875,1.0974686,1.0962479,1.0950248,1.0937988,1.0925689,1.0913341,1.0900931,1.0888442,1.0875852,1.0863134,1.0850252,1.083716,1.0823803,1.0810107,1.079598,1.0781302,1.0765924,1.0749655,1.073225,1.07134,1.069271,1.0669678,1.0643673,1.0613886,1.0579299,1.0538622,1.049023,1.043208,1.0361614,1.0275645,1.0170226,1.004051,0.9880613,0.9683491,0.9440883,0.91433525,0.87805337,0.8341687,0.78166956,0.7197691,0.6481373,0.56720513,0.47851008,0.38501465,0.29128066,0.2033534,0.12822732,0.07287296,0.042982273,0.042982273,0.07287296,0.12822732,0.2033534,0.29128066,0.38501465,0.47851008,0.56720513,0.6481373,0.7197691,0.78166956,0.8341687,0.87805337,0.91433525,0.9440883,0.9683491,0.9880613,1.004051,1.0170226,1.0275645,1.0361614,1.043208,1.049023,1.0538622,1.0579299,1.0613886,1.0643673,1.0669678,1.069271,1.07134,1.073225,1.0749655,1.0765924,1.0781302,1.079598,1.0810107,1.0823803,1.083716,1.0850252,1.0863134,1.0875852,1.0888442,1.0900931,1.0913341,1.0925689,1.0937988,1.0950248,1.0962479,1.0974686,1.0986875],[1.1051711,1.1039522,1.1027315,1.1015084,1.1002823,1.0990524,1.0978177,1.0965767,1.0953277,1.0940688,1.0927969,1.0915087,1.0901996,1.0888638,1.0874943,1.0860815,1.0846138,1.083076,1.081449,1.0797086,1.0778235,1.0757545,1.0734514,1.0708508,1.0678722,1.0644134,1.0603458,1.0555066,1.0496916,1.042645,1.0340481,1.0235062,1.0105345,0.99454486,0.97483265,0.9505718,0.9208188,0.88453686,0.8406522,0.78815305,0.7262527,0.65462077,0.5736886,0.48499364,0.3914982,0.2977642,0.20983696,0.13471088,0.07935651,0.049465824,0.049465824,0.07935651,0.13471088,0.20983696,0.2977642,0.3914982,0.48499364,0.5736886,0.65462077,0.7262527,0.78815305,0.8406522,0.88453686,0.9208188,0.9505718,0.97483265,0.99454486,1.0105345,1.0235062,1.0340481,1.042645,1.0496916,1.0555066,1.0603458,1.0644134,1.0678722,1.0708508,1.0734514,1.0757545,1.0778235,1.0797086,1.081449,1.083076,1.0846138,1.0860815,1.0874943,1.0888638,1.0901996,1.0915087,1.0927969,1.0940688,1.0953277,1.0965767,1.0978177,1.0990524,1.1002823,1.1015084,1.1027315,1.1039522,1.1051711],[1.1126809,1.111462,1.1102413,1.1090182,1.1077921,1.1065623,1.1053275,1.1040865,1.1028376,1.1015786,1.1003067,1.0990186,1.0977094,1.0963737,1.0950041,1.0935913,1.0921236,1.0905858,1.0889589,1.0872184,1.0853333,1.0832644,1.0809612,1.0783607,1.075382,1.0719233,1.0678556,1.0630164,1.0572014,1.0501549,1.041558,1.0310161,1.0180445,1.0020547,0.98234254,0.9580817,0.9283287,0.8920468,0.8481621,0.795663,0.73376256,0.6621307,0.5811986,0.49250352,0.3990081,0.3052741,0.21734686,0.14222077,0.08686641,0.056975722,0.056975722,0.08686641,0.14222077,0.21734686,0.3052741,0.3990081,0.49250352,0.5811986,0.6621307,0.73376256,0.795663,0.8481621,0.8920468,0.9283287,0.9580817,0.98234254,1.0020547,1.0180445,1.0310161,1.041558,1.0501549,1.0572014,1.0630164,1.0678556,1.0719233,1.075382,1.0783607,1.0809612,1.0832644,1.0853333,1.0872184,1.0889589,1.0905858,1.0921236,1.0935913,1.0950041,1.0963737,1.0977094,1.0990186,1.1003067,1.1015786,1.1028376,1.1040865,1.1053275,1.1065623,1.1077921,1.1090182,1.1102413,1.111462,1.1126809],[1.1213585,1.1201396,1.1189189,1.1176958,1.1164697,1.1152399,1.1140051,1.1127641,1.1115152,1.1102562,1.1089844,1.1076962,1.106387,1.1050513,1.1036817,1.1022689,1.1008012,1.0992634,1.0976365,1.095896,1.094011,1.091942,1.0896388,1.0870383,1.0840596,1.0806009,1.0765332,1.071694,1.065879,1.0588324,1.0502355,1.0396936,1.026722,1.0107323,0.9910201,0.96675926,0.93700624,0.90072435,0.85683966,0.80434054,0.7424401,0.67080826,0.5898761,0.50118107,0.40768564,0.31395164,0.2260244,0.15089831,0.09554395,0.065653265,0.065653265,0.09554395,0.15089831,0.2260244,0.31395164,0.40768564,0.50118107,0.5898761,0.67080826,0.7424401,0.80434054,0.85683966,0.90072435,0.93700624,0.96675926,0.9910201,1.0107323,1.026722,1.0396936,1.0502355,1.0588324,1.065879,1.071694,1.0765332,1.0806009,1.0840596,1.0870383,1.0896388,1.091942,1.094011,1.095896,1.0976365,1.0992634,1.1008012,1.1022689,1.1036817,1.1050513,1.106387,1.1076962,1.1089844,1.1102562,1.1115152,1.1127641,1.1140051,1.1152399,1.1164697,1.1176958,1.1189189,1.1201396,1.1213585],[1.1313571,1.1301382,1.1289175,1.1276944,1.1264683,1.1252384,1.1240036,1.1227627,1.1215137,1.1202548,1.1189829,1.1176947,1.1163856,1.1150498,1.1136802,1.1122675,1.1107998,1.109262,1.107635,1.1058946,1.1040095,1.1019405,1.0996374,1.0970368,1.0940582,1.0905994,1.0865318,1.0816926,1.0758775,1.068831,1.0602341,1.0496922,1.0367205,1.0207309,1.0010186,0.9767578,0.9470048,0.9107229,0.8668382,0.8143391,0.75243866,0.6808068,0.5998747,0.5111796,0.4176842,0.32395023,0.23602296,0.16089687,0.10554251,0.07565183,0.07565183,0.10554251,0.16089687,0.23602296,0.32395023,0.4176842,0.5111796,0.5998747,0.6808068,0.75243866,0.8143391,0.8668382,0.9107229,0.9470048,0.9767578,1.0010186,1.0207309,1.0367205,1.0496922,1.0602341,1.068831,1.0758775,1.0816926,1.0865318,1.0905994,1.0940582,1.0970368,1.0996374,1.1019405,1.1040095,1.1058946,1.107635,1.109262,1.1107998,1.1122675,1.1136802,1.1150498,1.1163856,1.1176947,1.1189829,1.1202548,1.1215137,1.1227627,1.1240036,1.1252384,1.1264683,1.1276944,1.1289175,1.1301382,1.1313571],[1.1428405,1.1416216,1.1404009,1.1391778,1.1379517,1.1367218,1.1354871,1.1342461,1.1329972,1.1317382,1.1304663,1.1291782,1.127869,1.1265333,1.1251637,1.1237509,1.1222832,1.1207454,1.1191185,1.117378,1.1154929,1.113424,1.1111208,1.1085203,1.1055416,1.1020828,1.0980152,1.093176,1.087361,1.0803144,1.0717175,1.0611756,1.048204,1.0322143,1.0125021,0.98824126,0.9584882,0.92220634,0.87832165,0.82582253,0.7639221,0.69229025,0.6113581,0.5226631,0.42916766,0.33543366,0.24750641,0.17238033,0.11702596,0.08713528,0.08713528,0.11702596,0.17238033,0.24750641,0.33543366,0.42916766,0.5226631,0.6113581,0.69229025,0.7639221,0.82582253,0.87832165,0.92220634,0.9584882,0.98824126,1.0125021,1.0322143,1.048204,1.0611756,1.0717175,1.0803144,1.087361,1.093176,1.0980152,1.1020828,1.1055416,1.1085203,1.1111208,1.113424,1.1154929,1.117378,1.1191185,1.1207454,1.1222832,1.1237509,1.1251637,1.1265333,1.127869,1.1291782,1.1304663,1.1317382,1.1329972,1.1342461,1.1354871,1.1367218,1.1379517,1.1391778,1.1404009,1.1416216,1.1428405],[1.1559803,1.1547614,1.1535407,1.1523176,1.1510916,1.1498617,1.1486269,1.147386,1.146137,1.144878,1.1436062,1.142318,1.1410089,1.1396731,1.1383035,1.1368908,1.1354231,1.1338853,1.1322583,1.1305178,1.1286328,1.1265638,1.1242607,1.1216601,1.1186814,1.1152227,1.111155,1.1063159,1.1005008,1.0934542,1.0848573,1.0743154,1.0613438,1.0453541,1.0256419,1.001381,0.97162807,0.9353461,0.8914615,0.8389623,0.77706194,0.70543003,0.6244979,0.5358029,0.44230747,0.34857348,0.26064622,0.18552014,0.13016577,0.10027509,0.10027509,0.13016577,0.18552014,0.26064622,0.34857348,0.44230747,0.5358029,0.6244979,0.70543003,0.77706194,0.8389623,0.8914615,0.9353461,0.97162807,1.001381,1.0256419,1.0453541,1.0613438,1.0743154,1.0848573,1.0934542,1.1005008,1.1063159,1.111155,1.1152227,1.1186814,1.1216601,1.1242607,1.1265638,1.1286328,1.1305178,1.1322583,1.1338853,1.1354231,1.1368908,1.1383035,1.1396731,1.1410089,1.142318,1.1436062,1.144878,1.146137,1.147386,1.1486269,1.1498617,1.1510916,1.1523176,1.1535407,1.1547614,1.1559803],[1.1709516,1.1697327,1.168512,1.1672889,1.1660628,1.164833,1.1635982,1.1623572,1.1611083,1.1598493,1.1585774,1.1572893,1.1559801,1.1546444,1.1532748,1.151862,1.1503943,1.1488565,1.1472296,1.1454891,1.143604,1.141535,1.1392319,1.1366314,1.1336527,1.130194,1.1261263,1.1212871,1.1154721,1.1084255,1.0998286,1.0892867,1.076315,1.0603254,1.0406132,1.0163523,0.9865993,0.95031744,0.90643275,0.85393363,0.7920332,0.72040135,0.6394692,0.55077416,0.45727873,0.36354476,0.27561748,0.2004914,0.14513704,0.11524636,0.11524636,0.14513704,0.2004914,0.27561748,0.36354476,0.45727873,0.55077416,0.6394692,0.72040135,0.7920332,0.85393363,0.90643275,0.95031744,0.9865993,1.0163523,1.0406132,1.0603254,1.076315,1.0892867,1.0998286,1.1084255,1.1154721,1.1212871,1.1261263,1.130194,1.1336527,1.1366314,1.1392319,1.141535,1.143604,1.1454891,1.1472296,1.1488565,1.1503943,1.151862,1.1532748,1.1546444,1.1559801,1.1572893,1.1585774,1.1598493,1.1611083,1.1623572,1.1635982,1.164833,1.1660628,1.1672889,1.168512,1.1697327,1.1709516],[1.1879271,1.1867082,1.1854875,1.1842644,1.1830384,1.1818085,1.1805737,1.1793327,1.1780838,1.1768248,1.175553,1.1742648,1.1729556,1.1716199,1.1702503,1.1688375,1.1673698,1.165832,1.1642051,1.1624646,1.1605796,1.1585106,1.1562074,1.1536069,1.1506282,1.1471695,1.1431018,1.1382626,1.1324476,1.125401,1.1168041,1.1062622,1.0932906,1.0773009,1.0575887,1.0333278,1.0035748,0.9672929,0.92340827,0.8709091,0.8090087,0.7373768,0.65644467,0.5677497,0.47425425,0.38052025,0.292593,0.21746692,0.16211255,0.13222186,0.13222186,0.16211255,0.21746692,0.292593,0.38052025,0.47425425,0.5677497,0.65644467,0.7373768,0.8090087,0.8709091,0.92340827,0.9672929,1.0035748,1.0333278,1.0575887,1.0773009,1.0932906,1.1062622,1.1168041,1.125401,1.1324476,1.1382626,1.1431018,1.1471695,1.1506282,1.1536069,1.1562074,1.1585106,1.1605796,1.1624646,1.1642051,1.165832,1.1673698,1.1688375,1.1702503,1.1716199,1.1729556,1.1742648,1.175553,1.1768248,1.1780838,1.1793327,1.1805737,1.1818085,1.1830384,1.1842644,1.1854875,1.1867082,1.1879271],[1.2070696,1.2058507,1.20463,1.2034069,1.2021809,1.200951,1.1997162,1.1984752,1.1972263,1.1959673,1.1946955,1.1934073,1.1920981,1.1907624,1.1893928,1.18798,1.1865124,1.1849746,1.1833476,1.1816071,1.1797221,1.1776531,1.17535,1.1727494,1.1697707,1.166312,1.1622443,1.1574051,1.1515901,1.1445435,1.1359466,1.1254047,1.1124331,1.0964434,1.0767312,1.0524703,1.0227174,0.9864354,0.9425508,0.8900516,0.8281512,0.7565193,0.6755872,0.5868922,0.49339676,0.39966276,0.3117355,0.23660943,0.18125506,0.15136437,0.15136437,0.18125506,0.23660943,0.3117355,0.39966276,0.49339676,0.5868922,0.6755872,0.7565193,0.8281512,0.8900516,0.9425508,0.9864354,1.0227174,1.0524703,1.0767312,1.0964434,1.1124331,1.1254047,1.1359466,1.1445435,1.1515901,1.1574051,1.1622443,1.166312,1.1697707,1.1727494,1.17535,1.1776531,1.1797221,1.1816071,1.1833476,1.1849746,1.1865124,1.18798,1.1893928,1.1907624,1.1920981,1.1934073,1.1946955,1.1959673,1.1972263,1.1984752,1.1997162,1.200951,1.2021809,1.2034069,1.20463,1.2058507,1.2070696],[1.2285224,1.2273035,1.2260828,1.2248597,1.2236336,1.2224038,1.221169,1.219928,1.2186791,1.2174201,1.2161483,1.2148601,1.2135509,1.2122152,1.2108456,1.2094328,1.2079651,1.2064273,1.2048004,1.2030599,1.2011749,1.1991059,1.1968027,1.1942022,1.1912235,1.1877648,1.1836971,1.1788579,1.1730429,1.1659964,1.1573995,1.1468576,1.133886,1.1178962,1.098184,1.0739232,1.0441701,1.0078883,0.9640036,0.9115045,0.84960407,0.7779722,0.6970401,0.60834503,0.5148496,0.42111564,0.33318835,0.25806227,0.20270792,0.17281723,0.17281723,0.20270792,0.25806227,0.33318835,0.42111564,0.5148496,0.60834503,0.6970401,0.7779722,0.84960407,0.9115045,0.9640036,1.0078883,1.0441701,1.0739232,1.098184,1.1178962,1.133886,1.1468576,1.1573995,1.1659964,1.1730429,1.1788579,1.1836971,1.1877648,1.1912235,1.1942022,1.1968027,1.1991059,1.2011749,1.2030599,1.2048004,1.2064273,1.2079651,1.2094328,1.2108456,1.2122152,1.2135509,1.2148601,1.2161483,1.2174201,1.2186791,1.219928,1.221169,1.2224038,1.2236336,1.2248597,1.2260828,1.2273035,1.2285224],[1.2523983,1.2511793,1.2499586,1.2487355,1.2475095,1.2462796,1.2450448,1.2438039,1.2425549,1.2412959,1.2400241,1.2387359,1.2374268,1.236091,1.2347214,1.2333087,1.231841,1.2303032,1.2286762,1.2269357,1.2250507,1.2229817,1.2206786,1.218078,1.2150993,1.2116406,1.2075729,1.2027338,1.1969187,1.1898721,1.1812752,1.1707333,1.1577617,1.141772,1.1220598,1.097799,1.068046,1.031764,0.9878794,0.9353802,0.87347984,0.80184793,0.7209158,0.6322208,0.5387254,0.44499138,0.35706413,0.28193805,0.22658367,0.19669299,0.19669299,0.22658367,0.28193805,0.35706413,0.44499138,0.5387254,0.6322208,0.7209158,0.80184793,0.87347984,0.9353802,0.9878794,1.031764,1.068046,1.097799,1.1220598,1.141772,1.1577617,1.1707333,1.1812752,1.1898721,1.1969187,1.2027338,1.2075729,1.2116406,1.2150993,1.218078,1.2206786,1.2229817,1.2250507,1.2269357,1.2286762,1.2303032,1.231841,1.2333087,1.2347214,1.236091,1.2374268,1.2387359,1.2400241,1.2412959,1.2425549,1.2438039,1.2450448,1.2462796,1.2475095,1.2487355,1.2499586,1.2511793,1.2523983],[1.2787662,1.2775472,1.2763265,1.2751034,1.2738774,1.2726475,1.2714127,1.2701718,1.2689228,1.2676638,1.266392,1.2651038,1.2637947,1.2624589,1.2610893,1.2596766,1.2582089,1.2566711,1.2550441,1.2533036,1.2514186,1.2493496,1.2470465,1.2444459,1.2414672,1.2380085,1.2339408,1.2291017,1.2232866,1.21624,1.2076432,1.1971012,1.1841296,1.1681399,1.1484277,1.1241668,1.0944139,1.0581319,1.0142473,0.9617482,0.89984775,0.8282159,0.74728376,0.6585887,0.5650933,0.4713593,0.38343203,0.30830595,0.2529516,0.2230609,0.2230609,0.2529516,0.30830595,0.38343203,0.4713593,0.5650933,0.6585887,0.74728376,0.8282159,0.89984775,0.9617482,1.0142473,1.0581319,1.0944139,1.1241668,1.1484277,1.1681399,1.1841296,1.1971012,1.2076432,1.21624,1.2232866,1.2291017,1.2339408,1.2380085,1.2414672,1.2444459,1.2470465,1.2493496,1.2514186,1.2533036,1.2550441,1.2566711,1.2582089,1.2596766,1.2610893,1.2624589,1.2637947,1.2651038,1.266392,1.2676638,1.2689228,1.2701718,1.2714127,1.2726475,1.2738774,1.2751034,1.2763265,1.2775472,1.2787662],[1.3076391,1.3064202,1.3051995,1.3039764,1.3027503,1.3015205,1.3002857,1.2990447,1.2977958,1.2965368,1.295265,1.2939768,1.2926676,1.2913319,1.2899623,1.2885495,1.2870818,1.285544,1.2839171,1.2821766,1.2802916,1.2782226,1.2759194,1.2733189,1.2703402,1.2668815,1.2628138,1.2579746,1.2521596,1.2451131,1.2365162,1.2259743,1.2130027,1.1970129,1.1773007,1.1530399,1.1232868,1.087005,1.0431203,0.9906212,0.9287208,0.8570889,0.7761568,0.68746173,0.5939663,0.50023234,0.41230506,0.33717898,0.28182462,0.25193393,0.25193393,0.28182462,0.33717898,0.41230506,0.50023234,0.5939663,0.68746173,0.7761568,0.8570889,0.9287208,0.9906212,1.0431203,1.087005,1.1232868,1.1530399,1.1773007,1.1970129,1.2130027,1.2259743,1.2365162,1.2451131,1.2521596,1.2579746,1.2628138,1.2668815,1.2703402,1.2733189,1.2759194,1.2782226,1.2802916,1.2821766,1.2839171,1.285544,1.2870818,1.2885495,1.2899623,1.2913319,1.2926676,1.2939768,1.295265,1.2965368,1.2977958,1.2990447,1.3002857,1.3015205,1.3027503,1.3039764,1.3051995,1.3064202,1.3076391],[1.3389618,1.3377429,1.3365222,1.3352991,1.3340731,1.3328432,1.3316084,1.3303674,1.3291185,1.3278595,1.3265877,1.3252995,1.3239903,1.3226546,1.321285,1.3198723,1.3184046,1.3168668,1.3152398,1.3134993,1.3116143,1.3095453,1.3072422,1.3046416,1.3016629,1.2982042,1.2941365,1.2892973,1.2834823,1.2764357,1.2678388,1.2572969,1.2443253,1.2283356,1.2086234,1.1843625,1.1546096,1.1183276,1.074443,1.0219438,0.96004343,0.8884115,0.8074794,0.7187844,0.62528896,0.53155494,0.44362772,0.36850163,0.31314728,0.2832566,0.2832566,0.31314728,0.36850163,0.44362772,0.53155494,0.62528896,0.7187844,0.8074794,0.8884115,0.96004343,1.0219438,1.074443,1.1183276,1.1546096,1.1843625,1.2086234,1.2283356,1.2443253,1.2572969,1.2678388,1.2764357,1.2834823,1.2892973,1.2941365,1.2982042,1.3016629,1.3046416,1.3072422,1.3095453,1.3116143,1.3134993,1.3152398,1.3168668,1.3184046,1.3198723,1.321285,1.3226546,1.3239903,1.3252995,1.3265877,1.3278595,1.3291185,1.3303674,1.3316084,1.3328432,1.3340731,1.3352991,1.3365222,1.3377429,1.3389618],[1.3725997,1.3713808,1.3701601,1.368937,1.367711,1.3664811,1.3652463,1.3640053,1.3627564,1.3614974,1.3602256,1.3589374,1.3576282,1.3562925,1.3549229,1.3535101,1.3520424,1.3505046,1.3488777,1.3471372,1.3452522,1.3431832,1.34088,1.3382795,1.3353008,1.3318421,1.3277744,1.3229352,1.3171202,1.3100736,1.3014767,1.2909348,1.2779632,1.2619735,1.2422613,1.2180004,1.1882474,1.1519656,1.1080809,1.0555818,0.9936813,0.92204946,0.8411173,0.75242233,0.65892684,0.5651929,0.47726563,0.40213954,0.3467852,0.3168945,0.3168945,0.3467852,0.40213954,0.47726563,0.5651929,0.65892684,0.75242233,0.8411173,0.92204946,0.9936813,1.0555818,1.1080809,1.1519656,1.1882474,1.2180004,1.2422613,1.2619735,1.2779632,1.2909348,1.3014767,1.3100736,1.3171202,1.3229352,1.3277744,1.3318421,1.3353008,1.3382795,1.34088,1.3431832,1.3452522,1.3471372,1.3488777,1.3505046,1.3520424,1.3535101,1.3549229,1.3562925,1.3576282,1.3589374,1.3602256,1.3614974,1.3627564,1.3640053,1.3652463,1.3664811,1.367711,1.368937,1.3701601,1.3713808,1.3725997],[1.4083338,1.407115,1.4058943,1.4046712,1.403445,1.4022152,1.4009805,1.3997395,1.3984904,1.3972316,1.3959596,1.3946714,1.3933623,1.3920267,1.390657,1.3892443,1.3877766,1.3862388,1.3846118,1.3828714,1.3809862,1.3789172,1.3766141,1.3740137,1.3710349,1.3675761,1.3635085,1.3586693,1.3528543,1.3458078,1.3372109,1.326669,1.3136973,1.2977076,1.2779953,1.2537346,1.2239816,1.1876997,1.143815,1.0913159,1.0294154,0.9577836,0.87685144,0.7881564,0.694661,0.600927,0.5129998,0.43787366,0.3825193,0.35262862,0.35262862,0.3825193,0.43787366,0.5129998,0.600927,0.694661,0.7881564,0.87685144,0.9577836,1.0294154,1.0913159,1.143815,1.1876997,1.2239816,1.2537346,1.2779953,1.2977076,1.3136973,1.326669,1.3372109,1.3458078,1.3528543,1.3586693,1.3635085,1.3675761,1.3710349,1.3740137,1.3766141,1.3789172,1.3809862,1.3828714,1.3846118,1.3862388,1.3877766,1.3892443,1.390657,1.3920267,1.3933623,1.3946714,1.3959596,1.3972316,1.3984904,1.3997395,1.4009805,1.4022152,1.403445,1.4046712,1.4058943,1.407115,1.4083338],[1.4458596,1.4446406,1.4434199,1.4421968,1.4409708,1.4397409,1.4385061,1.4372652,1.4360162,1.4347572,1.4334854,1.4321972,1.430888,1.4295523,1.4281827,1.42677,1.4253023,1.4237645,1.4221375,1.420397,1.418512,1.416443,1.4141399,1.4115393,1.4085606,1.4051019,1.4010342,1.396195,1.39038,1.3833334,1.3747365,1.3641946,1.351223,1.3352333,1.3155211,1.2912602,1.2615073,1.2252253,1.1813407,1.1288415,1.0669411,0.99530923,0.9143771,0.8256821,0.7321867,0.63845265,0.5505254,0.47539935,0.420045,0.3901543,0.3901543,0.420045,0.47539935,0.5505254,0.63845265,0.7321867,0.8256821,0.9143771,0.99530923,1.0669411,1.1288415,1.1813407,1.2252253,1.2615073,1.2912602,1.3155211,1.3352333,1.351223,1.3641946,1.3747365,1.3833334,1.39038,1.396195,1.4010342,1.4051019,1.4085606,1.4115393,1.4141399,1.416443,1.418512,1.420397,1.4221375,1.4237645,1.4253023,1.42677,1.4281827,1.4295523,1.430888,1.4321972,1.4334854,1.4347572,1.4360162,1.4372652,1.4385061,1.4397409,1.4409708,1.4421968,1.4434199,1.4446406,1.4458596],[1.4847922,1.4835733,1.4823526,1.4811295,1.4799035,1.4786736,1.4774388,1.4761978,1.4749489,1.4736899,1.4724181,1.4711299,1.4698207,1.468485,1.4671154,1.4657027,1.464235,1.4626971,1.4610702,1.4593297,1.4574447,1.4553757,1.4530725,1.450472,1.4474933,1.4440346,1.4399669,1.4351277,1.4293127,1.4222662,1.4136693,1.4031274,1.3901558,1.374166,1.3544538,1.330193,1.30044,1.2641581,1.2202734,1.1677743,1.1058738,1.034242,0.9533099,0.86461484,0.7711194,0.67738545,0.58945817,0.51433206,0.45897773,0.42908704,0.42908704,0.45897773,0.51433206,0.58945817,0.67738545,0.7711194,0.86461484,0.9533099,1.034242,1.1058738,1.1677743,1.2202734,1.2641581,1.30044,1.330193,1.3544538,1.374166,1.3901558,1.4031274,1.4136693,1.4222662,1.4293127,1.4351277,1.4399669,1.4440346,1.4474933,1.450472,1.4530725,1.4553757,1.4574447,1.4593297,1.4610702,1.4626971,1.464235,1.4657027,1.4671154,1.468485,1.4698207,1.4711299,1.4724181,1.4736899,1.4749489,1.4761978,1.4774388,1.4786736,1.4799035,1.4811295,1.4823526,1.4835733,1.4847922],[1.5246806,1.5234617,1.522241,1.5210179,1.5197918,1.518562,1.5173272,1.5160862,1.5148373,1.5135783,1.5123065,1.5110183,1.5097091,1.5083734,1.5070038,1.505591,1.5041233,1.5025855,1.5009586,1.4992181,1.497333,1.495264,1.4929609,1.4903604,1.4873817,1.483923,1.4798553,1.4750161,1.4692011,1.4621546,1.4535577,1.4430158,1.4300442,1.4140544,1.3943422,1.3700814,1.3403283,1.3040464,1.2601618,1.2076626,1.1457622,1.0741303,0.9931982,0.9045032,0.81100774,0.7172738,0.6293465,0.55422044,0.49886608,0.4689754,0.4689754,0.49886608,0.55422044,0.6293465,0.7172738,0.81100774,0.9045032,0.9931982,1.0741303,1.1457622,1.2076626,1.2601618,1.3040464,1.3403283,1.3700814,1.3943422,1.4140544,1.4300442,1.4430158,1.4535577,1.4621546,1.4692011,1.4750161,1.4798553,1.483923,1.4873817,1.4903604,1.4929609,1.495264,1.497333,1.4992181,1.5009586,1.5025855,1.5041233,1.505591,1.5070038,1.5083734,1.5097091,1.5110183,1.5123065,1.5135783,1.5148373,1.5160862,1.5173272,1.518562,1.5197918,1.5210179,1.522241,1.5234617,1.5246806],[1.5650257,1.5638068,1.5625861,1.561363,1.5601369,1.558907,1.5576723,1.5564313,1.5551823,1.5539234,1.5526515,1.5513633,1.5500542,1.5487185,1.5473489,1.5459361,1.5444684,1.5429306,1.5413036,1.5395632,1.5376781,1.5356091,1.533306,1.5307055,1.5277268,1.524268,1.5202004,1.5153612,1.5095462,1.5024996,1.4939027,1.4833608,1.4703891,1.4543995,1.4346873,1.4104264,1.3806734,1.3443916,1.3005068,1.2480078,1.1861073,1.1144755,1.0335433,0.9448483,0.8513528,0.75761884,0.66969156,0.5945655,0.53921115,0.50932044,0.50932044,0.53921115,0.5945655,0.66969156,0.75761884,0.8513528,0.9448483,1.0335433,1.1144755,1.1861073,1.2480078,1.3005068,1.3443916,1.3806734,1.4104264,1.4346873,1.4543995,1.4703891,1.4833608,1.4939027,1.5024996,1.5095462,1.5153612,1.5202004,1.524268,1.5277268,1.5307055,1.533306,1.5356091,1.5376781,1.5395632,1.5413036,1.5429306,1.5444684,1.5459361,1.5473489,1.5487185,1.5500542,1.5513633,1.5526515,1.5539234,1.5551823,1.5564313,1.5576723,1.558907,1.5601369,1.561363,1.5625861,1.5638068,1.5650257],[1.6053052,1.6040862,1.6028655,1.6016424,1.6004164,1.5991864,1.5979517,1.5967107,1.5954618,1.5942028,1.592931,1.5916429,1.5903337,1.5889978,1.5876284,1.5862155,1.5847478,1.58321,1.581583,1.5798426,1.5779576,1.5758886,1.5735855,1.5709848,1.5680063,1.5645475,1.5604799,1.5556407,1.5498257,1.5427791,1.5341822,1.5236403,1.5106686,1.494679,1.4749668,1.4507059,1.4209528,1.384671,1.3407862,1.2882872,1.2263868,1.1547549,1.0738227,0.9851277,0.8916323,0.7978983,0.7099711,0.63484496,0.5794906,0.5495999,0.5495999,0.5794906,0.63484496,0.7099711,0.7978983,0.8916323,0.9851277,1.0738227,1.1547549,1.2263868,1.2882872,1.3407862,1.384671,1.4209528,1.4507059,1.4749668,1.494679,1.5106686,1.5236403,1.5341822,1.5427791,1.5498257,1.5556407,1.5604799,1.5645475,1.5680063,1.5709848,1.5735855,1.5758886,1.5779576,1.5798426,1.581583,1.58321,1.5847478,1.5862155,1.5876284,1.5889978,1.5903337,1.5916429,1.592931,1.5942028,1.5954618,1.5967107,1.5979517,1.5991864,1.6004164,1.6016424,1.6028655,1.6040862,1.6053052],[1.645,1.643781,1.6425602,1.6413372,1.6401112,1.6388812,1.6376464,1.6364055,1.6351566,1.6338975,1.6326258,1.6313376,1.6300285,1.6286926,1.6273232,1.6259103,1.6244426,1.6229048,1.6212778,1.6195374,1.6176524,1.6155834,1.6132803,1.6106796,1.6077011,1.6042423,1.6001747,1.5953355,1.5895205,1.5824739,1.573877,1.5633351,1.5503634,1.5343738,1.5146616,1.4904007,1.4606476,1.4243658,1.380481,1.327982,1.2660816,1.1944497,1.1135175,1.0248225,0.9313271,0.8375931,0.74966586,0.67453974,0.6191854,0.5892947,0.5892947,0.6191854,0.67453974,0.74966586,0.8375931,0.9313271,1.0248225,1.1135175,1.1944497,1.2660816,1.327982,1.380481,1.4243658,1.4606476,1.4904007,1.5146616,1.5343738,1.5503634,1.5633351,1.573877,1.5824739,1.5895205,1.5953355,1.6001747,1.6042423,1.6077011,1.6106796,1.6132803,1.6155834,1.6176524,1.6195374,1.6212778,1.6229048,1.6244426,1.6259103,1.6273232,1.6286926,1.6300285,1.6313376,1.6326258,1.6338975,1.6351566,1.6364055,1.6376464,1.6388812,1.6401112,1.6413372,1.6425602,1.643781,1.645],[1.6836209,1.6824019,1.6811812,1.6799581,1.6787322,1.6775022,1.6762674,1.6750264,1.6737776,1.6725185,1.6712468,1.6699586,1.6686494,1.6673136,1.6659441,1.6645312,1.6630635,1.6615257,1.6598988,1.6581583,1.6562734,1.6542044,1.6519012,1.6493006,1.646322,1.6428633,1.6387956,1.6339564,1.6281414,1.6210948,1.6124979,1.601956,1.5889844,1.5729947,1.5532825,1.5290216,1.4992685,1.4629867,1.419102,1.3666029,1.3047025,1.2330706,1.1521385,1.0634434,0.96994805,0.876214,0.7882868,0.7131607,0.65780634,0.6279156,0.6279156,0.65780634,0.7131607,0.7882868,0.876214,0.96994805,1.0634434,1.1521385,1.2330706,1.3047025,1.3666029,1.419102,1.4629867,1.4992685,1.5290216,1.5532825,1.5729947,1.5889844,1.601956,1.6124979,1.6210948,1.6281414,1.6339564,1.6387956,1.6428633,1.646322,1.6493006,1.6519012,1.6542044,1.6562734,1.6581583,1.6598988,1.6615257,1.6630635,1.6645312,1.6659441,1.6673136,1.6686494,1.6699586,1.6712468,1.6725185,1.6737776,1.6750264,1.6762674,1.6775022,1.6787322,1.6799581,1.6811812,1.6824019,1.6836209],[1.7207317,1.7195128,1.7182921,1.717069,1.715843,1.7146131,1.7133783,1.7121373,1.7108884,1.7096294,1.7083576,1.7070694,1.7057602,1.7044245,1.7030549,1.7016422,1.7001745,1.6986367,1.6970097,1.6952692,1.6933842,1.6913152,1.689012,1.6864115,1.6834328,1.6799741,1.6759064,1.6710672,1.6652522,1.6582057,1.6496089,1.6390669,1.6260953,1.6101055,1.5903933,1.5661325,1.5363795,1.5000975,1.4562129,1.4037137,1.3418133,1.2701814,1.1892493,1.1005543,1.0070589,0.9133249,0.8253976,0.75027156,0.6949172,0.6650265,0.6650265,0.6949172,0.75027156,0.8253976,0.9133249,1.0070589,1.1005543,1.1892493,1.2701814,1.3418133,1.4037137,1.4562129,1.5000975,1.5363795,1.5661325,1.5903933,1.6101055,1.6260953,1.6390669,1.6496089,1.6582057,1.6652522,1.6710672,1.6759064,1.6799741,1.6834328,1.6864115,1.689012,1.6913152,1.6933842,1.6952692,1.6970097,1.6986367,1.7001745,1.7016422,1.7030549,1.7044245,1.7057602,1.7070694,1.7083576,1.7096294,1.7108884,1.7121373,1.7133783,1.7146131,1.715843,1.717069,1.7182921,1.7195128,1.7207317],[1.7559671,1.7547482,1.7535275,1.7523044,1.7510784,1.7498485,1.7486137,1.7473727,1.7461238,1.7448648,1.743593,1.7423048,1.7409956,1.7396599,1.7382903,1.7368776,1.7354099,1.733872,1.7322451,1.7305046,1.7286196,1.7265506,1.7242475,1.7216469,1.7186682,1.7152095,1.7111418,1.7063026,1.7004876,1.6934412,1.6848443,1.6743023,1.6613307,1.6453409,1.6256287,1.601368,1.5716149,1.5353329,1.4914483,1.4389491,1.3770487,1.3054168,1.2244847,1.1357898,1.0422943,0.9485603,0.860633,0.78550696,0.7301526,0.7002619,0.7002619,0.7301526,0.78550696,0.860633,0.9485603,1.0422943,1.1357898,1.2244847,1.3054168,1.3770487,1.4389491,1.4914483,1.5353329,1.5716149,1.601368,1.6256287,1.6453409,1.6613307,1.6743023,1.6848443,1.6934412,1.7004876,1.7063026,1.7111418,1.7152095,1.7186682,1.7216469,1.7242475,1.7265506,1.7286196,1.7305046,1.7322451,1.733872,1.7354099,1.7368776,1.7382903,1.7396599,1.7409956,1.7423048,1.743593,1.7448648,1.7461238,1.7473727,1.7486137,1.7498485,1.7510784,1.7523044,1.7535275,1.7547482,1.7559671],[1.7890434,1.7878244,1.7866037,1.7853806,1.7841547,1.7829247,1.7816899,1.7804489,1.7792001,1.777941,1.7766693,1.7753811,1.7740719,1.7727361,1.7713666,1.7699537,1.768486,1.7669482,1.7653213,1.7635808,1.7616959,1.7596269,1.7573237,1.7547231,1.7517445,1.7482858,1.7442181,1.7393789,1.7335639,1.7265173,1.7179204,1.7073785,1.6944069,1.6784172,1.658705,1.6344441,1.604691,1.5684092,1.5245245,1.4720254,1.410125,1.3384931,1.257561,1.1688659,1.0753706,0.9816365,0.8937093,0.8185832,0.76322883,0.7333381,0.7333381,0.76322883,0.8185832,0.8937093,0.9816365,1.0753706,1.1688659,1.257561,1.3384931,1.410125,1.4720254,1.5245245,1.5684092,1.604691,1.6344441,1.658705,1.6784172,1.6944069,1.7073785,1.7179204,1.7265173,1.7335639,1.7393789,1.7442181,1.7482858,1.7517445,1.7547231,1.7573237,1.7596269,1.7616959,1.7635808,1.7653213,1.7669482,1.768486,1.7699537,1.7713666,1.7727361,1.7740719,1.7753811,1.7766693,1.777941,1.7792001,1.7804489,1.7816899,1.7829247,1.7841547,1.7853806,1.7866037,1.7878244,1.7890434],[1.819763,1.818544,1.8173233,1.8161002,1.8148742,1.8136443,1.8124095,1.8111686,1.8099196,1.8086606,1.8073888,1.8061006,1.8047915,1.8034557,1.8020861,1.8006734,1.7992057,1.7976679,1.7960409,1.7943004,1.7924154,1.7903464,1.7880433,1.7854427,1.782464,1.7790053,1.7749376,1.7700984,1.7642834,1.757237,1.7486401,1.7380981,1.7251265,1.7091367,1.6894245,1.6651638,1.6354107,1.5991287,1.5552441,1.5027449,1.4408445,1.3692126,1.2882805,1.1995856,1.1060901,1.012356,0.9244288,0.84930277,0.7939484,0.7640577,0.7640577,0.7939484,0.84930277,0.9244288,1.012356,1.1060901,1.1995856,1.2882805,1.3692126,1.4408445,1.5027449,1.5552441,1.5991287,1.6354107,1.6651638,1.6894245,1.7091367,1.7251265,1.7380981,1.7486401,1.757237,1.7642834,1.7700984,1.7749376,1.7790053,1.782464,1.7854427,1.7880433,1.7903464,1.7924154,1.7943004,1.7960409,1.7976679,1.7992057,1.8006734,1.8020861,1.8034557,1.8047915,1.8061006,1.8073888,1.8086606,1.8099196,1.8111686,1.8124095,1.8136443,1.8148742,1.8161002,1.8173233,1.818544,1.819763],[1.8480121,1.8467932,1.8455725,1.8443494,1.8431233,1.8418934,1.8406587,1.8394177,1.8381687,1.8369098,1.8356379,1.8343498,1.8330406,1.8317049,1.8303353,1.8289225,1.8274548,1.825917,1.82429,1.8225496,1.8206645,1.8185955,1.8162924,1.8136919,1.8107132,1.8072544,1.8031868,1.7983476,1.7925326,1.785486,1.7768891,1.7663472,1.7533755,1.7373859,1.7176737,1.6934128,1.6636598,1.627378,1.5834932,1.5309942,1.4690937,1.3974619,1.3165298,1.2278347,1.1343392,1.0406053,0.95267797,0.8775519,0.82219756,0.79230684,0.79230684,0.82219756,0.8775519,0.95267797,1.0406053,1.1343392,1.2278347,1.3165298,1.3974619,1.4690937,1.5309942,1.5834932,1.627378,1.6636598,1.6934128,1.7176737,1.7373859,1.7533755,1.7663472,1.7768891,1.785486,1.7925326,1.7983476,1.8031868,1.8072544,1.8107132,1.8136919,1.8162924,1.8185955,1.8206645,1.8225496,1.82429,1.825917,1.8274548,1.8289225,1.8303353,1.8317049,1.8330406,1.8343498,1.8356379,1.8369098,1.8381687,1.8394177,1.8406587,1.8418934,1.8431233,1.8443494,1.8455725,1.8467932,1.8480121],[1.8737533,1.8725343,1.8713136,1.8700905,1.8688645,1.8676345,1.8663998,1.8651588,1.86391,1.8626509,1.8613791,1.860091,1.8587818,1.857446,1.8560765,1.8546636,1.8531959,1.8516581,1.8500311,1.8482907,1.8464057,1.8443367,1.8420336,1.839433,1.8364544,1.8329957,1.828928,1.8240888,1.8182738,1.8112272,1.8026303,1.7920884,1.7791167,1.7631271,1.7434149,1.719154,1.6894009,1.6531191,1.6092343,1.5567353,1.4948349,1.423203,1.3422709,1.2535758,1.1600804,1.0663464,0.9784192,0.9032931,0.8479387,0.818048,0.818048,0.8479387,0.9032931,0.9784192,1.0663464,1.1600804,1.2535758,1.3422709,1.423203,1.4948349,1.5567353,1.6092343,1.6531191,1.6894009,1.719154,1.7434149,1.7631271,1.7791167,1.7920884,1.8026303,1.8112272,1.8182738,1.8240888,1.828928,1.8329957,1.8364544,1.839433,1.8420336,1.8443367,1.8464057,1.8482907,1.8500311,1.8516581,1.8531959,1.8546636,1.8560765,1.857446,1.8587818,1.860091,1.8613791,1.8626509,1.86391,1.8651588,1.8663998,1.8676345,1.8688645,1.8700905,1.8713136,1.8725343,1.8737533],[1.897015,1.8957961,1.8945754,1.8933523,1.8921262,1.8908963,1.8896616,1.8884206,1.8871716,1.8859127,1.8846408,1.8833526,1.8820435,1.8807077,1.8793381,1.8779254,1.8764577,1.8749199,1.8732929,1.8715525,1.8696674,1.8675984,1.8652953,1.8626947,1.859716,1.8562573,1.8521897,1.8473505,1.8415354,1.8344889,1.825892,1.81535,1.8023784,1.7863888,1.7666765,1.7424157,1.7126627,1.6763809,1.6324961,1.5799971,1.5180966,1.4464648,1.3655326,1.2768376,1.1833421,1.0896082,1.0016809,0.9265548,0.87120044,0.8413097,0.8413097,0.87120044,0.9265548,1.0016809,1.0896082,1.1833421,1.2768376,1.3655326,1.4464648,1.5180966,1.5799971,1.6324961,1.6763809,1.7126627,1.7424157,1.7666765,1.7863888,1.8023784,1.81535,1.825892,1.8344889,1.8415354,1.8473505,1.8521897,1.8562573,1.859716,1.8626947,1.8652953,1.8675984,1.8696674,1.8715525,1.8732929,1.8749199,1.8764577,1.8779254,1.8793381,1.8807077,1.8820435,1.8833526,1.8846408,1.8859127,1.8871716,1.8884206,1.8896616,1.8908963,1.8921262,1.8933523,1.8945754,1.8957961,1.897015],[1.9178782,1.9166594,1.9154387,1.9142156,1.9129894,1.9117596,1.9105248,1.9092839,1.9080348,1.906776,1.905504,1.9042158,1.9029067,1.901571,1.9002013,1.8987887,1.897321,1.8957832,1.8941562,1.8924158,1.8905306,1.8884616,1.8861585,1.883558,1.8805792,1.8771205,1.8730528,1.8682137,1.8623986,1.8553522,1.8467553,1.8362134,1.8232417,1.8072519,1.7875397,1.763279,1.733526,1.697244,1.6533594,1.6008602,1.5389597,1.467328,1.3863958,1.2977008,1.2042054,1.1104714,1.0225441,0.94741803,0.8920637,0.86217296,0.86217296,0.8920637,0.94741803,1.0225441,1.1104714,1.2042054,1.2977008,1.3863958,1.467328,1.5389597,1.6008602,1.6533594,1.697244,1.733526,1.763279,1.7875397,1.8072519,1.8232417,1.8362134,1.8467553,1.8553522,1.8623986,1.8682137,1.8730528,1.8771205,1.8805792,1.883558,1.8861585,1.8884616,1.8905306,1.8924158,1.8941562,1.8957832,1.897321,1.8987887,1.9002013,1.901571,1.9029067,1.9042158,1.905504,1.906776,1.9080348,1.9092839,1.9105248,1.9117596,1.9129894,1.9142156,1.9154387,1.9166594,1.9178782],[1.9364645,1.9352456,1.9340249,1.9328018,1.9315758,1.9303459,1.9291111,1.9278702,1.9266212,1.9253622,1.9240904,1.9228022,1.921493,1.9201573,1.9187877,1.917375,1.9159073,1.9143695,1.9127425,1.911002,1.909117,1.907048,1.9047449,1.9021443,1.8991656,1.8957069,1.8916392,1.8868,1.880985,1.8739386,1.8653417,1.8547997,1.8418281,1.8258383,1.8061261,1.7818654,1.7521123,1.7158303,1.6719457,1.6194465,1.5575461,1.4859142,1.4049821,1.3162872,1.2227917,1.1290576,1.0411304,0.9660044,0.91065,0.8807593,0.8807593,0.91065,0.9660044,1.0411304,1.1290576,1.2227917,1.3162872,1.4049821,1.4859142,1.5575461,1.6194465,1.6719457,1.7158303,1.7521123,1.7818654,1.8061261,1.8258383,1.8418281,1.8547997,1.8653417,1.8739386,1.880985,1.8868,1.8916392,1.8957069,1.8991656,1.9021443,1.9047449,1.907048,1.909117,1.911002,1.9127425,1.9143695,1.9159073,1.917375,1.9187877,1.9201573,1.921493,1.9228022,1.9240904,1.9253622,1.9266212,1.9278702,1.9291111,1.9303459,1.9315758,1.9328018,1.9340249,1.9352456,1.9364645],[1.9529232,1.9517043,1.9504836,1.9492605,1.9480344,1.9468045,1.9455698,1.9443288,1.9430798,1.9418209,1.940549,1.9392608,1.9379517,1.936616,1.9352463,1.9338336,1.9323659,1.9308281,1.9292011,1.9274607,1.9255756,1.9235066,1.9212035,1.918603,1.9156243,1.9121655,1.9080979,1.9032587,1.8974437,1.8903971,1.8818002,1.8712583,1.8582866,1.842297,1.8225847,1.7983239,1.7685709,1.7322891,1.6884043,1.6359053,1.5740048,1.502373,1.4214408,1.3327458,1.2392503,1.1455164,1.057589,0.982463,0.92710865,0.8972179,0.8972179,0.92710865,0.982463,1.057589,1.1455164,1.2392503,1.3327458,1.4214408,1.502373,1.5740048,1.6359053,1.6884043,1.7322891,1.7685709,1.7983239,1.8225847,1.842297,1.8582866,1.8712583,1.8818002,1.8903971,1.8974437,1.9032587,1.9080979,1.9121655,1.9156243,1.918603,1.9212035,1.9235066,1.9255756,1.9274607,1.9292011,1.9308281,1.9323659,1.9338336,1.9352463,1.936616,1.9379517,1.9392608,1.940549,1.9418209,1.9430798,1.9443288,1.9455698,1.9468045,1.9480344,1.9492605,1.9504836,1.9517043,1.9529232],[1.9674202,1.9662013,1.9649806,1.9637575,1.9625314,1.9613016,1.9600668,1.9588258,1.9575769,1.9563179,1.955046,1.9537579,1.9524487,1.951113,1.9497434,1.9483306,1.9468629,1.9453251,1.9436982,1.9419577,1.9400727,1.9380037,1.9357005,1.9331,1.9301213,1.9266626,1.9225949,1.9177557,1.9119407,1.9048941,1.8962972,1.8857553,1.8727837,1.856794,1.8370818,1.8128209,1.783068,1.7467861,1.7029014,1.6504023,1.5885018,1.51687,1.4359379,1.3472428,1.2537473,1.1600134,1.0720861,0.99696004,0.9416057,0.911715,0.911715,0.9416057,0.99696004,1.0720861,1.1600134,1.2537473,1.3472428,1.4359379,1.51687,1.5885018,1.6504023,1.7029014,1.7467861,1.783068,1.8128209,1.8370818,1.856794,1.8727837,1.8857553,1.8962972,1.9048941,1.9119407,1.9177557,1.9225949,1.9266626,1.9301213,1.9331,1.9357005,1.9380037,1.9400727,1.9419577,1.9436982,1.9453251,1.9468629,1.9483306,1.9497434,1.951113,1.9524487,1.9537579,1.955046,1.9563179,1.9575769,1.9588258,1.9600668,1.9613016,1.9625314,1.9637575,1.9649806,1.9662013,1.9674202],[1.9801296,1.9789107,1.97769,1.9764669,1.9752408,1.974011,1.9727762,1.9715352,1.9702863,1.9690273,1.9677554,1.9664673,1.9651581,1.9638224,1.9624528,1.96104,1.9595723,1.9580345,1.9564075,1.9546671,1.952782,1.950713,1.9484099,1.9458094,1.9428307,1.939372,1.9353043,1.9304651,1.9246501,1.9176035,1.9090066,1.8984647,1.885493,1.8695034,1.8497912,1.8255303,1.7957773,1.7594955,1.7156107,1.6631117,1.6012112,1.5295794,1.4486473,1.3599522,1.2664567,1.1727228,1.0847955,1.0096694,0.95431507,0.92442435,0.92442435,0.95431507,1.0096694,1.0847955,1.1727228,1.2664567,1.3599522,1.4486473,1.5295794,1.6012112,1.6631117,1.7156107,1.7594955,1.7957773,1.8255303,1.8497912,1.8695034,1.885493,1.8984647,1.9090066,1.9176035,1.9246501,1.9304651,1.9353043,1.939372,1.9428307,1.9458094,1.9484099,1.950713,1.952782,1.9546671,1.9564075,1.9580345,1.9595723,1.96104,1.9624528,1.9638224,1.9651581,1.9664673,1.9677554,1.9690273,1.9702863,1.9715352,1.9727762,1.974011,1.9752408,1.9764669,1.97769,1.9789107,1.9801296],[1.991226,1.9900072,1.9887865,1.9875634,1.9863372,1.9851074,1.9838727,1.9826317,1.9813826,1.9801238,1.9788518,1.9775636,1.9762545,1.9749188,1.9735491,1.9721365,1.9706688,1.969131,1.967504,1.9657636,1.9638784,1.9618094,1.9595063,1.9569058,1.953927,1.9504683,1.9464006,1.9415615,1.9357464,1.9287,1.9201031,1.9095612,1.8965895,1.8805997,1.8608875,1.8366268,1.8068738,1.7705919,1.7267072,1.674208,1.6123075,1.5406758,1.4597436,1.3710486,1.2775532,1.1838192,1.095892,1.0207658,0.9654115,0.93552077,0.93552077,0.9654115,1.0207658,1.095892,1.1838192,1.2775532,1.3710486,1.4597436,1.5406758,1.6123075,1.674208,1.7267072,1.7705919,1.8068738,1.8366268,1.8608875,1.8805997,1.8965895,1.9095612,1.9201031,1.9287,1.9357464,1.9415615,1.9464006,1.9504683,1.953927,1.9569058,1.9595063,1.9618094,1.9638784,1.9657636,1.967504,1.969131,1.9706688,1.9721365,1.9735491,1.9749188,1.9762545,1.9775636,1.9788518,1.9801238,1.9813826,1.9826317,1.9838727,1.9851074,1.9863372,1.9875634,1.9887865,1.9900072,1.991226],[2.0008793,1.9996604,1.9984397,1.9972166,1.9959905,1.9947606,1.9935259,1.9922849,1.9910359,1.989777,1.9885051,1.987217,1.9859078,1.984572,1.9832025,1.9817897,1.980322,1.9787842,1.9771572,1.9754168,1.9735317,1.9714627,1.9691596,1.966559,1.9635804,1.9601216,1.956054,1.9512148,1.9453998,1.9383533,1.9297564,1.9192145,1.9062428,1.8902531,1.8705409,1.8462801,1.816527,1.7802451,1.7363604,1.6838613,1.6219609,1.550329,1.4693968,1.3807019,1.2872064,1.1934724,1.1055452,1.0304191,0.97506475,0.94517404,0.94517404,0.97506475,1.0304191,1.1055452,1.1934724,1.2872064,1.3807019,1.4693968,1.550329,1.6219609,1.6838613,1.7363604,1.7802451,1.816527,1.8462801,1.8705409,1.8902531,1.9062428,1.9192145,1.9297564,1.9383533,1.9453998,1.9512148,1.956054,1.9601216,1.9635804,1.966559,1.9691596,1.9714627,1.9735317,1.9754168,1.9771572,1.9787842,1.980322,1.9817897,1.9832025,1.984572,1.9859078,1.987217,1.9885051,1.989777,1.9910359,1.9922849,1.9935259,1.9947606,1.9959905,1.9972166,1.9984397,1.9996604,2.0008793],[2.009251,2.008032,2.0068114,2.0055883,2.004362,2.0031323,2.0018976,2.0006566,1.9994076,1.9981487,1.9968768,1.9955887,1.9942795,1.9929438,1.9915742,1.9901614,1.9886937,1.9871559,1.985529,1.9837885,1.9819034,1.9798344,1.9775313,1.9749308,1.9719521,1.9684933,1.9644257,1.9595865,1.9537715,1.9467249,1.938128,1.9275861,1.9146144,1.8986248,1.8789126,1.8546517,1.8248987,1.7886169,1.7447321,1.6922331,1.6303326,1.5587008,1.4777687,1.3890736,1.2955781,1.2018442,1.1139169,1.0387908,0.98343647,0.95354575,0.95354575,0.98343647,1.0387908,1.1139169,1.2018442,1.2955781,1.3890736,1.4777687,1.5587008,1.6303326,1.6922331,1.7447321,1.7886169,1.8248987,1.8546517,1.8789126,1.8986248,1.9146144,1.9275861,1.938128,1.9467249,1.9537715,1.9595865,1.9644257,1.9684933,1.9719521,1.9749308,1.9775313,1.9798344,1.9819034,1.9837885,1.985529,1.9871559,1.9886937,1.9901614,1.9915742,1.9929438,1.9942795,1.9955887,1.9968768,1.9981487,1.9994076,2.0006566,2.0018976,2.0031323,2.004362,2.0055883,2.0068114,2.008032,2.009251],[2.0164917,2.0152726,2.014052,2.0128288,2.0116029,2.0103729,2.009138,2.0078971,2.0066483,2.0053892,2.0041175,2.0028293,2.0015202,2.0001843,1.9988148,1.997402,1.9959342,1.9943964,1.9927695,1.991029,1.9891441,1.9870751,1.984772,1.9821713,1.9791927,1.975734,1.9716663,1.9668272,1.9610121,1.9539655,1.9453686,1.9348267,1.9218551,1.9058654,1.8861532,1.8618923,1.8321393,1.7958574,1.7519727,1.6994736,1.6375732,1.5659413,1.4850092,1.3963141,1.3028188,1.2090847,1.1211575,1.0460314,0.99067706,0.96078634,0.96078634,0.99067706,1.0460314,1.1211575,1.2090847,1.3028188,1.3963141,1.4850092,1.5659413,1.6375732,1.6994736,1.7519727,1.7958574,1.8321393,1.8618923,1.8861532,1.9058654,1.9218551,1.9348267,1.9453686,1.9539655,1.9610121,1.9668272,1.9716663,1.975734,1.9791927,1.9821713,1.984772,1.9870751,1.9891441,1.991029,1.9927695,1.9943964,1.9959342,1.997402,1.9988148,2.0001843,2.0015202,2.0028293,2.0041175,2.0053892,2.0066483,2.0078971,2.009138,2.0103729,2.0116029,2.0128288,2.014052,2.0152726,2.0164917],[2.0227392,2.0215201,2.0202994,2.0190763,2.0178504,2.0166206,2.0153856,2.014145,2.0128958,2.0116367,2.010365,2.0090768,2.0077677,2.006432,2.0050623,2.0036497,2.002182,2.0006442,1.9990171,1.9972767,1.9953916,1.9933226,1.9910195,1.9884189,1.9854403,1.9819815,1.9779139,1.9730747,1.9672596,1.9602132,1.9516163,1.9410744,1.9281027,1.912113,1.8924007,1.86814,1.8383869,1.802105,1.7582203,1.7057211,1.6438208,1.5721889,1.4912567,1.4025618,1.3090663,1.2153323,1.127405,1.052279,0.99692464,0.9670339,0.9670339,0.99692464,1.052279,1.127405,1.2153323,1.3090663,1.4025618,1.4912567,1.5721889,1.6438208,1.7057211,1.7582203,1.802105,1.8383869,1.86814,1.8924007,1.912113,1.9281027,1.9410744,1.9516163,1.9602132,1.9672596,1.9730747,1.9779139,1.9819815,1.9854403,1.9884189,1.9910195,1.9933226,1.9953916,1.9972767,1.9990171,2.0006442,2.002182,2.0036497,2.0050623,2.006432,2.0077677,2.0090768,2.010365,2.0116367,2.0128958,2.014145,2.0153856,2.0166206,2.0178504,2.0190763,2.0202994,2.0215201,2.0227392],[2.028119,2.0269,2.0256793,2.0244563,2.02323,2.0220003,2.0207655,2.0195246,2.0182757,2.0170166,2.0157447,2.0144567,2.0131474,2.0118117,2.0104423,2.0090294,2.0075617,2.006024,2.004397,2.0026565,2.0007715,1.9987024,1.9963993,1.9937987,1.99082,1.9873613,1.9832937,1.9784545,1.9726394,1.9655929,1.956996,1.946454,1.9334824,1.9174927,1.8977805,1.8735197,1.8437667,1.8074849,1.7636001,1.711101,1.6492006,1.5775688,1.4966366,1.4079416,1.3144461,1.2207122,1.1327848,1.0576588,1.0023044,0.9724137,0.9724137,1.0023044,1.0576588,1.1327848,1.2207122,1.3144461,1.4079416,1.4966366,1.5775688,1.6492006,1.711101,1.7636001,1.8074849,1.8437667,1.8735197,1.8977805,1.9174927,1.9334824,1.946454,1.956996,1.9655929,1.9726394,1.9784545,1.9832937,1.9873613,1.99082,1.9937987,1.9963993,1.9987024,2.0007715,2.0026565,2.004397,2.006024,2.0075617,2.0090294,2.0104423,2.0118117,2.0131474,2.0144567,2.0157447,2.0170166,2.0182757,2.0195246,2.0207655,2.0220003,2.02323,2.0244563,2.0256793,2.0269,2.028119],[2.0327435,2.0315247,2.030304,2.0290809,2.027855,2.026625,2.0253901,2.0241492,2.0229,2.0216413,2.0203695,2.019081,2.0177722,2.0164363,2.0150666,2.013654,2.0121863,2.0106485,2.0090215,2.007281,2.005396,2.003327,2.0010238,1.9984233,1.9954447,1.9919859,1.9879183,1.9830791,1.977264,1.9702175,1.9616206,1.9510787,1.938107,1.9221174,1.9024051,1.8781443,1.8483913,1.8121095,1.7682247,1.7157257,1.6538252,1.5821934,1.5012612,1.4125662,1.3190707,1.2253368,1.1374094,1.0622834,1.006929,0.9770383,0.9770383,1.006929,1.0622834,1.1374094,1.2253368,1.3190707,1.4125662,1.5012612,1.5821934,1.6538252,1.7157257,1.7682247,1.8121095,1.8483913,1.8781443,1.9024051,1.9221174,1.938107,1.9510787,1.9616206,1.9702175,1.977264,1.9830791,1.9879183,1.9919859,1.9954447,1.9984233,2.0010238,2.003327,2.005396,2.007281,2.0090215,2.0106485,2.0121863,2.013654,2.0150666,2.0164363,2.0177722,2.019081,2.0203695,2.0216413,2.0229,2.0241492,2.0253901,2.026625,2.027855,2.0290809,2.030304,2.0315247,2.0327435],[2.036713,2.0354939,2.0342731,2.03305,2.031824,2.030594,2.0293593,2.0281184,2.0268695,2.0256104,2.0243387,2.0230505,2.0217414,2.0204055,2.019036,2.0176232,2.0161555,2.0146177,2.0129907,2.0112503,2.0093653,2.0072963,2.0049932,2.0023925,1.999414,1.9959552,1.9918876,1.9870484,1.9812334,1.9741868,1.9655899,1.955048,1.9420763,1.9260867,1.9063745,1.8821136,1.8523605,1.8160787,1.7721939,1.7196949,1.6577945,1.5861626,1.5052304,1.4165354,1.32304,1.229306,1.1413788,1.0662526,1.0108982,0.9810076,0.9810076,1.0108982,1.0662526,1.1413788,1.229306,1.32304,1.4165354,1.5052304,1.5861626,1.6577945,1.7196949,1.7721939,1.8160787,1.8523605,1.8821136,1.9063745,1.9260867,1.9420763,1.955048,1.9655899,1.9741868,1.9812334,1.9870484,1.9918876,1.9959552,1.999414,2.0023925,2.0049932,2.0072963,2.0093653,2.0112503,2.0129907,2.0146177,2.0161555,2.0176232,2.019036,2.0204055,2.0217414,2.0230505,2.0243387,2.0256104,2.0268695,2.0281184,2.0293593,2.030594,2.031824,2.03305,2.0342731,2.0354939,2.036713],[2.0401154,2.0388966,2.0376759,2.0364528,2.0352266,2.0339968,2.032762,2.031521,2.030272,2.0290132,2.0277412,2.026453,2.0251439,2.0238082,2.0224385,2.021026,2.0195582,2.0180204,2.0163934,2.014653,2.0127678,2.0106988,2.0083957,2.0057952,2.0028164,1.9993577,1.99529,1.9904509,1.9846358,1.9775894,1.9689925,1.9584506,1.9454789,1.9294891,1.9097769,1.8855162,1.8557632,1.8194813,1.7755966,1.7230974,1.661197,1.5895652,1.508633,1.419938,1.3264426,1.2327086,1.1447814,1.0696552,1.0143008,0.98441017,0.98441017,1.0143008,1.0696552,1.1447814,1.2327086,1.3264426,1.419938,1.508633,1.5895652,1.661197,1.7230974,1.7755966,1.8194813,1.8557632,1.8855162,1.9097769,1.9294891,1.9454789,1.9584506,1.9689925,1.9775894,1.9846358,1.9904509,1.99529,1.9993577,2.0028164,2.0057952,2.0083957,2.0106988,2.0127678,2.014653,2.0163934,2.0180204,2.0195582,2.021026,2.0224385,2.0238082,2.0251439,2.026453,2.0277412,2.0290132,2.030272,2.031521,2.032762,2.0339968,2.0352266,2.0364528,2.0376759,2.0388966,2.0401154],[2.0430288,2.04181,2.0405893,2.0393662,2.03814,2.0369103,2.0356755,2.0344346,2.0331855,2.0319266,2.0306547,2.0293665,2.0280573,2.0267217,2.025352,2.0239394,2.0224717,2.0209339,2.019307,2.0175664,2.0156813,2.0136123,2.0113091,2.0087087,2.00573,2.0022712,1.9982035,1.9933643,1.9875493,1.9805028,1.971906,1.961364,1.9483924,1.9324026,1.9126904,1.8884296,1.8586767,1.8223947,1.7785101,1.7260109,1.6641104,1.5924786,1.5115465,1.4228514,1.3293561,1.235622,1.1476948,1.0725687,1.0172143,0.98732364,0.98732364,1.0172143,1.0725687,1.1476948,1.235622,1.3293561,1.4228514,1.5115465,1.5924786,1.6641104,1.7260109,1.7785101,1.8223947,1.8586767,1.8884296,1.9126904,1.9324026,1.9483924,1.961364,1.971906,1.9805028,1.9875493,1.9933643,1.9982035,2.0022712,2.00573,2.0087087,2.0113091,2.0136123,2.0156813,2.0175664,2.019307,2.0209339,2.0224717,2.0239394,2.025352,2.0267217,2.0280573,2.0293665,2.0306547,2.0319266,2.0331855,2.0344346,2.0356755,2.0369103,2.03814,2.0393662,2.0405893,2.04181,2.0430288],[2.0455213,2.0443025,2.0430818,2.0418587,2.0406327,2.0394027,2.038168,2.036927,2.035678,2.034419,2.0331473,2.031859,2.03055,2.0292141,2.0278444,2.0264318,2.024964,2.0234263,2.0217993,2.0200589,2.0181737,2.0161047,2.0138016,2.0112011,2.0082226,2.0047636,2.0006962,1.9958569,1.9900419,1.9829953,1.9743984,1.9638565,1.9508848,1.9348952,1.915183,1.8909221,1.8611691,1.8248873,1.7810025,1.7285035,1.666603,1.5949712,1.514039,1.425344,1.3318485,1.2381146,1.1501873,1.0750612,1.0197068,0.9898161,0.9898161,1.0197068,1.0750612,1.1501873,1.2381146,1.3318485,1.425344,1.514039,1.5949712,1.666603,1.7285035,1.7810025,1.8248873,1.8611691,1.8909221,1.915183,1.9348952,1.9508848,1.9638565,1.9743984,1.9829953,1.9900419,1.9958569,2.0006962,2.0047636,2.0082226,2.0112011,2.0138016,2.0161047,2.0181737,2.0200589,2.0217993,2.0234263,2.024964,2.0264318,2.0278444,2.0292141,2.03055,2.031859,2.0331473,2.034419,2.035678,2.036927,2.038168,2.0394027,2.0406327,2.0418587,2.0430818,2.0443025,2.0455213],[2.0476518,2.046433,2.0452123,2.0439892,2.042763,2.041533,2.0402985,2.0390573,2.0378084,2.0365496,2.0352776,2.0339894,2.0326803,2.0313444,2.029975,2.028562,2.0270944,2.0255566,2.0239296,2.0221891,2.0203042,2.0182352,2.015932,2.0133314,2.0103528,2.006894,2.0028265,1.9979873,1.9921722,1.9851258,1.9765289,1.965987,1.9530153,1.9370255,1.9173133,1.8930526,1.8632995,1.8270175,1.7831329,1.7306337,1.6687334,1.5971014,1.5161693,1.4274744,1.3339789,1.2402449,1.1523176,1.0771916,1.0218372,0.9919465,0.9919465,1.0218372,1.0771916,1.1523176,1.2402449,1.3339789,1.4274744,1.5161693,1.5971014,1.6687334,1.7306337,1.7831329,1.8270175,1.8632995,1.8930526,1.9173133,1.9370255,1.9530153,1.965987,1.9765289,1.9851258,1.9921722,1.9979873,2.0028265,2.006894,2.0103528,2.0133314,2.015932,2.0182352,2.0203042,2.0221891,2.0239296,2.0255566,2.0270944,2.028562,2.029975,2.0313444,2.0326803,2.0339894,2.0352776,2.0365496,2.0378084,2.0390573,2.0402985,2.041533,2.042763,2.0439892,2.0452123,2.046433,2.0476518],[2.0494716,2.0482526,2.0470319,2.0458088,2.0445828,2.0433528,2.042118,2.040877,2.0396283,2.0383692,2.0370975,2.0358093,2.0345001,2.0331643,2.0317948,2.030382,2.0289142,2.0273764,2.0257494,2.024009,2.022124,2.020055,2.017752,2.0151513,2.0121727,2.008714,2.0046463,1.9998071,1.9939921,1.9869455,1.9783486,1.9678067,1.954835,1.9388454,1.9191332,1.8948723,1.8651192,1.8288374,1.7849526,1.7324536,1.6705532,1.5989213,1.5179892,1.4292941,1.3357987,1.2420647,1.1541375,1.0790113,1.023657,0.9937663,0.9937663,1.023657,1.0790113,1.1541375,1.2420647,1.3357987,1.4292941,1.5179892,1.5989213,1.6705532,1.7324536,1.7849526,1.8288374,1.8651192,1.8948723,1.9191332,1.9388454,1.954835,1.9678067,1.9783486,1.9869455,1.9939921,1.9998071,2.0046463,2.008714,2.0121727,2.0151513,2.017752,2.020055,2.022124,2.024009,2.0257494,2.0273764,2.0289142,2.030382,2.0317948,2.0331643,2.0345001,2.0358093,2.0370975,2.0383692,2.0396283,2.040877,2.042118,2.0433528,2.0445828,2.0458088,2.0470319,2.0482526,2.0494716],[2.051025,2.049806,2.0485854,2.0473623,2.0461364,2.0449064,2.0436716,2.0424306,2.0411816,2.0399227,2.038651,2.0373626,2.0360537,2.0347178,2.033348,2.0319355,2.0304677,2.02893,2.027303,2.0255625,2.0236773,2.0216084,2.0193052,2.0167048,2.0137262,2.0102673,2.0061998,2.0013604,1.9955455,1.9884989,1.979902,1.9693601,1.9563885,1.9403988,1.9206866,1.8964257,1.8666728,1.8303909,1.7865062,1.7340071,1.6721066,1.6004748,1.5195427,1.4308476,1.3373522,1.2436182,1.1556909,1.0805649,1.0252105,0.9953198,0.9953198,1.0252105,1.0805649,1.1556909,1.2436182,1.3373522,1.4308476,1.5195427,1.6004748,1.6721066,1.7340071,1.7865062,1.8303909,1.8666728,1.8964257,1.9206866,1.9403988,1.9563885,1.9693601,1.979902,1.9884989,1.9955455,2.0013604,2.0061998,2.0102673,2.0137262,2.0167048,2.0193052,2.0216084,2.0236773,2.0255625,2.027303,2.02893,2.0304677,2.0319355,2.033348,2.0347178,2.0360537,2.0373626,2.038651,2.0399227,2.0411816,2.0424306,2.0436716,2.0449064,2.0461364,2.0473623,2.0485854,2.049806,2.051025],[2.0523505,2.0511317,2.049911,2.048688,2.0474617,2.046232,2.0449972,2.0437562,2.0425072,2.0412483,2.0399764,2.0386882,2.037379,2.0360434,2.0346737,2.033261,2.0317934,2.0302556,2.0286286,2.0268881,2.025003,2.022934,2.0206308,2.0180304,2.0150516,2.0115929,2.0075252,2.002686,1.996871,1.9898245,1.9812276,1.9706857,1.9577141,1.9417243,1.9220121,1.8977513,1.8679984,1.8317164,1.7878318,1.7353326,1.6734321,1.6018003,1.5208682,1.4321731,1.3386778,1.2449437,1.1570165,1.0818903,1.026536,0.99664533,0.99664533,1.026536,1.0818903,1.1570165,1.2449437,1.3386778,1.4321731,1.5208682,1.6018003,1.6734321,1.7353326,1.7878318,1.8317164,1.8679984,1.8977513,1.9220121,1.9417243,1.9577141,1.9706857,1.9812276,1.9898245,1.996871,2.002686,2.0075252,2.0115929,2.0150516,2.0180304,2.0206308,2.022934,2.025003,2.0268881,2.0286286,2.0302556,2.0317934,2.033261,2.0346737,2.0360434,2.037379,2.0386882,2.0399764,2.0412483,2.0425072,2.0437562,2.0449972,2.046232,2.0474617,2.048688,2.049911,2.0511317,2.0523505],[2.053481,2.052262,2.0510414,2.0498183,2.048592,2.0473623,2.0461276,2.0448866,2.0436378,2.0423787,2.0411067,2.0398188,2.0385094,2.0371737,2.0358043,2.0343914,2.0329237,2.031386,2.029759,2.0280185,2.0261335,2.0240645,2.0217614,2.0191607,2.016182,2.0127234,2.0086555,2.0038166,1.9980015,1.9909549,1.982358,1.9718161,1.9588444,1.9428548,1.9231426,1.8988817,1.8691287,1.8328469,1.7889621,1.7364631,1.6745626,1.6029308,1.5219986,1.4333036,1.3398081,1.2460742,1.1581469,1.0830208,1.0276664,0.99777573,0.99777573,1.0276664,1.0830208,1.1581469,1.2460742,1.3398081,1.4333036,1.5219986,1.6029308,1.6745626,1.7364631,1.7889621,1.8328469,1.8691287,1.8988817,1.9231426,1.9428548,1.9588444,1.9718161,1.982358,1.9909549,1.9980015,2.0038166,2.0086555,2.0127234,2.016182,2.0191607,2.0217614,2.0240645,2.0261335,2.0280185,2.029759,2.031386,2.0329237,2.0343914,2.0358043,2.0371737,2.0385094,2.0398188,2.0411067,2.0423787,2.0436378,2.0448866,2.0461276,2.0473623,2.048592,2.0498183,2.0510414,2.052262,2.053481],[2.0544448,2.053226,2.0520053,2.0507822,2.049556,2.048326,2.0470915,2.0458503,2.0446014,2.0433426,2.0420706,2.0407825,2.0394733,2.0381374,2.036768,2.035355,2.0338874,2.0323496,2.0307226,2.0289822,2.0270972,2.0250282,2.022725,2.0201244,2.0171459,2.0136871,2.0096195,2.0047803,1.9989653,1.9919188,1.9833219,1.97278,1.9598083,1.9438186,1.9241064,1.8998456,1.8700925,1.8338106,1.7899259,1.7374268,1.6755264,1.6038945,1.5229623,1.4342674,1.3407719,1.2470379,1.1591107,1.0839846,1.0286303,0.99873954,0.99873954,1.0286303,1.0839846,1.1591107,1.2470379,1.3407719,1.4342674,1.5229623,1.6038945,1.6755264,1.7374268,1.7899259,1.8338106,1.8700925,1.8998456,1.9241064,1.9438186,1.9598083,1.97278,1.9833219,1.9919188,1.9989653,2.0047803,2.0096195,2.0136871,2.0171459,2.0201244,2.022725,2.0250282,2.0270972,2.0289822,2.0307226,2.0323496,2.0338874,2.035355,2.036768,2.0381374,2.0394733,2.0407825,2.0420706,2.0433426,2.0446014,2.0458503,2.0470915,2.048326,2.049556,2.0507822,2.0520053,2.053226,2.0544448],[2.0552664,2.0540473,2.0528266,2.0516036,2.0503774,2.0491476,2.0479128,2.0466719,2.045423,2.044164,2.042892,2.041604,2.0402946,2.038959,2.0375896,2.0361767,2.034709,2.0331712,2.0315442,2.0298038,2.0279188,2.0258498,2.0235467,2.020946,2.0179672,2.0145087,2.0104408,2.005602,1.9997867,1.9927402,1.9841433,1.9736013,1.9606297,1.94464,1.9249278,1.900667,1.870914,1.8346322,1.7907474,1.7382483,1.6763479,1.6047161,1.5237839,1.4350889,1.3415934,1.2478595,1.1599321,1.0848061,1.0294517,0.999561,0.999561,1.0294517,1.0848061,1.1599321,1.2478595,1.3415934,1.4350889,1.5237839,1.6047161,1.6763479,1.7382483,1.7907474,1.8346322,1.870914,1.900667,1.9249278,1.94464,1.9606297,1.9736013,1.9841433,1.9927402,1.9997867,2.005602,2.0104408,2.0145087,2.0179672,2.020946,2.0235467,2.0258498,2.0279188,2.0298038,2.0315442,2.0331712,2.034709,2.0361767,2.0375896,2.038959,2.0402946,2.041604,2.042892,2.044164,2.045423,2.0466719,2.0479128,2.0491476,2.0503774,2.0516036,2.0528266,2.0540473,2.0552664],[2.0559661,2.0547473,2.0535266,2.0523036,2.0510774,2.0498476,2.0486128,2.0473719,2.0461228,2.044864,2.043592,2.0423038,2.0409946,2.039659,2.0382893,2.0368767,2.035409,2.0338712,2.0322442,2.0305037,2.0286186,2.0265496,2.0242465,2.021646,2.0186672,2.0152085,2.0111408,2.0063016,2.0004866,1.9934402,1.9848433,1.9743013,1.9613297,1.9453399,1.9256277,1.901367,1.871614,1.835332,1.7914474,1.7389482,1.6770477,1.6054159,1.5244838,1.4357888,1.3422934,1.2485594,1.1606321,1.085506,1.0301516,1.000261,1.000261,1.0301516,1.085506,1.1606321,1.2485594,1.3422934,1.4357888,1.5244838,1.6054159,1.6770477,1.7389482,1.7914474,1.835332,1.871614,1.901367,1.9256277,1.9453399,1.9613297,1.9743013,1.9848433,1.9934402,2.0004866,2.0063016,2.0111408,2.0152085,2.0186672,2.021646,2.0242465,2.0265496,2.0286186,2.0305037,2.0322442,2.0338712,2.035409,2.0368767,2.0382893,2.039659,2.0409946,2.0423038,2.043592,2.044864,2.0461228,2.0473719,2.0486128,2.0498476,2.0510774,2.0523036,2.0535266,2.0547473,2.0559661],[2.0565624,2.0553436,2.054123,2.0528998,2.0516737,2.0504436,2.049209,2.047968,2.046719,2.0454602,2.0441883,2.0429,2.041591,2.040255,2.0388856,2.0374727,2.036005,2.0344672,2.0328403,2.0310998,2.0292149,2.0271459,2.0248427,2.022242,2.0192635,2.0158048,2.011737,2.006898,2.001083,1.9940364,1.9854395,1.9748976,1.961926,1.9459362,1.926224,1.9019632,1.8722101,1.8359282,1.7920436,1.7395444,1.677644,1.6060121,1.52508,1.436385,1.3428895,1.2491555,1.1612283,1.0861022,1.0307479,1.0008572,1.0008572,1.0307479,1.0861022,1.1612283,1.2491555,1.3428895,1.436385,1.52508,1.6060121,1.677644,1.7395444,1.7920436,1.8359282,1.8722101,1.9019632,1.926224,1.9459362,1.961926,1.9748976,1.9854395,1.9940364,2.001083,2.006898,2.011737,2.0158048,2.0192635,2.022242,2.0248427,2.0271459,2.0292149,2.0310998,2.0328403,2.0344672,2.036005,2.0374727,2.0388856,2.040255,2.041591,2.0429,2.0441883,2.0454602,2.046719,2.047968,2.049209,2.0504436,2.0516737,2.0528998,2.054123,2.0553436,2.0565624],[2.0570703,2.0558515,2.0546308,2.0534077,2.0521815,2.0509515,2.049717,2.0484757,2.047227,2.045968,2.044696,2.043408,2.0420988,2.040763,2.0393934,2.0379806,2.0365129,2.034975,2.033348,2.0316076,2.0297227,2.0276537,2.0253506,2.02275,2.0197713,2.0163126,2.012245,2.0074058,2.0015907,1.9945443,1.9859474,1.9754055,1.9624338,1.946444,1.9267318,1.9024711,1.872718,1.836436,1.7925514,1.7400522,1.6781518,1.6065199,1.5255878,1.4368929,1.3433974,1.2496634,1.1617361,1.0866101,1.0312557,1.0013651,1.0013651,1.0312557,1.0866101,1.1617361,1.2496634,1.3433974,1.4368929,1.5255878,1.6065199,1.6781518,1.7400522,1.7925514,1.836436,1.872718,1.9024711,1.9267318,1.946444,1.9624338,1.9754055,1.9859474,1.9945443,2.0015907,2.0074058,2.012245,2.0163126,2.0197713,2.02275,2.0253506,2.0276537,2.0297227,2.0316076,2.033348,2.034975,2.0365129,2.0379806,2.0393934,2.040763,2.0420988,2.043408,2.044696,2.045968,2.047227,2.0484757,2.049717,2.0509515,2.0521815,2.0534077,2.0546308,2.0558515,2.0570703],[2.0575027,2.056284,2.0550632,2.0538402,2.052614,2.0513842,2.0501494,2.0489085,2.0476594,2.0464005,2.0451286,2.0438404,2.0425313,2.0411956,2.039826,2.0384133,2.0369456,2.0354078,2.0337808,2.0320404,2.0301552,2.0280862,2.025783,2.0231826,2.0202038,2.016745,2.0126774,2.0078382,2.0020232,1.9949768,1.9863799,1.975838,1.9628663,1.9468765,1.9271643,1.9029036,1.8731506,1.8368686,1.792984,1.7404848,1.6785843,1.6069525,1.5260204,1.4373254,1.34383,1.250096,1.1621687,1.0870426,1.0316882,1.0017976,1.0017976,1.0316882,1.0870426,1.1621687,1.250096,1.34383,1.4373254,1.5260204,1.6069525,1.6785843,1.7404848,1.792984,1.8368686,1.8731506,1.9029036,1.9271643,1.9468765,1.9628663,1.975838,1.9863799,1.9949768,2.0020232,2.0078382,2.0126774,2.016745,2.0202038,2.0231826,2.025783,2.0280862,2.0301552,2.0320404,2.0337808,2.0354078,2.0369456,2.0384133,2.039826,2.0411956,2.0425313,2.0438404,2.0451286,2.0464005,2.0476594,2.0489085,2.0501494,2.0513842,2.052614,2.0538402,2.0550632,2.056284,2.0575027]],\"type\":\"surface\",\"scene\":\"scene\"},{\"colorscale\":[[0.0,\"rgb(255,255,255)\"],[0.125,\"rgb(240,240,240)\"],[0.25,\"rgb(217,217,217)\"],[0.375,\"rgb(189,189,189)\"],[0.5,\"rgb(150,150,150)\"],[0.625,\"rgb(115,115,115)\"],[0.75,\"rgb(82,82,82)\"],[0.875,\"rgb(37,37,37)\"],[1.0,\"rgb(0,0,0)\"]],\"customdata\":[[1.0600208,1.0588019,1.0575812,1.0563581,1.055132,1.0539021,1.0526674,1.0514264,1.0501775,1.0489185,1.0476466,1.0463585,1.0450493,1.0437136,1.042344,1.0409312,1.0394635,1.0379257,1.0362988,1.0345583,1.0326732,1.0306042,1.0283011,1.0257006,1.0227219,1.0192631,1.0151955,1.0103563,1.0045413,0.99749476,0.98889786,0.97835594,0.9653843,0.9493946,0.9296824,0.90542156,0.8756685,0.83938664,0.79550195,0.74300283,0.6811024,0.60947055,0.5285384,0.43984336,0.34634793,0.25261393,0.1646867,0.0895606,0.034206238,0.0043155537,0.0043155537,0.034206238,0.0895606,0.1646867,0.25261393,0.34634793,0.43984336,0.5285384,0.60947055,0.6811024,0.74300283,0.79550195,0.83938664,0.8756685,0.90542156,0.9296824,0.9493946,0.9653843,0.97835594,0.98889786,0.99749476,1.0045413,1.0103563,1.0151955,1.0192631,1.0227219,1.0257006,1.0283011,1.0306042,1.0326732,1.0345583,1.0362988,1.0379257,1.0394635,1.0409312,1.042344,1.0437136,1.0450493,1.0463585,1.0476466,1.0489185,1.0501775,1.0514264,1.0526674,1.0539021,1.055132,1.0563581,1.0575812,1.0588019,1.0600208],[1.0600288,1.0588099,1.0575892,1.0563661,1.05514,1.0539101,1.0526754,1.0514344,1.0501854,1.0489265,1.0476546,1.0463665,1.0450573,1.0437216,1.042352,1.0409392,1.0394715,1.0379337,1.0363067,1.0345663,1.0326812,1.0306122,1.0283091,1.0257086,1.0227299,1.0192711,1.0152035,1.0103643,1.0045493,0.9975027,0.9889058,0.9783639,0.96539223,0.9494025,0.9296903,0.9054295,0.87567645,0.83939457,0.7955099,0.74301076,0.6811103,0.6094785,0.52854633,0.4398513,0.3463559,0.2526219,0.16469465,0.08956856,0.034214202,0.0043235165,0.0043235165,0.034214202,0.08956856,0.16469465,0.2526219,0.3463559,0.4398513,0.52854633,0.6094785,0.6811103,0.74301076,0.7955099,0.83939457,0.87567645,0.9054295,0.9296903,0.9494025,0.96539223,0.9783639,0.9889058,0.9975027,1.0045493,1.0103643,1.0152035,1.0192711,1.0227299,1.0257086,1.0283091,1.0306122,1.0326812,1.0345663,1.0363067,1.0379337,1.0394715,1.0409392,1.042352,1.0437216,1.0450573,1.0463665,1.0476546,1.0489265,1.0501854,1.0514344,1.0526754,1.0539101,1.05514,1.0563661,1.0575892,1.0588099,1.0600288],[1.0600381,1.0588192,1.0575985,1.0563754,1.0551493,1.0539194,1.0526847,1.0514437,1.0501947,1.0489358,1.0476639,1.0463758,1.0450666,1.0437309,1.0423613,1.0409485,1.0394808,1.037943,1.036316,1.0345756,1.0326905,1.0306215,1.0283184,1.0257179,1.0227392,1.0192804,1.0152128,1.0103736,1.0045586,0.99751204,0.98891515,0.9783732,0.9654016,0.94941187,0.92969966,0.90543884,0.8756858,0.8394039,0.79551923,0.7430201,0.6811197,0.60948783,0.5285557,0.4398607,0.34636527,0.25263128,0.16470401,0.08957792,0.03422356,0.004332876,0.004332876,0.03422356,0.08957792,0.16470401,0.25263128,0.34636527,0.4398607,0.5285557,0.60948783,0.6811197,0.7430201,0.79551923,0.8394039,0.8756858,0.90543884,0.92969966,0.94941187,0.9654016,0.9783732,0.98891515,0.99751204,1.0045586,1.0103736,1.0152128,1.0192804,1.0227392,1.0257179,1.0283184,1.0306215,1.0326905,1.0345756,1.036316,1.037943,1.0394808,1.0409485,1.0423613,1.0437309,1.0450666,1.0463758,1.0476639,1.0489358,1.0501947,1.0514437,1.0526847,1.0539194,1.0551493,1.0563754,1.0575985,1.0588192,1.0600381],[1.060049,1.0588301,1.0576094,1.0563864,1.0551603,1.0539304,1.0526956,1.0514547,1.0502057,1.0489467,1.0476749,1.0463867,1.0450776,1.0437418,1.0423722,1.0409595,1.0394918,1.037954,1.036327,1.0345865,1.0327015,1.0306325,1.0283294,1.0257288,1.0227501,1.0192914,1.0152237,1.0103846,1.0045695,0.99752307,0.9889262,0.97838426,0.9654126,0.9494229,0.9297107,0.90544987,0.87569684,0.83941495,0.79553026,0.74303114,0.6811307,0.60949886,0.5285667,0.4398717,0.34637627,0.25264227,0.164715,0.089588925,0.03423456,0.0043438766,0.0043438766,0.03423456,0.089588925,0.164715,0.25264227,0.34637627,0.4398717,0.5285667,0.60949886,0.6811307,0.74303114,0.79553026,0.83941495,0.87569684,0.90544987,0.9297107,0.9494229,0.9654126,0.97838426,0.9889262,0.99752307,1.0045695,1.0103846,1.0152237,1.0192914,1.0227501,1.0257288,1.0283294,1.0306325,1.0327015,1.0345865,1.036327,1.037954,1.0394918,1.0409595,1.0423722,1.0437418,1.0450776,1.0463867,1.0476749,1.0489467,1.0502057,1.0514547,1.0526956,1.0539304,1.0551603,1.0563864,1.0576094,1.0588301,1.060049],[1.060062,1.0588431,1.0576224,1.0563993,1.0551733,1.0539434,1.0527086,1.0514677,1.0502187,1.0489597,1.0476879,1.0463997,1.0450906,1.0437548,1.0423852,1.0409725,1.0395048,1.037967,1.03634,1.0345995,1.0327145,1.0306455,1.0283424,1.0257418,1.0227631,1.0193044,1.0152367,1.0103976,1.0045825,0.997536,0.9889391,0.9783972,0.96542555,0.94943583,0.9297236,0.9054628,0.8757098,0.8394279,0.7955432,0.7430441,0.68114364,0.6095118,0.52857965,0.43988463,0.3463892,0.2526552,0.16472794,0.08960185,0.03424749,0.004356807,0.004356807,0.03424749,0.08960185,0.16472794,0.2526552,0.3463892,0.43988463,0.52857965,0.6095118,0.68114364,0.7430441,0.7955432,0.8394279,0.8757098,0.9054628,0.9297236,0.94943583,0.96542555,0.9783972,0.9889391,0.997536,1.0045825,1.0103976,1.0152367,1.0193044,1.0227631,1.0257418,1.0283424,1.0306455,1.0327145,1.0345995,1.03634,1.037967,1.0395048,1.0409725,1.0423852,1.0437548,1.0450906,1.0463997,1.0476879,1.0489597,1.0502187,1.0514677,1.0527086,1.0539434,1.0551733,1.0563993,1.0576224,1.0588431,1.060062],[1.0600772,1.0588583,1.0576376,1.0564145,1.0551884,1.0539585,1.0527238,1.0514828,1.0502338,1.0489749,1.047703,1.0464149,1.0451057,1.04377,1.0424004,1.0409876,1.0395199,1.0379821,1.0363551,1.0346147,1.0327296,1.0306606,1.0283575,1.025757,1.0227783,1.0193195,1.0152519,1.0104127,1.0045977,0.9975512,0.9889543,0.9784124,0.96544075,0.949451,0.9297388,0.905478,0.875725,0.8394431,0.7955584,0.7430593,0.68115884,0.609527,0.52859485,0.4398998,0.34640437,0.25267038,0.16474314,0.08961705,0.03426269,0.004372005,0.004372005,0.03426269,0.08961705,0.16474314,0.25267038,0.34640437,0.4398998,0.52859485,0.609527,0.68115884,0.7430593,0.7955584,0.8394431,0.875725,0.905478,0.9297388,0.949451,0.96544075,0.9784124,0.9889543,0.9975512,1.0045977,1.0104127,1.0152519,1.0193195,1.0227783,1.025757,1.0283575,1.0306606,1.0327296,1.0346147,1.0363551,1.0379821,1.0395199,1.0409876,1.0424004,1.04377,1.0451057,1.0464149,1.047703,1.0489749,1.0502338,1.0514828,1.0527238,1.0539585,1.0551884,1.0564145,1.0576376,1.0588583,1.0600772],[1.0600951,1.0588762,1.0576555,1.0564324,1.0552063,1.0539764,1.0527416,1.0515007,1.0502517,1.0489928,1.0477209,1.0464327,1.0451236,1.0437878,1.0424182,1.0410055,1.0395378,1.038,1.036373,1.0346326,1.0327475,1.0306785,1.0283754,1.0257748,1.0227962,1.0193374,1.0152698,1.0104306,1.0046155,0.997569,0.9889721,0.9784302,0.9654586,0.94946885,0.92975664,0.9054958,0.8757428,0.8394609,0.7955762,0.7430771,0.68117666,0.6095448,0.5286127,0.43991768,0.34642226,0.25268826,0.164761,0.08963491,0.034280553,0.0043898677,0.0043898677,0.034280553,0.08963491,0.164761,0.25268826,0.34642226,0.43991768,0.5286127,0.6095448,0.68117666,0.7430771,0.7955762,0.8394609,0.8757428,0.9054958,0.92975664,0.94946885,0.9654586,0.9784302,0.9889721,0.997569,1.0046155,1.0104306,1.0152698,1.0193374,1.0227962,1.0257748,1.0283754,1.0306785,1.0327475,1.0346326,1.036373,1.038,1.0395378,1.0410055,1.0424182,1.0437878,1.0451236,1.0464327,1.0477209,1.0489928,1.0502517,1.0515007,1.0527416,1.0539764,1.0552063,1.0564324,1.0576555,1.0588762,1.0600951],[1.060116,1.0588971,1.0576764,1.0564533,1.0552273,1.0539974,1.0527626,1.0515217,1.0502727,1.0490137,1.0477419,1.0464537,1.0451446,1.0438088,1.0424392,1.0410265,1.0395588,1.038021,1.036394,1.0346535,1.0327685,1.0306995,1.0283964,1.0257958,1.0228171,1.0193584,1.0152907,1.0104516,1.0046365,0.99759007,0.98899317,0.97845125,0.9654796,0.9494899,0.9297777,0.90551686,0.87576383,0.83948195,0.79559726,0.74309814,0.6811977,0.60956585,0.5286337,0.43993866,0.34644324,0.25270924,0.164782,0.089655906,0.03430155,0.0044108634,0.0044108634,0.03430155,0.089655906,0.164782,0.25270924,0.34644324,0.43993866,0.5286337,0.60956585,0.6811977,0.74309814,0.79559726,0.83948195,0.87576383,0.90551686,0.9297777,0.9494899,0.9654796,0.97845125,0.98899317,0.99759007,1.0046365,1.0104516,1.0152907,1.0193584,1.0228171,1.0257958,1.0283964,1.0306995,1.0327685,1.0346535,1.036394,1.038021,1.0395588,1.0410265,1.0424392,1.0438088,1.0451446,1.0464537,1.0477419,1.0490137,1.0502727,1.0515217,1.0527626,1.0539974,1.0552273,1.0564533,1.0576764,1.0588971,1.060116],[1.0601407,1.0589218,1.0577011,1.056478,1.055252,1.0540221,1.0527873,1.0515463,1.0502974,1.0490384,1.0477666,1.0464784,1.0451692,1.0438335,1.0424639,1.0410511,1.0395834,1.0380456,1.0364187,1.0346782,1.0327932,1.0307242,1.028421,1.0258205,1.0228418,1.0193831,1.0153154,1.0104762,1.0046612,0.99761474,0.98901784,0.9784759,0.9655043,0.94951457,0.92980236,0.90554154,0.8757885,0.8395066,0.79562193,0.7431228,0.6812224,0.60959053,0.5286584,0.43996334,0.3464679,0.25273392,0.16480668,0.08968059,0.034326226,0.0044355406,0.0044355406,0.034326226,0.08968059,0.16480668,0.25273392,0.3464679,0.43996334,0.5286584,0.60959053,0.6812224,0.7431228,0.79562193,0.8395066,0.8757885,0.90554154,0.92980236,0.94951457,0.9655043,0.9784759,0.98901784,0.99761474,1.0046612,1.0104762,1.0153154,1.0193831,1.0228418,1.0258205,1.028421,1.0307242,1.0327932,1.0346782,1.0364187,1.0380456,1.0395834,1.0410511,1.0424639,1.0438335,1.0451692,1.0464784,1.0477666,1.0490384,1.0502974,1.0515463,1.0527873,1.0540221,1.055252,1.056478,1.0577011,1.0589218,1.0601407],[1.0601698,1.0589509,1.0577302,1.0565071,1.055281,1.0540512,1.0528164,1.0515754,1.0503265,1.0490675,1.0477957,1.0465075,1.0451983,1.0438626,1.042493,1.0410802,1.0396125,1.0380747,1.0364478,1.0347073,1.0328223,1.0307533,1.0284501,1.0258496,1.0228709,1.0194122,1.0153445,1.0105053,1.0046903,0.9976437,0.9890468,0.9785049,0.96553326,0.94954354,0.9298313,0.9055705,0.8758175,0.8395356,0.7956509,0.7431518,0.68125135,0.6096195,0.52868736,0.43999237,0.34649694,0.25276294,0.16483568,0.08970959,0.03435523,0.004464545,0.004464545,0.03435523,0.08970959,0.16483568,0.25276294,0.34649694,0.43999237,0.52868736,0.6096195,0.68125135,0.7431518,0.7956509,0.8395356,0.8758175,0.9055705,0.9298313,0.94954354,0.96553326,0.9785049,0.9890468,0.9976437,1.0046903,1.0105053,1.0153445,1.0194122,1.0228709,1.0258496,1.0284501,1.0307533,1.0328223,1.0347073,1.0364478,1.0380747,1.0396125,1.0410802,1.042493,1.0438626,1.0451983,1.0465075,1.0477957,1.0490675,1.0503265,1.0515754,1.0528164,1.0540512,1.055281,1.0565071,1.0577302,1.0589509,1.0601698],[1.0602039,1.058985,1.0577643,1.0565412,1.0553151,1.0540853,1.0528505,1.0516095,1.0503606,1.0491016,1.0478297,1.0465416,1.0452324,1.0438967,1.0425271,1.0411143,1.0396466,1.0381088,1.0364819,1.0347414,1.0328563,1.0307873,1.0284842,1.0258837,1.022905,1.0194463,1.0153786,1.0105394,1.0047244,0.9976778,0.9890809,0.978539,0.96556735,0.9495776,0.9298654,0.9056046,0.8758516,0.8395697,0.795685,0.7431859,0.68128544,0.6096536,0.52872145,0.44002643,0.346531,0.252797,0.16486977,0.08974368,0.03438932,0.0044986345,0.0044986345,0.03438932,0.08974368,0.16486977,0.252797,0.346531,0.44002643,0.52872145,0.6096536,0.68128544,0.7431859,0.795685,0.8395697,0.8758516,0.9056046,0.9298654,0.9495776,0.96556735,0.978539,0.9890809,0.9976778,1.0047244,1.0105394,1.0153786,1.0194463,1.022905,1.0258837,1.0284842,1.0307873,1.0328563,1.0347414,1.0364819,1.0381088,1.0396466,1.0411143,1.0425271,1.0438967,1.0452324,1.0465416,1.0478297,1.0491016,1.0503606,1.0516095,1.0528505,1.0540853,1.0553151,1.0565412,1.0577643,1.058985,1.0602039],[1.060244,1.059025,1.0578043,1.0565813,1.0553552,1.0541253,1.0528905,1.0516496,1.0504006,1.0491416,1.0478698,1.0465816,1.0452725,1.0439367,1.0425671,1.0411544,1.0396867,1.0381489,1.0365219,1.0347815,1.0328964,1.0308274,1.0285243,1.0259237,1.022945,1.0194863,1.0154186,1.0105795,1.0047644,0.99771786,0.98912096,0.97857904,0.9656074,0.9496177,0.9299055,0.90564466,0.8758916,0.83960974,0.79572505,0.74322593,0.6813255,0.60969365,0.5287615,0.44006652,0.3465711,0.2528371,0.16490984,0.08978375,0.034429386,0.0045387014,0.0045387014,0.034429386,0.08978375,0.16490984,0.2528371,0.3465711,0.44006652,0.5287615,0.60969365,0.6813255,0.74322593,0.79572505,0.83960974,0.8758916,0.90564466,0.9299055,0.9496177,0.9656074,0.97857904,0.98912096,0.99771786,1.0047644,1.0105795,1.0154186,1.0194863,1.022945,1.0259237,1.0285243,1.0308274,1.0328964,1.0347815,1.0365219,1.0381489,1.0396867,1.0411544,1.0425671,1.0439367,1.0452725,1.0465816,1.0478698,1.0491416,1.0504006,1.0516496,1.0528905,1.0541253,1.0553552,1.0565813,1.0578043,1.059025,1.060244],[1.060291,1.0590721,1.0578514,1.0566283,1.0554023,1.0541724,1.0529376,1.0516967,1.0504477,1.0491887,1.0479169,1.0466287,1.0453196,1.0439838,1.0426142,1.0412015,1.0397338,1.038196,1.036569,1.0348285,1.0329435,1.0308745,1.0285714,1.0259708,1.0229921,1.0195334,1.0154657,1.0106266,1.0048115,0.99776495,0.98916805,0.97862613,0.9656545,0.9496648,0.92995256,0.90569174,0.8759387,0.83965683,0.79577214,0.743273,0.6813726,0.60974073,0.5288086,0.4401136,0.34661818,0.25288418,0.16495693,0.08983084,0.034476478,0.004585792,0.004585792,0.034476478,0.08983084,0.16495693,0.25288418,0.34661818,0.4401136,0.5288086,0.60974073,0.6813726,0.743273,0.79577214,0.83965683,0.8759387,0.90569174,0.92995256,0.9496648,0.9656545,0.97862613,0.98916805,0.99776495,1.0048115,1.0106266,1.0154657,1.0195334,1.0229921,1.0259708,1.0285714,1.0308745,1.0329435,1.0348285,1.036569,1.038196,1.0397338,1.0412015,1.0426142,1.0439838,1.0453196,1.0466287,1.0479169,1.0491887,1.0504477,1.0516967,1.0529376,1.0541724,1.0554023,1.0566283,1.0578514,1.0590721,1.060291],[1.0603464,1.0591274,1.0579067,1.0566837,1.0554576,1.0542277,1.0529929,1.051752,1.050503,1.049244,1.0479722,1.046684,1.0453749,1.0440391,1.0426695,1.0412568,1.0397891,1.0382513,1.0366243,1.0348839,1.0329988,1.0309298,1.0286267,1.0260261,1.0230474,1.0195887,1.015521,1.0106819,1.0048668,0.9978203,0.9892234,0.9786815,0.96570987,0.94972014,0.93000793,0.9057471,0.8759941,0.8397122,0.7958275,0.7433284,0.68142796,0.6097961,0.52886397,0.44016895,0.34667352,0.25293952,0.16501227,0.08988618,0.03453182,0.0046411366,0.0046411366,0.03453182,0.08988618,0.16501227,0.25293952,0.34667352,0.44016895,0.52886397,0.6097961,0.68142796,0.7433284,0.7958275,0.8397122,0.8759941,0.9057471,0.93000793,0.94972014,0.96570987,0.9786815,0.9892234,0.9978203,1.0048668,1.0106819,1.015521,1.0195887,1.0230474,1.0260261,1.0286267,1.0309298,1.0329988,1.0348839,1.0366243,1.0382513,1.0397891,1.0412568,1.0426695,1.0440391,1.0453749,1.046684,1.0479722,1.049244,1.050503,1.051752,1.0529929,1.0542277,1.0554576,1.0566837,1.0579067,1.0591274,1.0603464],[1.0604115,1.0591925,1.0579718,1.0567487,1.0555227,1.0542928,1.053058,1.051817,1.0505681,1.0493091,1.0480373,1.0467491,1.04544,1.0441042,1.0427346,1.0413219,1.0398542,1.0383164,1.0366894,1.034949,1.0330639,1.0309949,1.0286918,1.0260912,1.0231125,1.0196538,1.0155861,1.010747,1.0049319,0.99788535,0.98928845,0.97874653,0.9657749,0.9497852,0.93007296,0.90581214,0.8760591,0.83977723,0.79589254,0.7433934,0.681493,0.60986114,0.528929,0.440234,0.34673858,0.25300458,0.16507731,0.089951225,0.034596868,0.0047061816,0.0047061816,0.034596868,0.089951225,0.16507731,0.25300458,0.34673858,0.440234,0.528929,0.60986114,0.681493,0.7433934,0.79589254,0.83977723,0.8760591,0.90581214,0.93007296,0.9497852,0.9657749,0.97874653,0.98928845,0.99788535,1.0049319,1.010747,1.0155861,1.0196538,1.0231125,1.0260912,1.0286918,1.0309949,1.0330639,1.034949,1.0366894,1.0383164,1.0398542,1.0413219,1.0427346,1.0441042,1.04544,1.0467491,1.0480373,1.0493091,1.0505681,1.051817,1.053058,1.0542928,1.0555227,1.0567487,1.0579718,1.0591925,1.0604115],[1.0604879,1.059269,1.0580482,1.0568252,1.0555991,1.0543692,1.0531344,1.0518935,1.0506445,1.0493855,1.0481137,1.0468255,1.0455164,1.0441806,1.042811,1.0413983,1.0399306,1.0383928,1.0367658,1.0350254,1.0331403,1.0310713,1.0287682,1.0261676,1.023189,1.0197302,1.0156626,1.0108234,1.0050083,0.9979618,0.9893649,0.978823,0.96585137,0.94986165,0.93014944,0.9058886,0.8761356,0.8398537,0.795969,0.7434699,0.68156946,0.6099376,0.52900547,0.44031045,0.34681502,0.25308102,0.16515376,0.09002767,0.03467331,0.004782625,0.004782625,0.03467331,0.09002767,0.16515376,0.25308102,0.34681502,0.44031045,0.52900547,0.6099376,0.68156946,0.7434699,0.795969,0.8398537,0.8761356,0.9058886,0.93014944,0.94986165,0.96585137,0.978823,0.9893649,0.9979618,1.0050083,1.0108234,1.0156626,1.0197302,1.023189,1.0261676,1.0287682,1.0310713,1.0331403,1.0350254,1.0367658,1.0383928,1.0399306,1.0413983,1.042811,1.0441806,1.0455164,1.0468255,1.0481137,1.0493855,1.0506445,1.0518935,1.0531344,1.0543692,1.0555991,1.0568252,1.0580482,1.059269,1.0604879],[1.0605778,1.0593588,1.0581381,1.056915,1.055689,1.0544591,1.0532243,1.0519834,1.0507344,1.0494754,1.0482036,1.0469154,1.0456063,1.0442705,1.0429009,1.0414882,1.0400205,1.0384827,1.0368557,1.0351152,1.0332302,1.0311612,1.0288581,1.0262575,1.0232788,1.0198201,1.0157524,1.0109133,1.0050982,0.99805164,0.98945475,0.97891283,0.9659412,0.94995147,0.93023926,0.90597844,0.8762254,0.8399435,0.79605883,0.7435597,0.6816593,0.61002743,0.5290953,0.44040027,0.34690484,0.25317085,0.1652436,0.09011751,0.034763146,0.0048724622,0.0048724622,0.034763146,0.09011751,0.1652436,0.25317085,0.34690484,0.44040027,0.5290953,0.61002743,0.6816593,0.7435597,0.79605883,0.8399435,0.8762254,0.90597844,0.93023926,0.94995147,0.9659412,0.97891283,0.98945475,0.99805164,1.0050982,1.0109133,1.0157524,1.0198201,1.0232788,1.0262575,1.0288581,1.0311612,1.0332302,1.0351152,1.0368557,1.0384827,1.0400205,1.0414882,1.0429009,1.0442705,1.0456063,1.0469154,1.0482036,1.0494754,1.0507344,1.0519834,1.0532243,1.0544591,1.055689,1.056915,1.0581381,1.0593588,1.0605778],[1.0606833,1.0594643,1.0582436,1.0570205,1.0557945,1.0545646,1.0533298,1.0520889,1.0508399,1.0495809,1.0483091,1.0470209,1.0457118,1.044376,1.0430064,1.0415937,1.040126,1.0385882,1.0369612,1.0352207,1.0333357,1.0312667,1.0289636,1.026363,1.0233843,1.0199256,1.0158579,1.0110188,1.0052037,0.9981572,0.9895603,0.9790184,0.96604675,0.950057,0.9303448,0.906084,0.876331,0.8400491,0.7961644,0.7436653,0.68176484,0.610133,0.52920085,0.44050586,0.34701043,0.25327644,0.16534917,0.09022308,0.034868725,0.0049780374,0.0049780374,0.034868725,0.09022308,0.16534917,0.25327644,0.34701043,0.44050586,0.52920085,0.610133,0.68176484,0.7436653,0.7961644,0.8400491,0.876331,0.906084,0.9303448,0.950057,0.96604675,0.9790184,0.9895603,0.9981572,1.0052037,1.0110188,1.0158579,1.0199256,1.0233843,1.026363,1.0289636,1.0312667,1.0333357,1.0352207,1.0369612,1.0385882,1.040126,1.0415937,1.0430064,1.044376,1.0457118,1.0470209,1.0483091,1.0495809,1.0508399,1.0520889,1.0533298,1.0545646,1.0557945,1.0570205,1.0582436,1.0594643,1.0606833],[1.0608073,1.0595884,1.0583677,1.0571446,1.0559186,1.0546887,1.0534539,1.052213,1.050964,1.049705,1.0484332,1.047145,1.0458359,1.0445001,1.0431305,1.0417178,1.0402501,1.0387123,1.0370853,1.0353448,1.0334598,1.0313908,1.0290877,1.0264871,1.0235084,1.0200497,1.015982,1.0111428,1.0053278,0.9982813,0.9896844,0.9791425,0.96617085,0.9501811,0.9304689,0.9062081,0.87645507,0.8401732,0.7962885,0.7437894,0.68188894,0.6102571,0.52932495,0.44062993,0.3471345,0.2534005,0.16547324,0.09034715,0.034992788,0.0051021036,0.0051021036,0.034992788,0.09034715,0.16547324,0.2534005,0.3471345,0.44062993,0.52932495,0.6102571,0.68188894,0.7437894,0.7962885,0.8401732,0.87645507,0.9062081,0.9304689,0.9501811,0.96617085,0.9791425,0.9896844,0.9982813,1.0053278,1.0111428,1.015982,1.0200497,1.0235084,1.0264871,1.0290877,1.0313908,1.0334598,1.0353448,1.0370853,1.0387123,1.0402501,1.0417178,1.0431305,1.0445001,1.0458359,1.047145,1.0484332,1.049705,1.050964,1.052213,1.0534539,1.0546887,1.0559186,1.0571446,1.0583677,1.0595884,1.0608073],[1.0609531,1.0597342,1.0585135,1.0572904,1.0560644,1.0548345,1.0535997,1.0523587,1.0511098,1.0498508,1.048579,1.0472908,1.0459816,1.0446459,1.0432763,1.0418636,1.0403959,1.038858,1.0372311,1.0354906,1.0336056,1.0315366,1.0292335,1.0266329,1.0236542,1.0201955,1.0161278,1.0112886,1.0054736,0.9984271,0.9898302,0.9792883,0.96631664,0.9503269,0.9306147,0.9063539,0.87660086,0.840319,0.7964343,0.74393517,0.68203473,0.6104029,0.52947074,0.4407757,0.34728026,0.25354627,0.16561903,0.090492934,0.035138577,0.0052478914,0.0052478914,0.035138577,0.090492934,0.16561903,0.25354627,0.34728026,0.4407757,0.52947074,0.6104029,0.68203473,0.74393517,0.7964343,0.840319,0.87660086,0.9063539,0.9306147,0.9503269,0.96631664,0.9792883,0.9898302,0.9984271,1.0054736,1.0112886,1.0161278,1.0201955,1.0236542,1.0266329,1.0292335,1.0315366,1.0336056,1.0354906,1.0372311,1.038858,1.0403959,1.0418636,1.0432763,1.0446459,1.0459816,1.0472908,1.048579,1.0498508,1.0511098,1.0523587,1.0535997,1.0548345,1.0560644,1.0572904,1.0585135,1.0597342,1.0609531],[1.0611244,1.0599055,1.0586848,1.0574617,1.0562357,1.0550058,1.053771,1.05253,1.0512811,1.0500221,1.0487503,1.0474621,1.046153,1.0448172,1.0434476,1.0420349,1.0405672,1.0390294,1.0374024,1.0356619,1.0337769,1.0317079,1.0294048,1.0268042,1.0238255,1.0203668,1.0162991,1.01146,1.0056449,0.9985984,0.9900015,0.9794596,0.96648794,0.9504982,0.930786,0.9065252,0.87677217,0.8404903,0.7966056,0.7441065,0.68220603,0.6105742,0.52964205,0.440947,0.34745157,0.25371757,0.16579033,0.090664245,0.035309885,0.0054191984,0.0054191984,0.035309885,0.090664245,0.16579033,0.25371757,0.34745157,0.440947,0.52964205,0.6105742,0.68220603,0.7441065,0.7966056,0.8404903,0.87677217,0.9065252,0.930786,0.9504982,0.96648794,0.9794596,0.9900015,0.9985984,1.0056449,1.01146,1.0162991,1.0203668,1.0238255,1.0268042,1.0294048,1.0317079,1.0337769,1.0356619,1.0374024,1.0390294,1.0405672,1.0420349,1.0434476,1.0448172,1.046153,1.0474621,1.0487503,1.0500221,1.0512811,1.05253,1.053771,1.0550058,1.0562357,1.0574617,1.0586848,1.0599055,1.0611244],[1.0613257,1.0601068,1.058886,1.057663,1.0564369,1.055207,1.0539722,1.0527313,1.0514823,1.0502234,1.0489515,1.0476633,1.0463542,1.0450184,1.0436488,1.0422361,1.0407684,1.0392306,1.0376036,1.0358632,1.0339781,1.0319091,1.029606,1.0270054,1.0240268,1.020568,1.0165004,1.0116612,1.0058461,0.9987997,0.9902028,0.97966087,0.9666892,0.9506995,0.9309873,0.9067265,0.87697345,0.84069157,0.7968069,0.74430776,0.6824073,0.6107755,0.52984333,0.44114828,0.34765285,0.25391886,0.16599162,0.09086552,0.035511162,0.0056204787,0.0056204787,0.035511162,0.09086552,0.16599162,0.25391886,0.34765285,0.44114828,0.52984333,0.6107755,0.6824073,0.74430776,0.7968069,0.84069157,0.87697345,0.9067265,0.9309873,0.9506995,0.9666892,0.97966087,0.9902028,0.9987997,1.0058461,1.0116612,1.0165004,1.020568,1.0240268,1.0270054,1.029606,1.0319091,1.0339781,1.0358632,1.0376036,1.0392306,1.0407684,1.0422361,1.0436488,1.0450184,1.0463542,1.0476633,1.0489515,1.0502234,1.0514823,1.0527313,1.0539722,1.055207,1.0564369,1.057663,1.058886,1.0601068,1.0613257],[1.0615622,1.0603433,1.0591226,1.0578995,1.0566734,1.0554435,1.0542088,1.0529678,1.0517188,1.0504599,1.049188,1.0478998,1.0465907,1.045255,1.0438854,1.0424726,1.0410049,1.0394671,1.0378401,1.0360997,1.0342146,1.0321456,1.0298425,1.027242,1.0242633,1.0208045,1.0167369,1.0118977,1.0060827,0.99903613,0.99043924,0.9798973,0.9669257,0.95093596,0.93122375,0.90696293,0.8772099,0.840928,0.7970433,0.7445442,0.6826438,0.6110119,0.5300798,0.44138476,0.34788933,0.25415534,0.1662281,0.091102004,0.035747647,0.005856961,0.005856961,0.035747647,0.091102004,0.1662281,0.25415534,0.34788933,0.44138476,0.5300798,0.6110119,0.6826438,0.7445442,0.7970433,0.840928,0.8772099,0.90696293,0.93122375,0.95093596,0.9669257,0.9798973,0.99043924,0.99903613,1.0060827,1.0118977,1.0167369,1.0208045,1.0242633,1.027242,1.0298425,1.0321456,1.0342146,1.0360997,1.0378401,1.0394671,1.0410049,1.0424726,1.0438854,1.045255,1.0465907,1.0478998,1.049188,1.0504599,1.0517188,1.0529678,1.0542088,1.0554435,1.0566734,1.0578995,1.0591226,1.0603433,1.0615622],[1.06184,1.0606211,1.0594004,1.0581774,1.0569513,1.0557214,1.0544866,1.0532457,1.0519967,1.0507377,1.0494659,1.0481777,1.0468686,1.0455328,1.0441632,1.0427505,1.0412828,1.039745,1.038118,1.0363775,1.0344925,1.0324235,1.0301204,1.0275198,1.0245411,1.0210824,1.0170147,1.0121756,1.0063605,0.99931395,0.99071705,0.98017514,0.9672035,0.9512138,0.93150157,0.90724075,0.8774877,0.84120584,0.79732114,0.744822,0.6829216,0.61128974,0.5303576,0.44166258,0.34816715,0.25443316,0.16650592,0.09137983,0.03602547,0.006134782,0.006134782,0.03602547,0.09137983,0.16650592,0.25443316,0.34816715,0.44166258,0.5303576,0.61128974,0.6829216,0.744822,0.79732114,0.84120584,0.8774877,0.90724075,0.93150157,0.9512138,0.9672035,0.98017514,0.99071705,0.99931395,1.0063605,1.0121756,1.0170147,1.0210824,1.0245411,1.0275198,1.0301204,1.0324235,1.0344925,1.0363775,1.038118,1.039745,1.0412828,1.0427505,1.0441632,1.0455328,1.0468686,1.0481777,1.0494659,1.0507377,1.0519967,1.0532457,1.0544866,1.0557214,1.0569513,1.0581774,1.0594004,1.0606211,1.06184],[1.0621663,1.0609474,1.0597267,1.0585036,1.0572776,1.0560477,1.0548129,1.0535719,1.052323,1.051064,1.0497922,1.048504,1.0471948,1.0458591,1.0444895,1.0430768,1.041609,1.0400712,1.0384443,1.0367038,1.0348188,1.0327498,1.0304466,1.0278461,1.0248674,1.0214087,1.017341,1.0125018,1.0066868,0.99964035,0.99104345,0.98050153,0.9675299,0.9515402,0.93182796,0.90756714,0.8778141,0.84153223,0.79764754,0.7451484,0.683248,0.61161613,0.530684,0.44198895,0.34849352,0.25475952,0.16683227,0.09170619,0.036351822,0.006461138,0.006461138,0.036351822,0.09170619,0.16683227,0.25475952,0.34849352,0.44198895,0.530684,0.61161613,0.683248,0.7451484,0.79764754,0.84153223,0.8778141,0.90756714,0.93182796,0.9515402,0.9675299,0.98050153,0.99104345,0.99964035,1.0066868,1.0125018,1.017341,1.0214087,1.0248674,1.0278461,1.0304466,1.0327498,1.0348188,1.0367038,1.0384443,1.0400712,1.041609,1.0430768,1.0444895,1.0458591,1.0471948,1.048504,1.0497922,1.051064,1.052323,1.0535719,1.0548129,1.0560477,1.0572776,1.0585036,1.0597267,1.0609474,1.0621663],[1.0625497,1.0613308,1.0601101,1.058887,1.0576609,1.056431,1.0551963,1.0539553,1.0527064,1.0514474,1.0501755,1.0488874,1.0475782,1.0462425,1.0448729,1.0434601,1.0419924,1.0404546,1.0388277,1.0370872,1.0352021,1.0331331,1.03083,1.0282295,1.0252508,1.021792,1.0177244,1.0128852,1.0070702,1.0000236,0.99142677,0.98088485,0.9679132,0.9519235,0.9322113,0.90795046,0.87819743,0.84191555,0.79803085,0.74553174,0.6836313,0.61199945,0.5310673,0.4423723,0.34887686,0.25514287,0.1672156,0.09208951,0.03673515,0.0068444656,0.0068444656,0.03673515,0.09208951,0.1672156,0.25514287,0.34887686,0.4423723,0.5310673,0.61199945,0.6836313,0.74553174,0.79803085,0.84191555,0.87819743,0.90795046,0.9322113,0.9519235,0.9679132,0.98088485,0.99142677,1.0000236,1.0070702,1.0128852,1.0177244,1.021792,1.0252508,1.0282295,1.03083,1.0331331,1.0352021,1.0370872,1.0388277,1.0404546,1.0419924,1.0434601,1.0448729,1.0462425,1.0475782,1.0488874,1.0501755,1.0514474,1.0527064,1.0539553,1.0551963,1.056431,1.0576609,1.058887,1.0601101,1.0613308,1.0625497],[1.0629998,1.0617809,1.0605602,1.0593371,1.0581111,1.0568812,1.0556464,1.0544055,1.0531565,1.0518975,1.0506257,1.0493375,1.0480283,1.0466926,1.045323,1.0439103,1.0424426,1.0409048,1.0392778,1.0375373,1.0356523,1.0335833,1.0312802,1.0286796,1.0257009,1.0222422,1.0181745,1.0133353,1.0075203,1.0004739,0.99187696,0.98133504,0.9683634,0.9523737,0.9326615,0.90840065,0.8786476,0.84236574,0.79848105,0.74598193,0.6840815,0.61244965,0.5315175,0.44282246,0.34932703,0.25559303,0.1676658,0.092539705,0.03718534,0.0072946576,0.0072946576,0.03718534,0.092539705,0.1676658,0.25559303,0.34932703,0.44282246,0.5315175,0.61244965,0.6840815,0.74598193,0.79848105,0.84236574,0.8786476,0.90840065,0.9326615,0.9523737,0.9683634,0.98133504,0.99187696,1.0004739,1.0075203,1.0133353,1.0181745,1.0222422,1.0257009,1.0286796,1.0312802,1.0335833,1.0356523,1.0375373,1.0392778,1.0409048,1.0424426,1.0439103,1.045323,1.0466926,1.0480283,1.0493375,1.0506257,1.0518975,1.0531565,1.0544055,1.0556464,1.0568812,1.0581111,1.0593371,1.0605602,1.0617809,1.0629998],[1.0635285,1.0623096,1.0610889,1.0598658,1.0586398,1.0574099,1.0561751,1.0549341,1.0536852,1.0524262,1.0511544,1.0498662,1.048557,1.0472213,1.0458517,1.044439,1.0429713,1.0414335,1.0398065,1.038066,1.036181,1.034112,1.0318089,1.0292083,1.0262296,1.0227709,1.0187032,1.013864,1.008049,1.0010024,0.9924056,0.9818637,0.96889204,0.9529023,0.9331901,0.9089293,0.87917626,0.8428944,0.7990097,0.74651057,0.6846101,0.6129783,0.53204614,0.44335112,0.3498557,0.2561217,0.16819443,0.09306835,0.037713982,0.007823298,0.007823298,0.037713982,0.09306835,0.16819443,0.2561217,0.3498557,0.44335112,0.53204614,0.6129783,0.6846101,0.74651057,0.7990097,0.8428944,0.87917626,0.9089293,0.9331901,0.9529023,0.96889204,0.9818637,0.9924056,1.0010024,1.008049,1.013864,1.0187032,1.0227709,1.0262296,1.0292083,1.0318089,1.034112,1.036181,1.038066,1.0398065,1.0414335,1.0429713,1.044439,1.0458517,1.0472213,1.048557,1.0498662,1.0511544,1.0524262,1.0536852,1.0549341,1.0561751,1.0574099,1.0586398,1.0598658,1.0610889,1.0623096,1.0635285],[1.0641491,1.0629302,1.0617095,1.0604864,1.0592604,1.0580305,1.0567957,1.0555547,1.0543058,1.0530468,1.051775,1.0504868,1.0491776,1.0478419,1.0464723,1.0450596,1.0435919,1.042054,1.0404271,1.0386866,1.0368016,1.0347326,1.0324295,1.0298289,1.0268502,1.0233915,1.0193238,1.0144846,1.0086696,1.0016232,0.99302626,0.98248434,0.9695127,0.953523,0.9338108,0.90954995,0.8797969,0.84351504,0.79963034,0.7471312,0.6852308,0.61359894,0.5326668,0.44397175,0.35047632,0.25674233,0.16881508,0.093688995,0.038334634,0.008443948,0.008443948,0.038334634,0.093688995,0.16881508,0.25674233,0.35047632,0.44397175,0.5326668,0.61359894,0.6852308,0.7471312,0.79963034,0.84351504,0.8797969,0.90954995,0.9338108,0.953523,0.9695127,0.98248434,0.99302626,1.0016232,1.0086696,1.0144846,1.0193238,1.0233915,1.0268502,1.0298289,1.0324295,1.0347326,1.0368016,1.0386866,1.0404271,1.042054,1.0435919,1.0450596,1.0464723,1.0478419,1.0491776,1.0504868,1.051775,1.0530468,1.0543058,1.0555547,1.0567957,1.0580305,1.0592604,1.0604864,1.0617095,1.0629302,1.0641491],[1.0648777,1.0636588,1.0624381,1.061215,1.059989,1.0587591,1.0575243,1.0562834,1.0550344,1.0537754,1.0525036,1.0512154,1.0499063,1.0485705,1.0472009,1.0457882,1.0443205,1.0427827,1.0411557,1.0394152,1.0375302,1.0354612,1.0331581,1.0305575,1.0275788,1.0241201,1.0200524,1.0152133,1.0093982,1.0023516,0.99375474,0.9832128,0.9702412,0.95425147,0.93453926,0.91027844,0.8805254,0.8442435,0.80035883,0.7478597,0.6859593,0.61432743,0.5333953,0.4447003,0.35120487,0.25747088,0.16954361,0.09441753,0.039063167,0.009172481,0.009172481,0.039063167,0.09441753,0.16954361,0.25747088,0.35120487,0.4447003,0.5333953,0.61432743,0.6859593,0.7478597,0.80035883,0.8442435,0.8805254,0.91027844,0.93453926,0.95425147,0.9702412,0.9832128,0.99375474,1.0023516,1.0093982,1.0152133,1.0200524,1.0241201,1.0275788,1.0305575,1.0331581,1.0354612,1.0375302,1.0394152,1.0411557,1.0427827,1.0443205,1.0457882,1.0472009,1.0485705,1.0499063,1.0512154,1.0525036,1.0537754,1.0550344,1.0562834,1.0575243,1.0587591,1.059989,1.061215,1.0624381,1.0636588,1.0648777],[1.0657327,1.0645138,1.0632931,1.06207,1.060844,1.0596141,1.0583793,1.0571383,1.0558894,1.0546304,1.0533586,1.0520704,1.0507612,1.0494255,1.0480559,1.0466431,1.0451754,1.0436376,1.0420107,1.0402702,1.0383852,1.0363162,1.034013,1.0314125,1.0284338,1.0249751,1.0209074,1.0160682,1.0102532,1.0032066,0.9946097,0.9840678,0.97109616,0.95510644,0.9353942,0.9111334,0.8813804,0.8450985,0.8012138,0.7487147,0.68681425,0.6151824,0.53425026,0.44555527,0.35205984,0.25832582,0.17039858,0.09527249,0.03991813,0.010027443,0.010027443,0.03991813,0.09527249,0.17039858,0.25832582,0.35205984,0.44555527,0.53425026,0.6151824,0.68681425,0.7487147,0.8012138,0.8450985,0.8813804,0.9111334,0.9353942,0.95510644,0.97109616,0.9840678,0.9946097,1.0032066,1.0102532,1.0160682,1.0209074,1.0249751,1.0284338,1.0314125,1.034013,1.0363162,1.0383852,1.0402702,1.0420107,1.0436376,1.0451754,1.0466431,1.0480559,1.0494255,1.0507612,1.0520704,1.0533586,1.0546304,1.0558894,1.0571383,1.0583793,1.0596141,1.060844,1.06207,1.0632931,1.0645138,1.0657327],[1.0667357,1.0655168,1.0642961,1.063073,1.061847,1.0606171,1.0593823,1.0581414,1.0568924,1.0556334,1.0543616,1.0530734,1.0517642,1.0504285,1.0490589,1.0476462,1.0461785,1.0446407,1.0430137,1.0412732,1.0393882,1.0373192,1.0350161,1.0324155,1.0294368,1.0259781,1.0219104,1.0170712,1.0112562,1.0042096,0.9956128,0.9850709,0.97209924,0.9561095,0.9363973,0.9121365,0.88238347,0.8461016,0.8022169,0.7497178,0.68781734,0.6161855,0.53525335,0.4465583,0.35306287,0.25932887,0.17140163,0.09627554,0.040921178,0.011030493,0.011030493,0.040921178,0.09627554,0.17140163,0.25932887,0.35306287,0.4465583,0.53525335,0.6161855,0.68781734,0.7497178,0.8022169,0.8461016,0.88238347,0.9121365,0.9363973,0.9561095,0.97209924,0.9850709,0.9956128,1.0042096,1.0112562,1.0170712,1.0219104,1.0259781,1.0294368,1.0324155,1.0350161,1.0373192,1.0393882,1.0412732,1.0430137,1.0446407,1.0461785,1.0476462,1.0490589,1.0504285,1.0517642,1.0530734,1.0543616,1.0556334,1.0568924,1.0581414,1.0593823,1.0606171,1.061847,1.063073,1.0642961,1.0655168,1.0667357],[1.0679121,1.0666932,1.0654725,1.0642494,1.0630233,1.0617934,1.0605587,1.0593177,1.0580688,1.0568098,1.0555379,1.0542498,1.0529406,1.0516049,1.0502353,1.0488225,1.0473548,1.045817,1.04419,1.0424496,1.0405645,1.0384955,1.0361924,1.0335919,1.0306132,1.0271544,1.0230868,1.0182476,1.0124326,1.0053861,0.9967892,0.9862473,0.97327566,0.95728594,0.93757373,0.9133129,0.8835599,0.847278,0.8033933,0.7508942,0.68899375,0.6173619,0.53642976,0.4477347,0.35423928,0.2605053,0.17257804,0.097451955,0.04209759,0.012206907,0.012206907,0.04209759,0.097451955,0.17257804,0.2605053,0.35423928,0.4477347,0.53642976,0.6173619,0.68899375,0.7508942,0.8033933,0.847278,0.8835599,0.9133129,0.93757373,0.95728594,0.97327566,0.9862473,0.9967892,1.0053861,1.0124326,1.0182476,1.0230868,1.0271544,1.0306132,1.0335919,1.0361924,1.0384955,1.0405645,1.0424496,1.04419,1.045817,1.0473548,1.0488225,1.0502353,1.0516049,1.0529406,1.0542498,1.0555379,1.0568098,1.0580688,1.0593177,1.0605587,1.0617934,1.0630233,1.0642494,1.0654725,1.0666932,1.0679121],[1.0692914,1.0680724,1.0668517,1.0656286,1.0644026,1.0631727,1.0619379,1.060697,1.059448,1.058189,1.0569172,1.055629,1.0543199,1.0529841,1.0516145,1.0502018,1.0487341,1.0471963,1.0455693,1.0438288,1.0419438,1.0398748,1.0375717,1.0349711,1.0319924,1.0285337,1.024466,1.0196269,1.0138118,1.0067652,0.9981684,0.9876265,0.97465485,0.95866513,0.9389529,0.9146921,0.8849391,0.8486572,0.8047725,0.7522734,0.69037294,0.6187411,0.53780895,0.44911394,0.3556185,0.2618845,0.17395726,0.09883116,0.0434768,0.013586117,0.013586117,0.0434768,0.09883116,0.17395726,0.2618845,0.3556185,0.44911394,0.53780895,0.6187411,0.69037294,0.7522734,0.8047725,0.8486572,0.8849391,0.9146921,0.9389529,0.95866513,0.97465485,0.9876265,0.9981684,1.0067652,1.0138118,1.0196269,1.024466,1.0285337,1.0319924,1.0349711,1.0375717,1.0398748,1.0419438,1.0438288,1.0455693,1.0471963,1.0487341,1.0502018,1.0516145,1.0529841,1.0543199,1.055629,1.0569172,1.058189,1.059448,1.060697,1.0619379,1.0631727,1.0644026,1.0656286,1.0668517,1.0680724,1.0692914],[1.0709076,1.0696887,1.068468,1.0672449,1.0660188,1.0647889,1.0635542,1.0623132,1.0610642,1.0598053,1.0585334,1.0572453,1.0559361,1.0546004,1.0532308,1.051818,1.0503503,1.0488125,1.0471855,1.0454451,1.04356,1.041491,1.0391879,1.0365874,1.0336087,1.0301499,1.0260823,1.0212431,1.0154281,1.0083815,0.99978465,0.98924273,0.9762711,0.9602814,0.94056916,0.91630834,0.8865553,0.85027343,0.80638874,0.7538896,0.6919892,0.62035733,0.5394252,0.45073017,0.35723475,0.26350075,0.17557348,0.1004474,0.045093037,0.015202352,0.015202352,0.045093037,0.1004474,0.17557348,0.26350075,0.35723475,0.45073017,0.5394252,0.62035733,0.6919892,0.7538896,0.80638874,0.85027343,0.8865553,0.91630834,0.94056916,0.9602814,0.9762711,0.98924273,0.99978465,1.0083815,1.0154281,1.0212431,1.0260823,1.0301499,1.0336087,1.0365874,1.0391879,1.041491,1.04356,1.0454451,1.0471855,1.0488125,1.0503503,1.051818,1.0532308,1.0546004,1.0559361,1.0572453,1.0585334,1.0598053,1.0610642,1.0623132,1.0635542,1.0647889,1.0660188,1.0672449,1.068468,1.0696887,1.0709076],[1.0728006,1.0715817,1.070361,1.0691379,1.0679119,1.066682,1.0654472,1.0642062,1.0629573,1.0616983,1.0604265,1.0591383,1.0578291,1.0564934,1.0551238,1.053711,1.0522434,1.0507056,1.0490786,1.0473381,1.0454531,1.0433841,1.041081,1.0384804,1.0355017,1.032043,1.0279753,1.0231361,1.0173211,1.0102745,1.0016776,0.9911357,0.9781641,0.96217436,0.94246215,0.9182013,0.8884483,0.8521664,0.8082817,0.7557826,0.69388217,0.6222503,0.5413182,0.4526232,0.35912776,0.26539376,0.1774665,0.102340415,0.04698605,0.017095365,0.017095365,0.04698605,0.102340415,0.1774665,0.26539376,0.35912776,0.4526232,0.5413182,0.6222503,0.69388217,0.7557826,0.8082817,0.8521664,0.8884483,0.9182013,0.94246215,0.96217436,0.9781641,0.9911357,1.0016776,1.0102745,1.0173211,1.0231361,1.0279753,1.032043,1.0355017,1.0384804,1.041081,1.0433841,1.0454531,1.0473381,1.0490786,1.0507056,1.0522434,1.053711,1.0551238,1.0564934,1.0578291,1.0591383,1.0604265,1.0616983,1.0629573,1.0642062,1.0654472,1.066682,1.0679119,1.0691379,1.070361,1.0715817,1.0728006],[1.0750164,1.0737975,1.0725768,1.0713537,1.0701276,1.0688977,1.067663,1.066422,1.065173,1.0639141,1.0626422,1.061354,1.0600449,1.0587091,1.0573395,1.0559268,1.0544591,1.0529213,1.0512943,1.0495539,1.0476688,1.0455998,1.0432967,1.0406961,1.0377175,1.0342587,1.0301911,1.0253519,1.0195369,1.0124904,1.0038935,0.9933516,0.98037994,0.9643902,0.944678,0.9204172,0.89066416,0.8543823,0.8104976,0.75799847,0.696098,0.6244662,0.54353404,0.454839,0.36134356,0.26760957,0.17968233,0.10455623,0.049201876,0.01931119,0.01931119,0.049201876,0.10455623,0.17968233,0.26760957,0.36134356,0.454839,0.54353404,0.6244662,0.696098,0.75799847,0.8104976,0.8543823,0.89066416,0.9204172,0.944678,0.9643902,0.98037994,0.9933516,1.0038935,1.0124904,1.0195369,1.0253519,1.0301911,1.0342587,1.0377175,1.0406961,1.0432967,1.0455998,1.0476688,1.0495539,1.0512943,1.0529213,1.0544591,1.0559268,1.0573395,1.0587091,1.0600449,1.061354,1.0626422,1.0639141,1.065173,1.066422,1.067663,1.0688977,1.0701276,1.0713537,1.0725768,1.0737975,1.0750164],[1.0776082,1.0763893,1.0751686,1.0739455,1.0727195,1.0714896,1.0702548,1.0690138,1.0677649,1.0665059,1.0652341,1.0639459,1.0626367,1.061301,1.0599314,1.0585186,1.057051,1.0555131,1.0538862,1.0521457,1.0502607,1.0481917,1.0458885,1.043288,1.0403093,1.0368506,1.0327829,1.0279437,1.0221287,1.0150821,1.0064852,0.99594337,0.9829717,0.966982,0.9472698,0.923009,0.89325595,0.85697407,0.8130894,0.76059026,0.6986898,0.62705797,0.5461258,0.4574308,0.36393538,0.27020139,0.18227413,0.10714804,0.05179368,0.021902993,0.021902993,0.05179368,0.10714804,0.18227413,0.27020139,0.36393538,0.4574308,0.5461258,0.62705797,0.6986898,0.76059026,0.8130894,0.85697407,0.89325595,0.923009,0.9472698,0.966982,0.9829717,0.99594337,1.0064852,1.0150821,1.0221287,1.0279437,1.0327829,1.0368506,1.0403093,1.043288,1.0458885,1.0481917,1.0502607,1.0521457,1.0538862,1.0555131,1.057051,1.0585186,1.0599314,1.061301,1.0626367,1.0639459,1.0652341,1.0665059,1.0677649,1.0690138,1.0702548,1.0714896,1.0727195,1.0739455,1.0751686,1.0763893,1.0776082],[1.0806373,1.0794184,1.0781977,1.0769746,1.0757486,1.0745187,1.0732839,1.072043,1.070794,1.069535,1.0682632,1.066975,1.0656658,1.0643301,1.0629605,1.0615478,1.06008,1.0585423,1.0569153,1.0551748,1.0532898,1.0512208,1.0489177,1.0463171,1.0433384,1.0398797,1.035812,1.0309728,1.0251578,1.0181112,1.0095143,0.9989724,0.9860008,0.97001106,0.95029885,0.926038,0.896285,0.8600031,0.8161184,0.7636193,0.70171887,0.630087,0.5491549,0.46045986,0.36696443,0.27323043,0.18530318,0.11017709,0.054822735,0.02493205,0.02493205,0.054822735,0.11017709,0.18530318,0.27323043,0.36696443,0.46045986,0.5491549,0.630087,0.70171887,0.7636193,0.8161184,0.8600031,0.896285,0.926038,0.95029885,0.97001106,0.9860008,0.9989724,1.0095143,1.0181112,1.0251578,1.0309728,1.035812,1.0398797,1.0433384,1.0463171,1.0489177,1.0512208,1.0532898,1.0551748,1.0569153,1.0585423,1.06008,1.0615478,1.0629605,1.0643301,1.0656658,1.066975,1.0682632,1.069535,1.070794,1.072043,1.0732839,1.0745187,1.0757486,1.0769746,1.0781977,1.0794184,1.0806373],[1.0841738,1.0829549,1.0817342,1.0805111,1.079285,1.0780551,1.0768204,1.0755794,1.0743304,1.0730715,1.0717996,1.0705115,1.0692023,1.0678666,1.066497,1.0650842,1.0636165,1.0620787,1.0604517,1.0587113,1.0568262,1.0547572,1.0524541,1.0498536,1.0468749,1.0434161,1.0393485,1.0345093,1.0286943,1.0216478,1.0130509,1.002509,0.98953736,0.97354764,0.9538354,0.9295746,0.8998216,0.8635397,0.819655,0.7671559,0.70525545,0.6336236,0.55269146,0.46399644,0.370501,0.27676702,0.18883976,0.113713674,0.05835931,0.028468627,0.028468627,0.05835931,0.113713674,0.18883976,0.27676702,0.370501,0.46399644,0.55269146,0.6336236,0.70525545,0.7671559,0.819655,0.8635397,0.8998216,0.9295746,0.9538354,0.97354764,0.98953736,1.002509,1.0130509,1.0216478,1.0286943,1.0345093,1.0393485,1.0434161,1.0468749,1.0498536,1.0524541,1.0547572,1.0568262,1.0587113,1.0604517,1.0620787,1.0636165,1.0650842,1.066497,1.0678666,1.0692023,1.0705115,1.0717996,1.0730715,1.0743304,1.0755794,1.0768204,1.0780551,1.079285,1.0805111,1.0817342,1.0829549,1.0841738],[1.0882983,1.0870794,1.0858587,1.0846356,1.0834095,1.0821797,1.0809449,1.0797039,1.078455,1.077196,1.0759242,1.074636,1.0733268,1.0719911,1.0706215,1.0692087,1.067741,1.0662032,1.0645763,1.0628358,1.0609508,1.0588818,1.0565786,1.0539781,1.0509994,1.0475407,1.043473,1.0386338,1.0328188,1.0257722,1.0171753,1.0066334,0.99366176,0.97767204,0.95795983,0.933699,0.903946,0.8676641,0.8237794,0.7712803,0.70937985,0.637748,0.55681586,0.46812084,0.3746254,0.28089142,0.19296417,0.117838085,0.06248372,0.032593034,0.032593034,0.06248372,0.117838085,0.19296417,0.28089142,0.3746254,0.46812084,0.55681586,0.637748,0.70937985,0.7712803,0.8237794,0.8676641,0.903946,0.933699,0.95795983,0.97767204,0.99366176,1.0066334,1.0171753,1.0257722,1.0328188,1.0386338,1.043473,1.0475407,1.0509994,1.0539781,1.0565786,1.0588818,1.0609508,1.0628358,1.0645763,1.0662032,1.067741,1.0692087,1.0706215,1.0719911,1.0733268,1.074636,1.0759242,1.077196,1.078455,1.0797039,1.0809449,1.0821797,1.0834095,1.0846356,1.0858587,1.0870794,1.0882983],[1.0931017,1.0918828,1.0906621,1.089439,1.088213,1.0869831,1.0857483,1.0845073,1.0832584,1.0819994,1.0807276,1.0794394,1.0781302,1.0767945,1.0754249,1.0740122,1.0725445,1.0710067,1.0693797,1.0676392,1.0657542,1.0636852,1.061382,1.0587815,1.0558028,1.0523441,1.0482764,1.0434372,1.0376222,1.0305758,1.0219789,1.0114369,0.9984653,0.9824756,0.96276337,0.93850255,0.9087495,0.87246764,0.82858294,0.7760838,0.7141834,0.64255154,0.5616194,0.47292435,0.37942892,0.28569493,0.19776769,0.12264159,0.06728724,0.03739655,0.03739655,0.06728724,0.12264159,0.19776769,0.28569493,0.37942892,0.47292435,0.5616194,0.64255154,0.7141834,0.7760838,0.82858294,0.87246764,0.9087495,0.93850255,0.96276337,0.9824756,0.9984653,1.0114369,1.0219789,1.0305758,1.0376222,1.0434372,1.0482764,1.0523441,1.0558028,1.0587815,1.061382,1.0636852,1.0657542,1.0676392,1.0693797,1.0710067,1.0725445,1.0740122,1.0754249,1.0767945,1.0781302,1.0794394,1.0807276,1.0819994,1.0832584,1.0845073,1.0857483,1.0869831,1.088213,1.089439,1.0906621,1.0918828,1.0931017],[1.0986875,1.0974686,1.0962479,1.0950248,1.0937988,1.0925689,1.0913341,1.0900931,1.0888442,1.0875852,1.0863134,1.0850252,1.083716,1.0823803,1.0810107,1.079598,1.0781302,1.0765924,1.0749655,1.073225,1.07134,1.069271,1.0669678,1.0643673,1.0613886,1.0579299,1.0538622,1.049023,1.043208,1.0361614,1.0275645,1.0170226,1.004051,0.9880613,0.9683491,0.9440883,0.91433525,0.87805337,0.8341687,0.78166956,0.7197691,0.6481373,0.56720513,0.47851008,0.38501465,0.29128066,0.2033534,0.12822732,0.07287296,0.042982273,0.042982273,0.07287296,0.12822732,0.2033534,0.29128066,0.38501465,0.47851008,0.56720513,0.6481373,0.7197691,0.78166956,0.8341687,0.87805337,0.91433525,0.9440883,0.9683491,0.9880613,1.004051,1.0170226,1.0275645,1.0361614,1.043208,1.049023,1.0538622,1.0579299,1.0613886,1.0643673,1.0669678,1.069271,1.07134,1.073225,1.0749655,1.0765924,1.0781302,1.079598,1.0810107,1.0823803,1.083716,1.0850252,1.0863134,1.0875852,1.0888442,1.0900931,1.0913341,1.0925689,1.0937988,1.0950248,1.0962479,1.0974686,1.0986875],[1.1051711,1.1039522,1.1027315,1.1015084,1.1002823,1.0990524,1.0978177,1.0965767,1.0953277,1.0940688,1.0927969,1.0915087,1.0901996,1.0888638,1.0874943,1.0860815,1.0846138,1.083076,1.081449,1.0797086,1.0778235,1.0757545,1.0734514,1.0708508,1.0678722,1.0644134,1.0603458,1.0555066,1.0496916,1.042645,1.0340481,1.0235062,1.0105345,0.99454486,0.97483265,0.9505718,0.9208188,0.88453686,0.8406522,0.78815305,0.7262527,0.65462077,0.5736886,0.48499364,0.3914982,0.2977642,0.20983696,0.13471088,0.07935651,0.049465824,0.049465824,0.07935651,0.13471088,0.20983696,0.2977642,0.3914982,0.48499364,0.5736886,0.65462077,0.7262527,0.78815305,0.8406522,0.88453686,0.9208188,0.9505718,0.97483265,0.99454486,1.0105345,1.0235062,1.0340481,1.042645,1.0496916,1.0555066,1.0603458,1.0644134,1.0678722,1.0708508,1.0734514,1.0757545,1.0778235,1.0797086,1.081449,1.083076,1.0846138,1.0860815,1.0874943,1.0888638,1.0901996,1.0915087,1.0927969,1.0940688,1.0953277,1.0965767,1.0978177,1.0990524,1.1002823,1.1015084,1.1027315,1.1039522,1.1051711],[1.1126809,1.111462,1.1102413,1.1090182,1.1077921,1.1065623,1.1053275,1.1040865,1.1028376,1.1015786,1.1003067,1.0990186,1.0977094,1.0963737,1.0950041,1.0935913,1.0921236,1.0905858,1.0889589,1.0872184,1.0853333,1.0832644,1.0809612,1.0783607,1.075382,1.0719233,1.0678556,1.0630164,1.0572014,1.0501549,1.041558,1.0310161,1.0180445,1.0020547,0.98234254,0.9580817,0.9283287,0.8920468,0.8481621,0.795663,0.73376256,0.6621307,0.5811986,0.49250352,0.3990081,0.3052741,0.21734686,0.14222077,0.08686641,0.056975722,0.056975722,0.08686641,0.14222077,0.21734686,0.3052741,0.3990081,0.49250352,0.5811986,0.6621307,0.73376256,0.795663,0.8481621,0.8920468,0.9283287,0.9580817,0.98234254,1.0020547,1.0180445,1.0310161,1.041558,1.0501549,1.0572014,1.0630164,1.0678556,1.0719233,1.075382,1.0783607,1.0809612,1.0832644,1.0853333,1.0872184,1.0889589,1.0905858,1.0921236,1.0935913,1.0950041,1.0963737,1.0977094,1.0990186,1.1003067,1.1015786,1.1028376,1.1040865,1.1053275,1.1065623,1.1077921,1.1090182,1.1102413,1.111462,1.1126809],[1.1213585,1.1201396,1.1189189,1.1176958,1.1164697,1.1152399,1.1140051,1.1127641,1.1115152,1.1102562,1.1089844,1.1076962,1.106387,1.1050513,1.1036817,1.1022689,1.1008012,1.0992634,1.0976365,1.095896,1.094011,1.091942,1.0896388,1.0870383,1.0840596,1.0806009,1.0765332,1.071694,1.065879,1.0588324,1.0502355,1.0396936,1.026722,1.0107323,0.9910201,0.96675926,0.93700624,0.90072435,0.85683966,0.80434054,0.7424401,0.67080826,0.5898761,0.50118107,0.40768564,0.31395164,0.2260244,0.15089831,0.09554395,0.065653265,0.065653265,0.09554395,0.15089831,0.2260244,0.31395164,0.40768564,0.50118107,0.5898761,0.67080826,0.7424401,0.80434054,0.85683966,0.90072435,0.93700624,0.96675926,0.9910201,1.0107323,1.026722,1.0396936,1.0502355,1.0588324,1.065879,1.071694,1.0765332,1.0806009,1.0840596,1.0870383,1.0896388,1.091942,1.094011,1.095896,1.0976365,1.0992634,1.1008012,1.1022689,1.1036817,1.1050513,1.106387,1.1076962,1.1089844,1.1102562,1.1115152,1.1127641,1.1140051,1.1152399,1.1164697,1.1176958,1.1189189,1.1201396,1.1213585],[1.1313571,1.1301382,1.1289175,1.1276944,1.1264683,1.1252384,1.1240036,1.1227627,1.1215137,1.1202548,1.1189829,1.1176947,1.1163856,1.1150498,1.1136802,1.1122675,1.1107998,1.109262,1.107635,1.1058946,1.1040095,1.1019405,1.0996374,1.0970368,1.0940582,1.0905994,1.0865318,1.0816926,1.0758775,1.068831,1.0602341,1.0496922,1.0367205,1.0207309,1.0010186,0.9767578,0.9470048,0.9107229,0.8668382,0.8143391,0.75243866,0.6808068,0.5998747,0.5111796,0.4176842,0.32395023,0.23602296,0.16089687,0.10554251,0.07565183,0.07565183,0.10554251,0.16089687,0.23602296,0.32395023,0.4176842,0.5111796,0.5998747,0.6808068,0.75243866,0.8143391,0.8668382,0.9107229,0.9470048,0.9767578,1.0010186,1.0207309,1.0367205,1.0496922,1.0602341,1.068831,1.0758775,1.0816926,1.0865318,1.0905994,1.0940582,1.0970368,1.0996374,1.1019405,1.1040095,1.1058946,1.107635,1.109262,1.1107998,1.1122675,1.1136802,1.1150498,1.1163856,1.1176947,1.1189829,1.1202548,1.1215137,1.1227627,1.1240036,1.1252384,1.1264683,1.1276944,1.1289175,1.1301382,1.1313571],[1.1428405,1.1416216,1.1404009,1.1391778,1.1379517,1.1367218,1.1354871,1.1342461,1.1329972,1.1317382,1.1304663,1.1291782,1.127869,1.1265333,1.1251637,1.1237509,1.1222832,1.1207454,1.1191185,1.117378,1.1154929,1.113424,1.1111208,1.1085203,1.1055416,1.1020828,1.0980152,1.093176,1.087361,1.0803144,1.0717175,1.0611756,1.048204,1.0322143,1.0125021,0.98824126,0.9584882,0.92220634,0.87832165,0.82582253,0.7639221,0.69229025,0.6113581,0.5226631,0.42916766,0.33543366,0.24750641,0.17238033,0.11702596,0.08713528,0.08713528,0.11702596,0.17238033,0.24750641,0.33543366,0.42916766,0.5226631,0.6113581,0.69229025,0.7639221,0.82582253,0.87832165,0.92220634,0.9584882,0.98824126,1.0125021,1.0322143,1.048204,1.0611756,1.0717175,1.0803144,1.087361,1.093176,1.0980152,1.1020828,1.1055416,1.1085203,1.1111208,1.113424,1.1154929,1.117378,1.1191185,1.1207454,1.1222832,1.1237509,1.1251637,1.1265333,1.127869,1.1291782,1.1304663,1.1317382,1.1329972,1.1342461,1.1354871,1.1367218,1.1379517,1.1391778,1.1404009,1.1416216,1.1428405],[1.1559803,1.1547614,1.1535407,1.1523176,1.1510916,1.1498617,1.1486269,1.147386,1.146137,1.144878,1.1436062,1.142318,1.1410089,1.1396731,1.1383035,1.1368908,1.1354231,1.1338853,1.1322583,1.1305178,1.1286328,1.1265638,1.1242607,1.1216601,1.1186814,1.1152227,1.111155,1.1063159,1.1005008,1.0934542,1.0848573,1.0743154,1.0613438,1.0453541,1.0256419,1.001381,0.97162807,0.9353461,0.8914615,0.8389623,0.77706194,0.70543003,0.6244979,0.5358029,0.44230747,0.34857348,0.26064622,0.18552014,0.13016577,0.10027509,0.10027509,0.13016577,0.18552014,0.26064622,0.34857348,0.44230747,0.5358029,0.6244979,0.70543003,0.77706194,0.8389623,0.8914615,0.9353461,0.97162807,1.001381,1.0256419,1.0453541,1.0613438,1.0743154,1.0848573,1.0934542,1.1005008,1.1063159,1.111155,1.1152227,1.1186814,1.1216601,1.1242607,1.1265638,1.1286328,1.1305178,1.1322583,1.1338853,1.1354231,1.1368908,1.1383035,1.1396731,1.1410089,1.142318,1.1436062,1.144878,1.146137,1.147386,1.1486269,1.1498617,1.1510916,1.1523176,1.1535407,1.1547614,1.1559803],[1.1709516,1.1697327,1.168512,1.1672889,1.1660628,1.164833,1.1635982,1.1623572,1.1611083,1.1598493,1.1585774,1.1572893,1.1559801,1.1546444,1.1532748,1.151862,1.1503943,1.1488565,1.1472296,1.1454891,1.143604,1.141535,1.1392319,1.1366314,1.1336527,1.130194,1.1261263,1.1212871,1.1154721,1.1084255,1.0998286,1.0892867,1.076315,1.0603254,1.0406132,1.0163523,0.9865993,0.95031744,0.90643275,0.85393363,0.7920332,0.72040135,0.6394692,0.55077416,0.45727873,0.36354476,0.27561748,0.2004914,0.14513704,0.11524636,0.11524636,0.14513704,0.2004914,0.27561748,0.36354476,0.45727873,0.55077416,0.6394692,0.72040135,0.7920332,0.85393363,0.90643275,0.95031744,0.9865993,1.0163523,1.0406132,1.0603254,1.076315,1.0892867,1.0998286,1.1084255,1.1154721,1.1212871,1.1261263,1.130194,1.1336527,1.1366314,1.1392319,1.141535,1.143604,1.1454891,1.1472296,1.1488565,1.1503943,1.151862,1.1532748,1.1546444,1.1559801,1.1572893,1.1585774,1.1598493,1.1611083,1.1623572,1.1635982,1.164833,1.1660628,1.1672889,1.168512,1.1697327,1.1709516],[1.1879271,1.1867082,1.1854875,1.1842644,1.1830384,1.1818085,1.1805737,1.1793327,1.1780838,1.1768248,1.175553,1.1742648,1.1729556,1.1716199,1.1702503,1.1688375,1.1673698,1.165832,1.1642051,1.1624646,1.1605796,1.1585106,1.1562074,1.1536069,1.1506282,1.1471695,1.1431018,1.1382626,1.1324476,1.125401,1.1168041,1.1062622,1.0932906,1.0773009,1.0575887,1.0333278,1.0035748,0.9672929,0.92340827,0.8709091,0.8090087,0.7373768,0.65644467,0.5677497,0.47425425,0.38052025,0.292593,0.21746692,0.16211255,0.13222186,0.13222186,0.16211255,0.21746692,0.292593,0.38052025,0.47425425,0.5677497,0.65644467,0.7373768,0.8090087,0.8709091,0.92340827,0.9672929,1.0035748,1.0333278,1.0575887,1.0773009,1.0932906,1.1062622,1.1168041,1.125401,1.1324476,1.1382626,1.1431018,1.1471695,1.1506282,1.1536069,1.1562074,1.1585106,1.1605796,1.1624646,1.1642051,1.165832,1.1673698,1.1688375,1.1702503,1.1716199,1.1729556,1.1742648,1.175553,1.1768248,1.1780838,1.1793327,1.1805737,1.1818085,1.1830384,1.1842644,1.1854875,1.1867082,1.1879271],[1.2070696,1.2058507,1.20463,1.2034069,1.2021809,1.200951,1.1997162,1.1984752,1.1972263,1.1959673,1.1946955,1.1934073,1.1920981,1.1907624,1.1893928,1.18798,1.1865124,1.1849746,1.1833476,1.1816071,1.1797221,1.1776531,1.17535,1.1727494,1.1697707,1.166312,1.1622443,1.1574051,1.1515901,1.1445435,1.1359466,1.1254047,1.1124331,1.0964434,1.0767312,1.0524703,1.0227174,0.9864354,0.9425508,0.8900516,0.8281512,0.7565193,0.6755872,0.5868922,0.49339676,0.39966276,0.3117355,0.23660943,0.18125506,0.15136437,0.15136437,0.18125506,0.23660943,0.3117355,0.39966276,0.49339676,0.5868922,0.6755872,0.7565193,0.8281512,0.8900516,0.9425508,0.9864354,1.0227174,1.0524703,1.0767312,1.0964434,1.1124331,1.1254047,1.1359466,1.1445435,1.1515901,1.1574051,1.1622443,1.166312,1.1697707,1.1727494,1.17535,1.1776531,1.1797221,1.1816071,1.1833476,1.1849746,1.1865124,1.18798,1.1893928,1.1907624,1.1920981,1.1934073,1.1946955,1.1959673,1.1972263,1.1984752,1.1997162,1.200951,1.2021809,1.2034069,1.20463,1.2058507,1.2070696],[1.2285224,1.2273035,1.2260828,1.2248597,1.2236336,1.2224038,1.221169,1.219928,1.2186791,1.2174201,1.2161483,1.2148601,1.2135509,1.2122152,1.2108456,1.2094328,1.2079651,1.2064273,1.2048004,1.2030599,1.2011749,1.1991059,1.1968027,1.1942022,1.1912235,1.1877648,1.1836971,1.1788579,1.1730429,1.1659964,1.1573995,1.1468576,1.133886,1.1178962,1.098184,1.0739232,1.0441701,1.0078883,0.9640036,0.9115045,0.84960407,0.7779722,0.6970401,0.60834503,0.5148496,0.42111564,0.33318835,0.25806227,0.20270792,0.17281723,0.17281723,0.20270792,0.25806227,0.33318835,0.42111564,0.5148496,0.60834503,0.6970401,0.7779722,0.84960407,0.9115045,0.9640036,1.0078883,1.0441701,1.0739232,1.098184,1.1178962,1.133886,1.1468576,1.1573995,1.1659964,1.1730429,1.1788579,1.1836971,1.1877648,1.1912235,1.1942022,1.1968027,1.1991059,1.2011749,1.2030599,1.2048004,1.2064273,1.2079651,1.2094328,1.2108456,1.2122152,1.2135509,1.2148601,1.2161483,1.2174201,1.2186791,1.219928,1.221169,1.2224038,1.2236336,1.2248597,1.2260828,1.2273035,1.2285224],[1.2523983,1.2511793,1.2499586,1.2487355,1.2475095,1.2462796,1.2450448,1.2438039,1.2425549,1.2412959,1.2400241,1.2387359,1.2374268,1.236091,1.2347214,1.2333087,1.231841,1.2303032,1.2286762,1.2269357,1.2250507,1.2229817,1.2206786,1.218078,1.2150993,1.2116406,1.2075729,1.2027338,1.1969187,1.1898721,1.1812752,1.1707333,1.1577617,1.141772,1.1220598,1.097799,1.068046,1.031764,0.9878794,0.9353802,0.87347984,0.80184793,0.7209158,0.6322208,0.5387254,0.44499138,0.35706413,0.28193805,0.22658367,0.19669299,0.19669299,0.22658367,0.28193805,0.35706413,0.44499138,0.5387254,0.6322208,0.7209158,0.80184793,0.87347984,0.9353802,0.9878794,1.031764,1.068046,1.097799,1.1220598,1.141772,1.1577617,1.1707333,1.1812752,1.1898721,1.1969187,1.2027338,1.2075729,1.2116406,1.2150993,1.218078,1.2206786,1.2229817,1.2250507,1.2269357,1.2286762,1.2303032,1.231841,1.2333087,1.2347214,1.236091,1.2374268,1.2387359,1.2400241,1.2412959,1.2425549,1.2438039,1.2450448,1.2462796,1.2475095,1.2487355,1.2499586,1.2511793,1.2523983],[1.2787662,1.2775472,1.2763265,1.2751034,1.2738774,1.2726475,1.2714127,1.2701718,1.2689228,1.2676638,1.266392,1.2651038,1.2637947,1.2624589,1.2610893,1.2596766,1.2582089,1.2566711,1.2550441,1.2533036,1.2514186,1.2493496,1.2470465,1.2444459,1.2414672,1.2380085,1.2339408,1.2291017,1.2232866,1.21624,1.2076432,1.1971012,1.1841296,1.1681399,1.1484277,1.1241668,1.0944139,1.0581319,1.0142473,0.9617482,0.89984775,0.8282159,0.74728376,0.6585887,0.5650933,0.4713593,0.38343203,0.30830595,0.2529516,0.2230609,0.2230609,0.2529516,0.30830595,0.38343203,0.4713593,0.5650933,0.6585887,0.74728376,0.8282159,0.89984775,0.9617482,1.0142473,1.0581319,1.0944139,1.1241668,1.1484277,1.1681399,1.1841296,1.1971012,1.2076432,1.21624,1.2232866,1.2291017,1.2339408,1.2380085,1.2414672,1.2444459,1.2470465,1.2493496,1.2514186,1.2533036,1.2550441,1.2566711,1.2582089,1.2596766,1.2610893,1.2624589,1.2637947,1.2651038,1.266392,1.2676638,1.2689228,1.2701718,1.2714127,1.2726475,1.2738774,1.2751034,1.2763265,1.2775472,1.2787662],[1.3076391,1.3064202,1.3051995,1.3039764,1.3027503,1.3015205,1.3002857,1.2990447,1.2977958,1.2965368,1.295265,1.2939768,1.2926676,1.2913319,1.2899623,1.2885495,1.2870818,1.285544,1.2839171,1.2821766,1.2802916,1.2782226,1.2759194,1.2733189,1.2703402,1.2668815,1.2628138,1.2579746,1.2521596,1.2451131,1.2365162,1.2259743,1.2130027,1.1970129,1.1773007,1.1530399,1.1232868,1.087005,1.0431203,0.9906212,0.9287208,0.8570889,0.7761568,0.68746173,0.5939663,0.50023234,0.41230506,0.33717898,0.28182462,0.25193393,0.25193393,0.28182462,0.33717898,0.41230506,0.50023234,0.5939663,0.68746173,0.7761568,0.8570889,0.9287208,0.9906212,1.0431203,1.087005,1.1232868,1.1530399,1.1773007,1.1970129,1.2130027,1.2259743,1.2365162,1.2451131,1.2521596,1.2579746,1.2628138,1.2668815,1.2703402,1.2733189,1.2759194,1.2782226,1.2802916,1.2821766,1.2839171,1.285544,1.2870818,1.2885495,1.2899623,1.2913319,1.2926676,1.2939768,1.295265,1.2965368,1.2977958,1.2990447,1.3002857,1.3015205,1.3027503,1.3039764,1.3051995,1.3064202,1.3076391],[1.3389618,1.3377429,1.3365222,1.3352991,1.3340731,1.3328432,1.3316084,1.3303674,1.3291185,1.3278595,1.3265877,1.3252995,1.3239903,1.3226546,1.321285,1.3198723,1.3184046,1.3168668,1.3152398,1.3134993,1.3116143,1.3095453,1.3072422,1.3046416,1.3016629,1.2982042,1.2941365,1.2892973,1.2834823,1.2764357,1.2678388,1.2572969,1.2443253,1.2283356,1.2086234,1.1843625,1.1546096,1.1183276,1.074443,1.0219438,0.96004343,0.8884115,0.8074794,0.7187844,0.62528896,0.53155494,0.44362772,0.36850163,0.31314728,0.2832566,0.2832566,0.31314728,0.36850163,0.44362772,0.53155494,0.62528896,0.7187844,0.8074794,0.8884115,0.96004343,1.0219438,1.074443,1.1183276,1.1546096,1.1843625,1.2086234,1.2283356,1.2443253,1.2572969,1.2678388,1.2764357,1.2834823,1.2892973,1.2941365,1.2982042,1.3016629,1.3046416,1.3072422,1.3095453,1.3116143,1.3134993,1.3152398,1.3168668,1.3184046,1.3198723,1.321285,1.3226546,1.3239903,1.3252995,1.3265877,1.3278595,1.3291185,1.3303674,1.3316084,1.3328432,1.3340731,1.3352991,1.3365222,1.3377429,1.3389618],[1.3725997,1.3713808,1.3701601,1.368937,1.367711,1.3664811,1.3652463,1.3640053,1.3627564,1.3614974,1.3602256,1.3589374,1.3576282,1.3562925,1.3549229,1.3535101,1.3520424,1.3505046,1.3488777,1.3471372,1.3452522,1.3431832,1.34088,1.3382795,1.3353008,1.3318421,1.3277744,1.3229352,1.3171202,1.3100736,1.3014767,1.2909348,1.2779632,1.2619735,1.2422613,1.2180004,1.1882474,1.1519656,1.1080809,1.0555818,0.9936813,0.92204946,0.8411173,0.75242233,0.65892684,0.5651929,0.47726563,0.40213954,0.3467852,0.3168945,0.3168945,0.3467852,0.40213954,0.47726563,0.5651929,0.65892684,0.75242233,0.8411173,0.92204946,0.9936813,1.0555818,1.1080809,1.1519656,1.1882474,1.2180004,1.2422613,1.2619735,1.2779632,1.2909348,1.3014767,1.3100736,1.3171202,1.3229352,1.3277744,1.3318421,1.3353008,1.3382795,1.34088,1.3431832,1.3452522,1.3471372,1.3488777,1.3505046,1.3520424,1.3535101,1.3549229,1.3562925,1.3576282,1.3589374,1.3602256,1.3614974,1.3627564,1.3640053,1.3652463,1.3664811,1.367711,1.368937,1.3701601,1.3713808,1.3725997],[1.4083338,1.407115,1.4058943,1.4046712,1.403445,1.4022152,1.4009805,1.3997395,1.3984904,1.3972316,1.3959596,1.3946714,1.3933623,1.3920267,1.390657,1.3892443,1.3877766,1.3862388,1.3846118,1.3828714,1.3809862,1.3789172,1.3766141,1.3740137,1.3710349,1.3675761,1.3635085,1.3586693,1.3528543,1.3458078,1.3372109,1.326669,1.3136973,1.2977076,1.2779953,1.2537346,1.2239816,1.1876997,1.143815,1.0913159,1.0294154,0.9577836,0.87685144,0.7881564,0.694661,0.600927,0.5129998,0.43787366,0.3825193,0.35262862,0.35262862,0.3825193,0.43787366,0.5129998,0.600927,0.694661,0.7881564,0.87685144,0.9577836,1.0294154,1.0913159,1.143815,1.1876997,1.2239816,1.2537346,1.2779953,1.2977076,1.3136973,1.326669,1.3372109,1.3458078,1.3528543,1.3586693,1.3635085,1.3675761,1.3710349,1.3740137,1.3766141,1.3789172,1.3809862,1.3828714,1.3846118,1.3862388,1.3877766,1.3892443,1.390657,1.3920267,1.3933623,1.3946714,1.3959596,1.3972316,1.3984904,1.3997395,1.4009805,1.4022152,1.403445,1.4046712,1.4058943,1.407115,1.4083338],[1.4458596,1.4446406,1.4434199,1.4421968,1.4409708,1.4397409,1.4385061,1.4372652,1.4360162,1.4347572,1.4334854,1.4321972,1.430888,1.4295523,1.4281827,1.42677,1.4253023,1.4237645,1.4221375,1.420397,1.418512,1.416443,1.4141399,1.4115393,1.4085606,1.4051019,1.4010342,1.396195,1.39038,1.3833334,1.3747365,1.3641946,1.351223,1.3352333,1.3155211,1.2912602,1.2615073,1.2252253,1.1813407,1.1288415,1.0669411,0.99530923,0.9143771,0.8256821,0.7321867,0.63845265,0.5505254,0.47539935,0.420045,0.3901543,0.3901543,0.420045,0.47539935,0.5505254,0.63845265,0.7321867,0.8256821,0.9143771,0.99530923,1.0669411,1.1288415,1.1813407,1.2252253,1.2615073,1.2912602,1.3155211,1.3352333,1.351223,1.3641946,1.3747365,1.3833334,1.39038,1.396195,1.4010342,1.4051019,1.4085606,1.4115393,1.4141399,1.416443,1.418512,1.420397,1.4221375,1.4237645,1.4253023,1.42677,1.4281827,1.4295523,1.430888,1.4321972,1.4334854,1.4347572,1.4360162,1.4372652,1.4385061,1.4397409,1.4409708,1.4421968,1.4434199,1.4446406,1.4458596],[1.4847922,1.4835733,1.4823526,1.4811295,1.4799035,1.4786736,1.4774388,1.4761978,1.4749489,1.4736899,1.4724181,1.4711299,1.4698207,1.468485,1.4671154,1.4657027,1.464235,1.4626971,1.4610702,1.4593297,1.4574447,1.4553757,1.4530725,1.450472,1.4474933,1.4440346,1.4399669,1.4351277,1.4293127,1.4222662,1.4136693,1.4031274,1.3901558,1.374166,1.3544538,1.330193,1.30044,1.2641581,1.2202734,1.1677743,1.1058738,1.034242,0.9533099,0.86461484,0.7711194,0.67738545,0.58945817,0.51433206,0.45897773,0.42908704,0.42908704,0.45897773,0.51433206,0.58945817,0.67738545,0.7711194,0.86461484,0.9533099,1.034242,1.1058738,1.1677743,1.2202734,1.2641581,1.30044,1.330193,1.3544538,1.374166,1.3901558,1.4031274,1.4136693,1.4222662,1.4293127,1.4351277,1.4399669,1.4440346,1.4474933,1.450472,1.4530725,1.4553757,1.4574447,1.4593297,1.4610702,1.4626971,1.464235,1.4657027,1.4671154,1.468485,1.4698207,1.4711299,1.4724181,1.4736899,1.4749489,1.4761978,1.4774388,1.4786736,1.4799035,1.4811295,1.4823526,1.4835733,1.4847922],[1.5246806,1.5234617,1.522241,1.5210179,1.5197918,1.518562,1.5173272,1.5160862,1.5148373,1.5135783,1.5123065,1.5110183,1.5097091,1.5083734,1.5070038,1.505591,1.5041233,1.5025855,1.5009586,1.4992181,1.497333,1.495264,1.4929609,1.4903604,1.4873817,1.483923,1.4798553,1.4750161,1.4692011,1.4621546,1.4535577,1.4430158,1.4300442,1.4140544,1.3943422,1.3700814,1.3403283,1.3040464,1.2601618,1.2076626,1.1457622,1.0741303,0.9931982,0.9045032,0.81100774,0.7172738,0.6293465,0.55422044,0.49886608,0.4689754,0.4689754,0.49886608,0.55422044,0.6293465,0.7172738,0.81100774,0.9045032,0.9931982,1.0741303,1.1457622,1.2076626,1.2601618,1.3040464,1.3403283,1.3700814,1.3943422,1.4140544,1.4300442,1.4430158,1.4535577,1.4621546,1.4692011,1.4750161,1.4798553,1.483923,1.4873817,1.4903604,1.4929609,1.495264,1.497333,1.4992181,1.5009586,1.5025855,1.5041233,1.505591,1.5070038,1.5083734,1.5097091,1.5110183,1.5123065,1.5135783,1.5148373,1.5160862,1.5173272,1.518562,1.5197918,1.5210179,1.522241,1.5234617,1.5246806],[1.5650257,1.5638068,1.5625861,1.561363,1.5601369,1.558907,1.5576723,1.5564313,1.5551823,1.5539234,1.5526515,1.5513633,1.5500542,1.5487185,1.5473489,1.5459361,1.5444684,1.5429306,1.5413036,1.5395632,1.5376781,1.5356091,1.533306,1.5307055,1.5277268,1.524268,1.5202004,1.5153612,1.5095462,1.5024996,1.4939027,1.4833608,1.4703891,1.4543995,1.4346873,1.4104264,1.3806734,1.3443916,1.3005068,1.2480078,1.1861073,1.1144755,1.0335433,0.9448483,0.8513528,0.75761884,0.66969156,0.5945655,0.53921115,0.50932044,0.50932044,0.53921115,0.5945655,0.66969156,0.75761884,0.8513528,0.9448483,1.0335433,1.1144755,1.1861073,1.2480078,1.3005068,1.3443916,1.3806734,1.4104264,1.4346873,1.4543995,1.4703891,1.4833608,1.4939027,1.5024996,1.5095462,1.5153612,1.5202004,1.524268,1.5277268,1.5307055,1.533306,1.5356091,1.5376781,1.5395632,1.5413036,1.5429306,1.5444684,1.5459361,1.5473489,1.5487185,1.5500542,1.5513633,1.5526515,1.5539234,1.5551823,1.5564313,1.5576723,1.558907,1.5601369,1.561363,1.5625861,1.5638068,1.5650257],[1.6053052,1.6040862,1.6028655,1.6016424,1.6004164,1.5991864,1.5979517,1.5967107,1.5954618,1.5942028,1.592931,1.5916429,1.5903337,1.5889978,1.5876284,1.5862155,1.5847478,1.58321,1.581583,1.5798426,1.5779576,1.5758886,1.5735855,1.5709848,1.5680063,1.5645475,1.5604799,1.5556407,1.5498257,1.5427791,1.5341822,1.5236403,1.5106686,1.494679,1.4749668,1.4507059,1.4209528,1.384671,1.3407862,1.2882872,1.2263868,1.1547549,1.0738227,0.9851277,0.8916323,0.7978983,0.7099711,0.63484496,0.5794906,0.5495999,0.5495999,0.5794906,0.63484496,0.7099711,0.7978983,0.8916323,0.9851277,1.0738227,1.1547549,1.2263868,1.2882872,1.3407862,1.384671,1.4209528,1.4507059,1.4749668,1.494679,1.5106686,1.5236403,1.5341822,1.5427791,1.5498257,1.5556407,1.5604799,1.5645475,1.5680063,1.5709848,1.5735855,1.5758886,1.5779576,1.5798426,1.581583,1.58321,1.5847478,1.5862155,1.5876284,1.5889978,1.5903337,1.5916429,1.592931,1.5942028,1.5954618,1.5967107,1.5979517,1.5991864,1.6004164,1.6016424,1.6028655,1.6040862,1.6053052],[1.645,1.643781,1.6425602,1.6413372,1.6401112,1.6388812,1.6376464,1.6364055,1.6351566,1.6338975,1.6326258,1.6313376,1.6300285,1.6286926,1.6273232,1.6259103,1.6244426,1.6229048,1.6212778,1.6195374,1.6176524,1.6155834,1.6132803,1.6106796,1.6077011,1.6042423,1.6001747,1.5953355,1.5895205,1.5824739,1.573877,1.5633351,1.5503634,1.5343738,1.5146616,1.4904007,1.4606476,1.4243658,1.380481,1.327982,1.2660816,1.1944497,1.1135175,1.0248225,0.9313271,0.8375931,0.74966586,0.67453974,0.6191854,0.5892947,0.5892947,0.6191854,0.67453974,0.74966586,0.8375931,0.9313271,1.0248225,1.1135175,1.1944497,1.2660816,1.327982,1.380481,1.4243658,1.4606476,1.4904007,1.5146616,1.5343738,1.5503634,1.5633351,1.573877,1.5824739,1.5895205,1.5953355,1.6001747,1.6042423,1.6077011,1.6106796,1.6132803,1.6155834,1.6176524,1.6195374,1.6212778,1.6229048,1.6244426,1.6259103,1.6273232,1.6286926,1.6300285,1.6313376,1.6326258,1.6338975,1.6351566,1.6364055,1.6376464,1.6388812,1.6401112,1.6413372,1.6425602,1.643781,1.645],[1.6836209,1.6824019,1.6811812,1.6799581,1.6787322,1.6775022,1.6762674,1.6750264,1.6737776,1.6725185,1.6712468,1.6699586,1.6686494,1.6673136,1.6659441,1.6645312,1.6630635,1.6615257,1.6598988,1.6581583,1.6562734,1.6542044,1.6519012,1.6493006,1.646322,1.6428633,1.6387956,1.6339564,1.6281414,1.6210948,1.6124979,1.601956,1.5889844,1.5729947,1.5532825,1.5290216,1.4992685,1.4629867,1.419102,1.3666029,1.3047025,1.2330706,1.1521385,1.0634434,0.96994805,0.876214,0.7882868,0.7131607,0.65780634,0.6279156,0.6279156,0.65780634,0.7131607,0.7882868,0.876214,0.96994805,1.0634434,1.1521385,1.2330706,1.3047025,1.3666029,1.419102,1.4629867,1.4992685,1.5290216,1.5532825,1.5729947,1.5889844,1.601956,1.6124979,1.6210948,1.6281414,1.6339564,1.6387956,1.6428633,1.646322,1.6493006,1.6519012,1.6542044,1.6562734,1.6581583,1.6598988,1.6615257,1.6630635,1.6645312,1.6659441,1.6673136,1.6686494,1.6699586,1.6712468,1.6725185,1.6737776,1.6750264,1.6762674,1.6775022,1.6787322,1.6799581,1.6811812,1.6824019,1.6836209],[1.7207317,1.7195128,1.7182921,1.717069,1.715843,1.7146131,1.7133783,1.7121373,1.7108884,1.7096294,1.7083576,1.7070694,1.7057602,1.7044245,1.7030549,1.7016422,1.7001745,1.6986367,1.6970097,1.6952692,1.6933842,1.6913152,1.689012,1.6864115,1.6834328,1.6799741,1.6759064,1.6710672,1.6652522,1.6582057,1.6496089,1.6390669,1.6260953,1.6101055,1.5903933,1.5661325,1.5363795,1.5000975,1.4562129,1.4037137,1.3418133,1.2701814,1.1892493,1.1005543,1.0070589,0.9133249,0.8253976,0.75027156,0.6949172,0.6650265,0.6650265,0.6949172,0.75027156,0.8253976,0.9133249,1.0070589,1.1005543,1.1892493,1.2701814,1.3418133,1.4037137,1.4562129,1.5000975,1.5363795,1.5661325,1.5903933,1.6101055,1.6260953,1.6390669,1.6496089,1.6582057,1.6652522,1.6710672,1.6759064,1.6799741,1.6834328,1.6864115,1.689012,1.6913152,1.6933842,1.6952692,1.6970097,1.6986367,1.7001745,1.7016422,1.7030549,1.7044245,1.7057602,1.7070694,1.7083576,1.7096294,1.7108884,1.7121373,1.7133783,1.7146131,1.715843,1.717069,1.7182921,1.7195128,1.7207317],[1.7559671,1.7547482,1.7535275,1.7523044,1.7510784,1.7498485,1.7486137,1.7473727,1.7461238,1.7448648,1.743593,1.7423048,1.7409956,1.7396599,1.7382903,1.7368776,1.7354099,1.733872,1.7322451,1.7305046,1.7286196,1.7265506,1.7242475,1.7216469,1.7186682,1.7152095,1.7111418,1.7063026,1.7004876,1.6934412,1.6848443,1.6743023,1.6613307,1.6453409,1.6256287,1.601368,1.5716149,1.5353329,1.4914483,1.4389491,1.3770487,1.3054168,1.2244847,1.1357898,1.0422943,0.9485603,0.860633,0.78550696,0.7301526,0.7002619,0.7002619,0.7301526,0.78550696,0.860633,0.9485603,1.0422943,1.1357898,1.2244847,1.3054168,1.3770487,1.4389491,1.4914483,1.5353329,1.5716149,1.601368,1.6256287,1.6453409,1.6613307,1.6743023,1.6848443,1.6934412,1.7004876,1.7063026,1.7111418,1.7152095,1.7186682,1.7216469,1.7242475,1.7265506,1.7286196,1.7305046,1.7322451,1.733872,1.7354099,1.7368776,1.7382903,1.7396599,1.7409956,1.7423048,1.743593,1.7448648,1.7461238,1.7473727,1.7486137,1.7498485,1.7510784,1.7523044,1.7535275,1.7547482,1.7559671],[1.7890434,1.7878244,1.7866037,1.7853806,1.7841547,1.7829247,1.7816899,1.7804489,1.7792001,1.777941,1.7766693,1.7753811,1.7740719,1.7727361,1.7713666,1.7699537,1.768486,1.7669482,1.7653213,1.7635808,1.7616959,1.7596269,1.7573237,1.7547231,1.7517445,1.7482858,1.7442181,1.7393789,1.7335639,1.7265173,1.7179204,1.7073785,1.6944069,1.6784172,1.658705,1.6344441,1.604691,1.5684092,1.5245245,1.4720254,1.410125,1.3384931,1.257561,1.1688659,1.0753706,0.9816365,0.8937093,0.8185832,0.76322883,0.7333381,0.7333381,0.76322883,0.8185832,0.8937093,0.9816365,1.0753706,1.1688659,1.257561,1.3384931,1.410125,1.4720254,1.5245245,1.5684092,1.604691,1.6344441,1.658705,1.6784172,1.6944069,1.7073785,1.7179204,1.7265173,1.7335639,1.7393789,1.7442181,1.7482858,1.7517445,1.7547231,1.7573237,1.7596269,1.7616959,1.7635808,1.7653213,1.7669482,1.768486,1.7699537,1.7713666,1.7727361,1.7740719,1.7753811,1.7766693,1.777941,1.7792001,1.7804489,1.7816899,1.7829247,1.7841547,1.7853806,1.7866037,1.7878244,1.7890434],[1.819763,1.818544,1.8173233,1.8161002,1.8148742,1.8136443,1.8124095,1.8111686,1.8099196,1.8086606,1.8073888,1.8061006,1.8047915,1.8034557,1.8020861,1.8006734,1.7992057,1.7976679,1.7960409,1.7943004,1.7924154,1.7903464,1.7880433,1.7854427,1.782464,1.7790053,1.7749376,1.7700984,1.7642834,1.757237,1.7486401,1.7380981,1.7251265,1.7091367,1.6894245,1.6651638,1.6354107,1.5991287,1.5552441,1.5027449,1.4408445,1.3692126,1.2882805,1.1995856,1.1060901,1.012356,0.9244288,0.84930277,0.7939484,0.7640577,0.7640577,0.7939484,0.84930277,0.9244288,1.012356,1.1060901,1.1995856,1.2882805,1.3692126,1.4408445,1.5027449,1.5552441,1.5991287,1.6354107,1.6651638,1.6894245,1.7091367,1.7251265,1.7380981,1.7486401,1.757237,1.7642834,1.7700984,1.7749376,1.7790053,1.782464,1.7854427,1.7880433,1.7903464,1.7924154,1.7943004,1.7960409,1.7976679,1.7992057,1.8006734,1.8020861,1.8034557,1.8047915,1.8061006,1.8073888,1.8086606,1.8099196,1.8111686,1.8124095,1.8136443,1.8148742,1.8161002,1.8173233,1.818544,1.819763],[1.8480121,1.8467932,1.8455725,1.8443494,1.8431233,1.8418934,1.8406587,1.8394177,1.8381687,1.8369098,1.8356379,1.8343498,1.8330406,1.8317049,1.8303353,1.8289225,1.8274548,1.825917,1.82429,1.8225496,1.8206645,1.8185955,1.8162924,1.8136919,1.8107132,1.8072544,1.8031868,1.7983476,1.7925326,1.785486,1.7768891,1.7663472,1.7533755,1.7373859,1.7176737,1.6934128,1.6636598,1.627378,1.5834932,1.5309942,1.4690937,1.3974619,1.3165298,1.2278347,1.1343392,1.0406053,0.95267797,0.8775519,0.82219756,0.79230684,0.79230684,0.82219756,0.8775519,0.95267797,1.0406053,1.1343392,1.2278347,1.3165298,1.3974619,1.4690937,1.5309942,1.5834932,1.627378,1.6636598,1.6934128,1.7176737,1.7373859,1.7533755,1.7663472,1.7768891,1.785486,1.7925326,1.7983476,1.8031868,1.8072544,1.8107132,1.8136919,1.8162924,1.8185955,1.8206645,1.8225496,1.82429,1.825917,1.8274548,1.8289225,1.8303353,1.8317049,1.8330406,1.8343498,1.8356379,1.8369098,1.8381687,1.8394177,1.8406587,1.8418934,1.8431233,1.8443494,1.8455725,1.8467932,1.8480121],[1.8737533,1.8725343,1.8713136,1.8700905,1.8688645,1.8676345,1.8663998,1.8651588,1.86391,1.8626509,1.8613791,1.860091,1.8587818,1.857446,1.8560765,1.8546636,1.8531959,1.8516581,1.8500311,1.8482907,1.8464057,1.8443367,1.8420336,1.839433,1.8364544,1.8329957,1.828928,1.8240888,1.8182738,1.8112272,1.8026303,1.7920884,1.7791167,1.7631271,1.7434149,1.719154,1.6894009,1.6531191,1.6092343,1.5567353,1.4948349,1.423203,1.3422709,1.2535758,1.1600804,1.0663464,0.9784192,0.9032931,0.8479387,0.818048,0.818048,0.8479387,0.9032931,0.9784192,1.0663464,1.1600804,1.2535758,1.3422709,1.423203,1.4948349,1.5567353,1.6092343,1.6531191,1.6894009,1.719154,1.7434149,1.7631271,1.7791167,1.7920884,1.8026303,1.8112272,1.8182738,1.8240888,1.828928,1.8329957,1.8364544,1.839433,1.8420336,1.8443367,1.8464057,1.8482907,1.8500311,1.8516581,1.8531959,1.8546636,1.8560765,1.857446,1.8587818,1.860091,1.8613791,1.8626509,1.86391,1.8651588,1.8663998,1.8676345,1.8688645,1.8700905,1.8713136,1.8725343,1.8737533],[1.897015,1.8957961,1.8945754,1.8933523,1.8921262,1.8908963,1.8896616,1.8884206,1.8871716,1.8859127,1.8846408,1.8833526,1.8820435,1.8807077,1.8793381,1.8779254,1.8764577,1.8749199,1.8732929,1.8715525,1.8696674,1.8675984,1.8652953,1.8626947,1.859716,1.8562573,1.8521897,1.8473505,1.8415354,1.8344889,1.825892,1.81535,1.8023784,1.7863888,1.7666765,1.7424157,1.7126627,1.6763809,1.6324961,1.5799971,1.5180966,1.4464648,1.3655326,1.2768376,1.1833421,1.0896082,1.0016809,0.9265548,0.87120044,0.8413097,0.8413097,0.87120044,0.9265548,1.0016809,1.0896082,1.1833421,1.2768376,1.3655326,1.4464648,1.5180966,1.5799971,1.6324961,1.6763809,1.7126627,1.7424157,1.7666765,1.7863888,1.8023784,1.81535,1.825892,1.8344889,1.8415354,1.8473505,1.8521897,1.8562573,1.859716,1.8626947,1.8652953,1.8675984,1.8696674,1.8715525,1.8732929,1.8749199,1.8764577,1.8779254,1.8793381,1.8807077,1.8820435,1.8833526,1.8846408,1.8859127,1.8871716,1.8884206,1.8896616,1.8908963,1.8921262,1.8933523,1.8945754,1.8957961,1.897015],[1.9178782,1.9166594,1.9154387,1.9142156,1.9129894,1.9117596,1.9105248,1.9092839,1.9080348,1.906776,1.905504,1.9042158,1.9029067,1.901571,1.9002013,1.8987887,1.897321,1.8957832,1.8941562,1.8924158,1.8905306,1.8884616,1.8861585,1.883558,1.8805792,1.8771205,1.8730528,1.8682137,1.8623986,1.8553522,1.8467553,1.8362134,1.8232417,1.8072519,1.7875397,1.763279,1.733526,1.697244,1.6533594,1.6008602,1.5389597,1.467328,1.3863958,1.2977008,1.2042054,1.1104714,1.0225441,0.94741803,0.8920637,0.86217296,0.86217296,0.8920637,0.94741803,1.0225441,1.1104714,1.2042054,1.2977008,1.3863958,1.467328,1.5389597,1.6008602,1.6533594,1.697244,1.733526,1.763279,1.7875397,1.8072519,1.8232417,1.8362134,1.8467553,1.8553522,1.8623986,1.8682137,1.8730528,1.8771205,1.8805792,1.883558,1.8861585,1.8884616,1.8905306,1.8924158,1.8941562,1.8957832,1.897321,1.8987887,1.9002013,1.901571,1.9029067,1.9042158,1.905504,1.906776,1.9080348,1.9092839,1.9105248,1.9117596,1.9129894,1.9142156,1.9154387,1.9166594,1.9178782],[1.9364645,1.9352456,1.9340249,1.9328018,1.9315758,1.9303459,1.9291111,1.9278702,1.9266212,1.9253622,1.9240904,1.9228022,1.921493,1.9201573,1.9187877,1.917375,1.9159073,1.9143695,1.9127425,1.911002,1.909117,1.907048,1.9047449,1.9021443,1.8991656,1.8957069,1.8916392,1.8868,1.880985,1.8739386,1.8653417,1.8547997,1.8418281,1.8258383,1.8061261,1.7818654,1.7521123,1.7158303,1.6719457,1.6194465,1.5575461,1.4859142,1.4049821,1.3162872,1.2227917,1.1290576,1.0411304,0.9660044,0.91065,0.8807593,0.8807593,0.91065,0.9660044,1.0411304,1.1290576,1.2227917,1.3162872,1.4049821,1.4859142,1.5575461,1.6194465,1.6719457,1.7158303,1.7521123,1.7818654,1.8061261,1.8258383,1.8418281,1.8547997,1.8653417,1.8739386,1.880985,1.8868,1.8916392,1.8957069,1.8991656,1.9021443,1.9047449,1.907048,1.909117,1.911002,1.9127425,1.9143695,1.9159073,1.917375,1.9187877,1.9201573,1.921493,1.9228022,1.9240904,1.9253622,1.9266212,1.9278702,1.9291111,1.9303459,1.9315758,1.9328018,1.9340249,1.9352456,1.9364645],[1.9529232,1.9517043,1.9504836,1.9492605,1.9480344,1.9468045,1.9455698,1.9443288,1.9430798,1.9418209,1.940549,1.9392608,1.9379517,1.936616,1.9352463,1.9338336,1.9323659,1.9308281,1.9292011,1.9274607,1.9255756,1.9235066,1.9212035,1.918603,1.9156243,1.9121655,1.9080979,1.9032587,1.8974437,1.8903971,1.8818002,1.8712583,1.8582866,1.842297,1.8225847,1.7983239,1.7685709,1.7322891,1.6884043,1.6359053,1.5740048,1.502373,1.4214408,1.3327458,1.2392503,1.1455164,1.057589,0.982463,0.92710865,0.8972179,0.8972179,0.92710865,0.982463,1.057589,1.1455164,1.2392503,1.3327458,1.4214408,1.502373,1.5740048,1.6359053,1.6884043,1.7322891,1.7685709,1.7983239,1.8225847,1.842297,1.8582866,1.8712583,1.8818002,1.8903971,1.8974437,1.9032587,1.9080979,1.9121655,1.9156243,1.918603,1.9212035,1.9235066,1.9255756,1.9274607,1.9292011,1.9308281,1.9323659,1.9338336,1.9352463,1.936616,1.9379517,1.9392608,1.940549,1.9418209,1.9430798,1.9443288,1.9455698,1.9468045,1.9480344,1.9492605,1.9504836,1.9517043,1.9529232],[1.9674202,1.9662013,1.9649806,1.9637575,1.9625314,1.9613016,1.9600668,1.9588258,1.9575769,1.9563179,1.955046,1.9537579,1.9524487,1.951113,1.9497434,1.9483306,1.9468629,1.9453251,1.9436982,1.9419577,1.9400727,1.9380037,1.9357005,1.9331,1.9301213,1.9266626,1.9225949,1.9177557,1.9119407,1.9048941,1.8962972,1.8857553,1.8727837,1.856794,1.8370818,1.8128209,1.783068,1.7467861,1.7029014,1.6504023,1.5885018,1.51687,1.4359379,1.3472428,1.2537473,1.1600134,1.0720861,0.99696004,0.9416057,0.911715,0.911715,0.9416057,0.99696004,1.0720861,1.1600134,1.2537473,1.3472428,1.4359379,1.51687,1.5885018,1.6504023,1.7029014,1.7467861,1.783068,1.8128209,1.8370818,1.856794,1.8727837,1.8857553,1.8962972,1.9048941,1.9119407,1.9177557,1.9225949,1.9266626,1.9301213,1.9331,1.9357005,1.9380037,1.9400727,1.9419577,1.9436982,1.9453251,1.9468629,1.9483306,1.9497434,1.951113,1.9524487,1.9537579,1.955046,1.9563179,1.9575769,1.9588258,1.9600668,1.9613016,1.9625314,1.9637575,1.9649806,1.9662013,1.9674202],[1.9801296,1.9789107,1.97769,1.9764669,1.9752408,1.974011,1.9727762,1.9715352,1.9702863,1.9690273,1.9677554,1.9664673,1.9651581,1.9638224,1.9624528,1.96104,1.9595723,1.9580345,1.9564075,1.9546671,1.952782,1.950713,1.9484099,1.9458094,1.9428307,1.939372,1.9353043,1.9304651,1.9246501,1.9176035,1.9090066,1.8984647,1.885493,1.8695034,1.8497912,1.8255303,1.7957773,1.7594955,1.7156107,1.6631117,1.6012112,1.5295794,1.4486473,1.3599522,1.2664567,1.1727228,1.0847955,1.0096694,0.95431507,0.92442435,0.92442435,0.95431507,1.0096694,1.0847955,1.1727228,1.2664567,1.3599522,1.4486473,1.5295794,1.6012112,1.6631117,1.7156107,1.7594955,1.7957773,1.8255303,1.8497912,1.8695034,1.885493,1.8984647,1.9090066,1.9176035,1.9246501,1.9304651,1.9353043,1.939372,1.9428307,1.9458094,1.9484099,1.950713,1.952782,1.9546671,1.9564075,1.9580345,1.9595723,1.96104,1.9624528,1.9638224,1.9651581,1.9664673,1.9677554,1.9690273,1.9702863,1.9715352,1.9727762,1.974011,1.9752408,1.9764669,1.97769,1.9789107,1.9801296],[1.991226,1.9900072,1.9887865,1.9875634,1.9863372,1.9851074,1.9838727,1.9826317,1.9813826,1.9801238,1.9788518,1.9775636,1.9762545,1.9749188,1.9735491,1.9721365,1.9706688,1.969131,1.967504,1.9657636,1.9638784,1.9618094,1.9595063,1.9569058,1.953927,1.9504683,1.9464006,1.9415615,1.9357464,1.9287,1.9201031,1.9095612,1.8965895,1.8805997,1.8608875,1.8366268,1.8068738,1.7705919,1.7267072,1.674208,1.6123075,1.5406758,1.4597436,1.3710486,1.2775532,1.1838192,1.095892,1.0207658,0.9654115,0.93552077,0.93552077,0.9654115,1.0207658,1.095892,1.1838192,1.2775532,1.3710486,1.4597436,1.5406758,1.6123075,1.674208,1.7267072,1.7705919,1.8068738,1.8366268,1.8608875,1.8805997,1.8965895,1.9095612,1.9201031,1.9287,1.9357464,1.9415615,1.9464006,1.9504683,1.953927,1.9569058,1.9595063,1.9618094,1.9638784,1.9657636,1.967504,1.969131,1.9706688,1.9721365,1.9735491,1.9749188,1.9762545,1.9775636,1.9788518,1.9801238,1.9813826,1.9826317,1.9838727,1.9851074,1.9863372,1.9875634,1.9887865,1.9900072,1.991226],[2.0008793,1.9996604,1.9984397,1.9972166,1.9959905,1.9947606,1.9935259,1.9922849,1.9910359,1.989777,1.9885051,1.987217,1.9859078,1.984572,1.9832025,1.9817897,1.980322,1.9787842,1.9771572,1.9754168,1.9735317,1.9714627,1.9691596,1.966559,1.9635804,1.9601216,1.956054,1.9512148,1.9453998,1.9383533,1.9297564,1.9192145,1.9062428,1.8902531,1.8705409,1.8462801,1.816527,1.7802451,1.7363604,1.6838613,1.6219609,1.550329,1.4693968,1.3807019,1.2872064,1.1934724,1.1055452,1.0304191,0.97506475,0.94517404,0.94517404,0.97506475,1.0304191,1.1055452,1.1934724,1.2872064,1.3807019,1.4693968,1.550329,1.6219609,1.6838613,1.7363604,1.7802451,1.816527,1.8462801,1.8705409,1.8902531,1.9062428,1.9192145,1.9297564,1.9383533,1.9453998,1.9512148,1.956054,1.9601216,1.9635804,1.966559,1.9691596,1.9714627,1.9735317,1.9754168,1.9771572,1.9787842,1.980322,1.9817897,1.9832025,1.984572,1.9859078,1.987217,1.9885051,1.989777,1.9910359,1.9922849,1.9935259,1.9947606,1.9959905,1.9972166,1.9984397,1.9996604,2.0008793],[2.009251,2.008032,2.0068114,2.0055883,2.004362,2.0031323,2.0018976,2.0006566,1.9994076,1.9981487,1.9968768,1.9955887,1.9942795,1.9929438,1.9915742,1.9901614,1.9886937,1.9871559,1.985529,1.9837885,1.9819034,1.9798344,1.9775313,1.9749308,1.9719521,1.9684933,1.9644257,1.9595865,1.9537715,1.9467249,1.938128,1.9275861,1.9146144,1.8986248,1.8789126,1.8546517,1.8248987,1.7886169,1.7447321,1.6922331,1.6303326,1.5587008,1.4777687,1.3890736,1.2955781,1.2018442,1.1139169,1.0387908,0.98343647,0.95354575,0.95354575,0.98343647,1.0387908,1.1139169,1.2018442,1.2955781,1.3890736,1.4777687,1.5587008,1.6303326,1.6922331,1.7447321,1.7886169,1.8248987,1.8546517,1.8789126,1.8986248,1.9146144,1.9275861,1.938128,1.9467249,1.9537715,1.9595865,1.9644257,1.9684933,1.9719521,1.9749308,1.9775313,1.9798344,1.9819034,1.9837885,1.985529,1.9871559,1.9886937,1.9901614,1.9915742,1.9929438,1.9942795,1.9955887,1.9968768,1.9981487,1.9994076,2.0006566,2.0018976,2.0031323,2.004362,2.0055883,2.0068114,2.008032,2.009251],[2.0164917,2.0152726,2.014052,2.0128288,2.0116029,2.0103729,2.009138,2.0078971,2.0066483,2.0053892,2.0041175,2.0028293,2.0015202,2.0001843,1.9988148,1.997402,1.9959342,1.9943964,1.9927695,1.991029,1.9891441,1.9870751,1.984772,1.9821713,1.9791927,1.975734,1.9716663,1.9668272,1.9610121,1.9539655,1.9453686,1.9348267,1.9218551,1.9058654,1.8861532,1.8618923,1.8321393,1.7958574,1.7519727,1.6994736,1.6375732,1.5659413,1.4850092,1.3963141,1.3028188,1.2090847,1.1211575,1.0460314,0.99067706,0.96078634,0.96078634,0.99067706,1.0460314,1.1211575,1.2090847,1.3028188,1.3963141,1.4850092,1.5659413,1.6375732,1.6994736,1.7519727,1.7958574,1.8321393,1.8618923,1.8861532,1.9058654,1.9218551,1.9348267,1.9453686,1.9539655,1.9610121,1.9668272,1.9716663,1.975734,1.9791927,1.9821713,1.984772,1.9870751,1.9891441,1.991029,1.9927695,1.9943964,1.9959342,1.997402,1.9988148,2.0001843,2.0015202,2.0028293,2.0041175,2.0053892,2.0066483,2.0078971,2.009138,2.0103729,2.0116029,2.0128288,2.014052,2.0152726,2.0164917],[2.0227392,2.0215201,2.0202994,2.0190763,2.0178504,2.0166206,2.0153856,2.014145,2.0128958,2.0116367,2.010365,2.0090768,2.0077677,2.006432,2.0050623,2.0036497,2.002182,2.0006442,1.9990171,1.9972767,1.9953916,1.9933226,1.9910195,1.9884189,1.9854403,1.9819815,1.9779139,1.9730747,1.9672596,1.9602132,1.9516163,1.9410744,1.9281027,1.912113,1.8924007,1.86814,1.8383869,1.802105,1.7582203,1.7057211,1.6438208,1.5721889,1.4912567,1.4025618,1.3090663,1.2153323,1.127405,1.052279,0.99692464,0.9670339,0.9670339,0.99692464,1.052279,1.127405,1.2153323,1.3090663,1.4025618,1.4912567,1.5721889,1.6438208,1.7057211,1.7582203,1.802105,1.8383869,1.86814,1.8924007,1.912113,1.9281027,1.9410744,1.9516163,1.9602132,1.9672596,1.9730747,1.9779139,1.9819815,1.9854403,1.9884189,1.9910195,1.9933226,1.9953916,1.9972767,1.9990171,2.0006442,2.002182,2.0036497,2.0050623,2.006432,2.0077677,2.0090768,2.010365,2.0116367,2.0128958,2.014145,2.0153856,2.0166206,2.0178504,2.0190763,2.0202994,2.0215201,2.0227392],[2.028119,2.0269,2.0256793,2.0244563,2.02323,2.0220003,2.0207655,2.0195246,2.0182757,2.0170166,2.0157447,2.0144567,2.0131474,2.0118117,2.0104423,2.0090294,2.0075617,2.006024,2.004397,2.0026565,2.0007715,1.9987024,1.9963993,1.9937987,1.99082,1.9873613,1.9832937,1.9784545,1.9726394,1.9655929,1.956996,1.946454,1.9334824,1.9174927,1.8977805,1.8735197,1.8437667,1.8074849,1.7636001,1.711101,1.6492006,1.5775688,1.4966366,1.4079416,1.3144461,1.2207122,1.1327848,1.0576588,1.0023044,0.9724137,0.9724137,1.0023044,1.0576588,1.1327848,1.2207122,1.3144461,1.4079416,1.4966366,1.5775688,1.6492006,1.711101,1.7636001,1.8074849,1.8437667,1.8735197,1.8977805,1.9174927,1.9334824,1.946454,1.956996,1.9655929,1.9726394,1.9784545,1.9832937,1.9873613,1.99082,1.9937987,1.9963993,1.9987024,2.0007715,2.0026565,2.004397,2.006024,2.0075617,2.0090294,2.0104423,2.0118117,2.0131474,2.0144567,2.0157447,2.0170166,2.0182757,2.0195246,2.0207655,2.0220003,2.02323,2.0244563,2.0256793,2.0269,2.028119],[2.0327435,2.0315247,2.030304,2.0290809,2.027855,2.026625,2.0253901,2.0241492,2.0229,2.0216413,2.0203695,2.019081,2.0177722,2.0164363,2.0150666,2.013654,2.0121863,2.0106485,2.0090215,2.007281,2.005396,2.003327,2.0010238,1.9984233,1.9954447,1.9919859,1.9879183,1.9830791,1.977264,1.9702175,1.9616206,1.9510787,1.938107,1.9221174,1.9024051,1.8781443,1.8483913,1.8121095,1.7682247,1.7157257,1.6538252,1.5821934,1.5012612,1.4125662,1.3190707,1.2253368,1.1374094,1.0622834,1.006929,0.9770383,0.9770383,1.006929,1.0622834,1.1374094,1.2253368,1.3190707,1.4125662,1.5012612,1.5821934,1.6538252,1.7157257,1.7682247,1.8121095,1.8483913,1.8781443,1.9024051,1.9221174,1.938107,1.9510787,1.9616206,1.9702175,1.977264,1.9830791,1.9879183,1.9919859,1.9954447,1.9984233,2.0010238,2.003327,2.005396,2.007281,2.0090215,2.0106485,2.0121863,2.013654,2.0150666,2.0164363,2.0177722,2.019081,2.0203695,2.0216413,2.0229,2.0241492,2.0253901,2.026625,2.027855,2.0290809,2.030304,2.0315247,2.0327435],[2.036713,2.0354939,2.0342731,2.03305,2.031824,2.030594,2.0293593,2.0281184,2.0268695,2.0256104,2.0243387,2.0230505,2.0217414,2.0204055,2.019036,2.0176232,2.0161555,2.0146177,2.0129907,2.0112503,2.0093653,2.0072963,2.0049932,2.0023925,1.999414,1.9959552,1.9918876,1.9870484,1.9812334,1.9741868,1.9655899,1.955048,1.9420763,1.9260867,1.9063745,1.8821136,1.8523605,1.8160787,1.7721939,1.7196949,1.6577945,1.5861626,1.5052304,1.4165354,1.32304,1.229306,1.1413788,1.0662526,1.0108982,0.9810076,0.9810076,1.0108982,1.0662526,1.1413788,1.229306,1.32304,1.4165354,1.5052304,1.5861626,1.6577945,1.7196949,1.7721939,1.8160787,1.8523605,1.8821136,1.9063745,1.9260867,1.9420763,1.955048,1.9655899,1.9741868,1.9812334,1.9870484,1.9918876,1.9959552,1.999414,2.0023925,2.0049932,2.0072963,2.0093653,2.0112503,2.0129907,2.0146177,2.0161555,2.0176232,2.019036,2.0204055,2.0217414,2.0230505,2.0243387,2.0256104,2.0268695,2.0281184,2.0293593,2.030594,2.031824,2.03305,2.0342731,2.0354939,2.036713],[2.0401154,2.0388966,2.0376759,2.0364528,2.0352266,2.0339968,2.032762,2.031521,2.030272,2.0290132,2.0277412,2.026453,2.0251439,2.0238082,2.0224385,2.021026,2.0195582,2.0180204,2.0163934,2.014653,2.0127678,2.0106988,2.0083957,2.0057952,2.0028164,1.9993577,1.99529,1.9904509,1.9846358,1.9775894,1.9689925,1.9584506,1.9454789,1.9294891,1.9097769,1.8855162,1.8557632,1.8194813,1.7755966,1.7230974,1.661197,1.5895652,1.508633,1.419938,1.3264426,1.2327086,1.1447814,1.0696552,1.0143008,0.98441017,0.98441017,1.0143008,1.0696552,1.1447814,1.2327086,1.3264426,1.419938,1.508633,1.5895652,1.661197,1.7230974,1.7755966,1.8194813,1.8557632,1.8855162,1.9097769,1.9294891,1.9454789,1.9584506,1.9689925,1.9775894,1.9846358,1.9904509,1.99529,1.9993577,2.0028164,2.0057952,2.0083957,2.0106988,2.0127678,2.014653,2.0163934,2.0180204,2.0195582,2.021026,2.0224385,2.0238082,2.0251439,2.026453,2.0277412,2.0290132,2.030272,2.031521,2.032762,2.0339968,2.0352266,2.0364528,2.0376759,2.0388966,2.0401154],[2.0430288,2.04181,2.0405893,2.0393662,2.03814,2.0369103,2.0356755,2.0344346,2.0331855,2.0319266,2.0306547,2.0293665,2.0280573,2.0267217,2.025352,2.0239394,2.0224717,2.0209339,2.019307,2.0175664,2.0156813,2.0136123,2.0113091,2.0087087,2.00573,2.0022712,1.9982035,1.9933643,1.9875493,1.9805028,1.971906,1.961364,1.9483924,1.9324026,1.9126904,1.8884296,1.8586767,1.8223947,1.7785101,1.7260109,1.6641104,1.5924786,1.5115465,1.4228514,1.3293561,1.235622,1.1476948,1.0725687,1.0172143,0.98732364,0.98732364,1.0172143,1.0725687,1.1476948,1.235622,1.3293561,1.4228514,1.5115465,1.5924786,1.6641104,1.7260109,1.7785101,1.8223947,1.8586767,1.8884296,1.9126904,1.9324026,1.9483924,1.961364,1.971906,1.9805028,1.9875493,1.9933643,1.9982035,2.0022712,2.00573,2.0087087,2.0113091,2.0136123,2.0156813,2.0175664,2.019307,2.0209339,2.0224717,2.0239394,2.025352,2.0267217,2.0280573,2.0293665,2.0306547,2.0319266,2.0331855,2.0344346,2.0356755,2.0369103,2.03814,2.0393662,2.0405893,2.04181,2.0430288],[2.0455213,2.0443025,2.0430818,2.0418587,2.0406327,2.0394027,2.038168,2.036927,2.035678,2.034419,2.0331473,2.031859,2.03055,2.0292141,2.0278444,2.0264318,2.024964,2.0234263,2.0217993,2.0200589,2.0181737,2.0161047,2.0138016,2.0112011,2.0082226,2.0047636,2.0006962,1.9958569,1.9900419,1.9829953,1.9743984,1.9638565,1.9508848,1.9348952,1.915183,1.8909221,1.8611691,1.8248873,1.7810025,1.7285035,1.666603,1.5949712,1.514039,1.425344,1.3318485,1.2381146,1.1501873,1.0750612,1.0197068,0.9898161,0.9898161,1.0197068,1.0750612,1.1501873,1.2381146,1.3318485,1.425344,1.514039,1.5949712,1.666603,1.7285035,1.7810025,1.8248873,1.8611691,1.8909221,1.915183,1.9348952,1.9508848,1.9638565,1.9743984,1.9829953,1.9900419,1.9958569,2.0006962,2.0047636,2.0082226,2.0112011,2.0138016,2.0161047,2.0181737,2.0200589,2.0217993,2.0234263,2.024964,2.0264318,2.0278444,2.0292141,2.03055,2.031859,2.0331473,2.034419,2.035678,2.036927,2.038168,2.0394027,2.0406327,2.0418587,2.0430818,2.0443025,2.0455213],[2.0476518,2.046433,2.0452123,2.0439892,2.042763,2.041533,2.0402985,2.0390573,2.0378084,2.0365496,2.0352776,2.0339894,2.0326803,2.0313444,2.029975,2.028562,2.0270944,2.0255566,2.0239296,2.0221891,2.0203042,2.0182352,2.015932,2.0133314,2.0103528,2.006894,2.0028265,1.9979873,1.9921722,1.9851258,1.9765289,1.965987,1.9530153,1.9370255,1.9173133,1.8930526,1.8632995,1.8270175,1.7831329,1.7306337,1.6687334,1.5971014,1.5161693,1.4274744,1.3339789,1.2402449,1.1523176,1.0771916,1.0218372,0.9919465,0.9919465,1.0218372,1.0771916,1.1523176,1.2402449,1.3339789,1.4274744,1.5161693,1.5971014,1.6687334,1.7306337,1.7831329,1.8270175,1.8632995,1.8930526,1.9173133,1.9370255,1.9530153,1.965987,1.9765289,1.9851258,1.9921722,1.9979873,2.0028265,2.006894,2.0103528,2.0133314,2.015932,2.0182352,2.0203042,2.0221891,2.0239296,2.0255566,2.0270944,2.028562,2.029975,2.0313444,2.0326803,2.0339894,2.0352776,2.0365496,2.0378084,2.0390573,2.0402985,2.041533,2.042763,2.0439892,2.0452123,2.046433,2.0476518],[2.0494716,2.0482526,2.0470319,2.0458088,2.0445828,2.0433528,2.042118,2.040877,2.0396283,2.0383692,2.0370975,2.0358093,2.0345001,2.0331643,2.0317948,2.030382,2.0289142,2.0273764,2.0257494,2.024009,2.022124,2.020055,2.017752,2.0151513,2.0121727,2.008714,2.0046463,1.9998071,1.9939921,1.9869455,1.9783486,1.9678067,1.954835,1.9388454,1.9191332,1.8948723,1.8651192,1.8288374,1.7849526,1.7324536,1.6705532,1.5989213,1.5179892,1.4292941,1.3357987,1.2420647,1.1541375,1.0790113,1.023657,0.9937663,0.9937663,1.023657,1.0790113,1.1541375,1.2420647,1.3357987,1.4292941,1.5179892,1.5989213,1.6705532,1.7324536,1.7849526,1.8288374,1.8651192,1.8948723,1.9191332,1.9388454,1.954835,1.9678067,1.9783486,1.9869455,1.9939921,1.9998071,2.0046463,2.008714,2.0121727,2.0151513,2.017752,2.020055,2.022124,2.024009,2.0257494,2.0273764,2.0289142,2.030382,2.0317948,2.0331643,2.0345001,2.0358093,2.0370975,2.0383692,2.0396283,2.040877,2.042118,2.0433528,2.0445828,2.0458088,2.0470319,2.0482526,2.0494716],[2.051025,2.049806,2.0485854,2.0473623,2.0461364,2.0449064,2.0436716,2.0424306,2.0411816,2.0399227,2.038651,2.0373626,2.0360537,2.0347178,2.033348,2.0319355,2.0304677,2.02893,2.027303,2.0255625,2.0236773,2.0216084,2.0193052,2.0167048,2.0137262,2.0102673,2.0061998,2.0013604,1.9955455,1.9884989,1.979902,1.9693601,1.9563885,1.9403988,1.9206866,1.8964257,1.8666728,1.8303909,1.7865062,1.7340071,1.6721066,1.6004748,1.5195427,1.4308476,1.3373522,1.2436182,1.1556909,1.0805649,1.0252105,0.9953198,0.9953198,1.0252105,1.0805649,1.1556909,1.2436182,1.3373522,1.4308476,1.5195427,1.6004748,1.6721066,1.7340071,1.7865062,1.8303909,1.8666728,1.8964257,1.9206866,1.9403988,1.9563885,1.9693601,1.979902,1.9884989,1.9955455,2.0013604,2.0061998,2.0102673,2.0137262,2.0167048,2.0193052,2.0216084,2.0236773,2.0255625,2.027303,2.02893,2.0304677,2.0319355,2.033348,2.0347178,2.0360537,2.0373626,2.038651,2.0399227,2.0411816,2.0424306,2.0436716,2.0449064,2.0461364,2.0473623,2.0485854,2.049806,2.051025],[2.0523505,2.0511317,2.049911,2.048688,2.0474617,2.046232,2.0449972,2.0437562,2.0425072,2.0412483,2.0399764,2.0386882,2.037379,2.0360434,2.0346737,2.033261,2.0317934,2.0302556,2.0286286,2.0268881,2.025003,2.022934,2.0206308,2.0180304,2.0150516,2.0115929,2.0075252,2.002686,1.996871,1.9898245,1.9812276,1.9706857,1.9577141,1.9417243,1.9220121,1.8977513,1.8679984,1.8317164,1.7878318,1.7353326,1.6734321,1.6018003,1.5208682,1.4321731,1.3386778,1.2449437,1.1570165,1.0818903,1.026536,0.99664533,0.99664533,1.026536,1.0818903,1.1570165,1.2449437,1.3386778,1.4321731,1.5208682,1.6018003,1.6734321,1.7353326,1.7878318,1.8317164,1.8679984,1.8977513,1.9220121,1.9417243,1.9577141,1.9706857,1.9812276,1.9898245,1.996871,2.002686,2.0075252,2.0115929,2.0150516,2.0180304,2.0206308,2.022934,2.025003,2.0268881,2.0286286,2.0302556,2.0317934,2.033261,2.0346737,2.0360434,2.037379,2.0386882,2.0399764,2.0412483,2.0425072,2.0437562,2.0449972,2.046232,2.0474617,2.048688,2.049911,2.0511317,2.0523505],[2.053481,2.052262,2.0510414,2.0498183,2.048592,2.0473623,2.0461276,2.0448866,2.0436378,2.0423787,2.0411067,2.0398188,2.0385094,2.0371737,2.0358043,2.0343914,2.0329237,2.031386,2.029759,2.0280185,2.0261335,2.0240645,2.0217614,2.0191607,2.016182,2.0127234,2.0086555,2.0038166,1.9980015,1.9909549,1.982358,1.9718161,1.9588444,1.9428548,1.9231426,1.8988817,1.8691287,1.8328469,1.7889621,1.7364631,1.6745626,1.6029308,1.5219986,1.4333036,1.3398081,1.2460742,1.1581469,1.0830208,1.0276664,0.99777573,0.99777573,1.0276664,1.0830208,1.1581469,1.2460742,1.3398081,1.4333036,1.5219986,1.6029308,1.6745626,1.7364631,1.7889621,1.8328469,1.8691287,1.8988817,1.9231426,1.9428548,1.9588444,1.9718161,1.982358,1.9909549,1.9980015,2.0038166,2.0086555,2.0127234,2.016182,2.0191607,2.0217614,2.0240645,2.0261335,2.0280185,2.029759,2.031386,2.0329237,2.0343914,2.0358043,2.0371737,2.0385094,2.0398188,2.0411067,2.0423787,2.0436378,2.0448866,2.0461276,2.0473623,2.048592,2.0498183,2.0510414,2.052262,2.053481],[2.0544448,2.053226,2.0520053,2.0507822,2.049556,2.048326,2.0470915,2.0458503,2.0446014,2.0433426,2.0420706,2.0407825,2.0394733,2.0381374,2.036768,2.035355,2.0338874,2.0323496,2.0307226,2.0289822,2.0270972,2.0250282,2.022725,2.0201244,2.0171459,2.0136871,2.0096195,2.0047803,1.9989653,1.9919188,1.9833219,1.97278,1.9598083,1.9438186,1.9241064,1.8998456,1.8700925,1.8338106,1.7899259,1.7374268,1.6755264,1.6038945,1.5229623,1.4342674,1.3407719,1.2470379,1.1591107,1.0839846,1.0286303,0.99873954,0.99873954,1.0286303,1.0839846,1.1591107,1.2470379,1.3407719,1.4342674,1.5229623,1.6038945,1.6755264,1.7374268,1.7899259,1.8338106,1.8700925,1.8998456,1.9241064,1.9438186,1.9598083,1.97278,1.9833219,1.9919188,1.9989653,2.0047803,2.0096195,2.0136871,2.0171459,2.0201244,2.022725,2.0250282,2.0270972,2.0289822,2.0307226,2.0323496,2.0338874,2.035355,2.036768,2.0381374,2.0394733,2.0407825,2.0420706,2.0433426,2.0446014,2.0458503,2.0470915,2.048326,2.049556,2.0507822,2.0520053,2.053226,2.0544448],[2.0552664,2.0540473,2.0528266,2.0516036,2.0503774,2.0491476,2.0479128,2.0466719,2.045423,2.044164,2.042892,2.041604,2.0402946,2.038959,2.0375896,2.0361767,2.034709,2.0331712,2.0315442,2.0298038,2.0279188,2.0258498,2.0235467,2.020946,2.0179672,2.0145087,2.0104408,2.005602,1.9997867,1.9927402,1.9841433,1.9736013,1.9606297,1.94464,1.9249278,1.900667,1.870914,1.8346322,1.7907474,1.7382483,1.6763479,1.6047161,1.5237839,1.4350889,1.3415934,1.2478595,1.1599321,1.0848061,1.0294517,0.999561,0.999561,1.0294517,1.0848061,1.1599321,1.2478595,1.3415934,1.4350889,1.5237839,1.6047161,1.6763479,1.7382483,1.7907474,1.8346322,1.870914,1.900667,1.9249278,1.94464,1.9606297,1.9736013,1.9841433,1.9927402,1.9997867,2.005602,2.0104408,2.0145087,2.0179672,2.020946,2.0235467,2.0258498,2.0279188,2.0298038,2.0315442,2.0331712,2.034709,2.0361767,2.0375896,2.038959,2.0402946,2.041604,2.042892,2.044164,2.045423,2.0466719,2.0479128,2.0491476,2.0503774,2.0516036,2.0528266,2.0540473,2.0552664],[2.0559661,2.0547473,2.0535266,2.0523036,2.0510774,2.0498476,2.0486128,2.0473719,2.0461228,2.044864,2.043592,2.0423038,2.0409946,2.039659,2.0382893,2.0368767,2.035409,2.0338712,2.0322442,2.0305037,2.0286186,2.0265496,2.0242465,2.021646,2.0186672,2.0152085,2.0111408,2.0063016,2.0004866,1.9934402,1.9848433,1.9743013,1.9613297,1.9453399,1.9256277,1.901367,1.871614,1.835332,1.7914474,1.7389482,1.6770477,1.6054159,1.5244838,1.4357888,1.3422934,1.2485594,1.1606321,1.085506,1.0301516,1.000261,1.000261,1.0301516,1.085506,1.1606321,1.2485594,1.3422934,1.4357888,1.5244838,1.6054159,1.6770477,1.7389482,1.7914474,1.835332,1.871614,1.901367,1.9256277,1.9453399,1.9613297,1.9743013,1.9848433,1.9934402,2.0004866,2.0063016,2.0111408,2.0152085,2.0186672,2.021646,2.0242465,2.0265496,2.0286186,2.0305037,2.0322442,2.0338712,2.035409,2.0368767,2.0382893,2.039659,2.0409946,2.0423038,2.043592,2.044864,2.0461228,2.0473719,2.0486128,2.0498476,2.0510774,2.0523036,2.0535266,2.0547473,2.0559661],[2.0565624,2.0553436,2.054123,2.0528998,2.0516737,2.0504436,2.049209,2.047968,2.046719,2.0454602,2.0441883,2.0429,2.041591,2.040255,2.0388856,2.0374727,2.036005,2.0344672,2.0328403,2.0310998,2.0292149,2.0271459,2.0248427,2.022242,2.0192635,2.0158048,2.011737,2.006898,2.001083,1.9940364,1.9854395,1.9748976,1.961926,1.9459362,1.926224,1.9019632,1.8722101,1.8359282,1.7920436,1.7395444,1.677644,1.6060121,1.52508,1.436385,1.3428895,1.2491555,1.1612283,1.0861022,1.0307479,1.0008572,1.0008572,1.0307479,1.0861022,1.1612283,1.2491555,1.3428895,1.436385,1.52508,1.6060121,1.677644,1.7395444,1.7920436,1.8359282,1.8722101,1.9019632,1.926224,1.9459362,1.961926,1.9748976,1.9854395,1.9940364,2.001083,2.006898,2.011737,2.0158048,2.0192635,2.022242,2.0248427,2.0271459,2.0292149,2.0310998,2.0328403,2.0344672,2.036005,2.0374727,2.0388856,2.040255,2.041591,2.0429,2.0441883,2.0454602,2.046719,2.047968,2.049209,2.0504436,2.0516737,2.0528998,2.054123,2.0553436,2.0565624],[2.0570703,2.0558515,2.0546308,2.0534077,2.0521815,2.0509515,2.049717,2.0484757,2.047227,2.045968,2.044696,2.043408,2.0420988,2.040763,2.0393934,2.0379806,2.0365129,2.034975,2.033348,2.0316076,2.0297227,2.0276537,2.0253506,2.02275,2.0197713,2.0163126,2.012245,2.0074058,2.0015907,1.9945443,1.9859474,1.9754055,1.9624338,1.946444,1.9267318,1.9024711,1.872718,1.836436,1.7925514,1.7400522,1.6781518,1.6065199,1.5255878,1.4368929,1.3433974,1.2496634,1.1617361,1.0866101,1.0312557,1.0013651,1.0013651,1.0312557,1.0866101,1.1617361,1.2496634,1.3433974,1.4368929,1.5255878,1.6065199,1.6781518,1.7400522,1.7925514,1.836436,1.872718,1.9024711,1.9267318,1.946444,1.9624338,1.9754055,1.9859474,1.9945443,2.0015907,2.0074058,2.012245,2.0163126,2.0197713,2.02275,2.0253506,2.0276537,2.0297227,2.0316076,2.033348,2.034975,2.0365129,2.0379806,2.0393934,2.040763,2.0420988,2.043408,2.044696,2.045968,2.047227,2.0484757,2.049717,2.0509515,2.0521815,2.0534077,2.0546308,2.0558515,2.0570703],[2.0575027,2.056284,2.0550632,2.0538402,2.052614,2.0513842,2.0501494,2.0489085,2.0476594,2.0464005,2.0451286,2.0438404,2.0425313,2.0411956,2.039826,2.0384133,2.0369456,2.0354078,2.0337808,2.0320404,2.0301552,2.0280862,2.025783,2.0231826,2.0202038,2.016745,2.0126774,2.0078382,2.0020232,1.9949768,1.9863799,1.975838,1.9628663,1.9468765,1.9271643,1.9029036,1.8731506,1.8368686,1.792984,1.7404848,1.6785843,1.6069525,1.5260204,1.4373254,1.34383,1.250096,1.1621687,1.0870426,1.0316882,1.0017976,1.0017976,1.0316882,1.0870426,1.1621687,1.250096,1.34383,1.4373254,1.5260204,1.6069525,1.6785843,1.7404848,1.792984,1.8368686,1.8731506,1.9029036,1.9271643,1.9468765,1.9628663,1.975838,1.9863799,1.9949768,2.0020232,2.0078382,2.0126774,2.016745,2.0202038,2.0231826,2.025783,2.0280862,2.0301552,2.0320404,2.0337808,2.0354078,2.0369456,2.0384133,2.039826,2.0411956,2.0425313,2.0438404,2.0451286,2.0464005,2.0476594,2.0489085,2.0501494,2.0513842,2.052614,2.0538402,2.0550632,2.056284,2.0575027]],\"hovertemplate\":\"\\u003cb\\u003ex\\u003c\\u002fb\\u003e = %{x:.2f}\\u003cbr\\u003e\\u003cb\\u003ey\\u003c\\u002fb\\u003e = %{y:.2f}\\u003cbr\\u003e\\u003cb\\u003ez\\u003c\\u002fb\\u003e = %{customdata:.2f}\\u003c\\u002fb\\u003e\",\"showscale\":false,\"x\":[-6.0,-5.878788,-5.757576,-5.6363635,-5.5151515,-5.3939395,-5.272727,-5.151515,-5.030303,-4.909091,-4.787879,-4.6666665,-4.5454545,-4.4242425,-4.30303,-4.181818,-4.060606,-3.939394,-3.8181818,-3.6969695,-3.5757575,-3.4545455,-3.3333333,-3.212121,-3.090909,-2.969697,-2.8484848,-2.7272725,-2.6060605,-2.4848485,-2.3636363,-2.242424,-2.121212,-1.9999999,-1.8787878,-1.7575756,-1.6363635,-1.5151514,-1.3939393,-1.2727271,-1.151515,-1.0303029,-0.90909076,-0.78787863,-0.6666665,-0.5454544,-0.42424226,-0.30303013,-0.18181801,-0.060605884,0.060605884,0.18181801,0.30303013,0.42424226,0.5454544,0.6666665,0.78787863,0.90909076,1.0303029,1.151515,1.2727271,1.3939393,1.5151514,1.6363635,1.7575756,1.8787878,1.9999999,2.121212,2.242424,2.3636363,2.4848485,2.6060605,2.7272725,2.8484848,2.969697,3.090909,3.212121,3.3333333,3.4545455,3.5757575,3.6969695,3.8181818,3.939394,4.060606,4.181818,4.30303,4.4242425,4.5454545,4.6666665,4.787879,4.909091,5.030303,5.151515,5.272727,5.3939395,5.5151515,5.6363635,5.757576,5.878788,6.0],\"y\":[-10.0,-9.838384,-9.676767,-9.515152,-9.353536,-9.191919,-9.030303,-8.868687,-8.70707,-8.545455,-8.383839,-8.222222,-8.060606,-7.8989897,-7.737374,-7.5757575,-7.4141417,-7.2525253,-7.090909,-6.929293,-6.767677,-6.6060605,-6.4444447,-6.2828283,-6.121212,-5.959596,-5.79798,-5.6363635,-5.4747477,-5.3131313,-5.151515,-4.989899,-4.828283,-4.6666665,-4.5050507,-4.3434343,-4.181818,-4.020202,-3.8585858,-3.6969697,-3.5353537,-3.3737373,-3.2121212,-3.0505052,-2.8888888,-2.7272727,-2.5656567,-2.4040403,-2.2424242,-2.0808082,-1.919192,-1.7575758,-1.5959595,-1.4343435,-1.2727273,-1.1111112,-0.94949496,-0.78787875,-0.6262626,-0.46464646,-0.3030303,-0.14141414,0.020202026,0.18181819,0.34343433,0.50505054,0.6666667,0.82828283,0.989899,1.1515151,1.3131313,1.4747474,1.6363636,1.7979798,1.9595959,2.121212,2.2828283,2.4444444,2.6060605,2.7676768,2.929293,3.090909,3.2525253,3.4141414,3.5757575,3.7373738,3.89899,4.060606,4.2222223,4.383838,4.5454545,4.707071,4.8686867,5.030303,5.1919193,5.353535,5.5151515,5.676768,5.8383837,6.0],\"z\":[[1.0600208,1.0588019,1.0575812,1.0563581,1.055132,1.0539021,1.0526674,1.0514264,1.0501775,1.0489185,1.0476466,1.0463585,1.0450493,1.0437136,1.042344,1.0409312,1.0394635,1.0379257,1.0362988,1.0345583,1.0326732,1.0306042,1.0283011,1.0257006,1.0227219,1.0192631,1.0151955,1.0103563,1.0045413,0.99749476,0.98889786,0.97835594,0.9653843,0.9493946,0.9296824,0.90542156,0.8756685,0.83938664,0.79550195,0.74300283,0.6811024,0.60947055,0.5285384,0.43984336,0.34634793,0.25261393,0.1646867,0.0895606,0.034206238,0.0043155537,0.0043155537,0.034206238,0.0895606,0.1646867,0.25261393,0.34634793,0.43984336,0.5285384,0.60947055,0.6811024,0.74300283,0.79550195,0.83938664,0.8756685,0.90542156,0.9296824,0.9493946,0.9653843,0.97835594,0.98889786,0.99749476,1.0045413,1.0103563,1.0151955,1.0192631,1.0227219,1.0257006,1.0283011,1.0306042,1.0326732,1.0345583,1.0362988,1.0379257,1.0394635,1.0409312,1.042344,1.0437136,1.0450493,1.0463585,1.0476466,1.0489185,1.0501775,1.0514264,1.0526674,1.0539021,1.055132,1.0563581,1.0575812,1.0588019,1.0600208],[1.0600288,1.0588099,1.0575892,1.0563661,1.05514,1.0539101,1.0526754,1.0514344,1.0501854,1.0489265,1.0476546,1.0463665,1.0450573,1.0437216,1.042352,1.0409392,1.0394715,1.0379337,1.0363067,1.0345663,1.0326812,1.0306122,1.0283091,1.0257086,1.0227299,1.0192711,1.0152035,1.0103643,1.0045493,0.9975027,0.9889058,0.9783639,0.96539223,0.9494025,0.9296903,0.9054295,0.87567645,0.83939457,0.7955099,0.74301076,0.6811103,0.6094785,0.52854633,0.4398513,0.3463559,0.2526219,0.16469465,0.08956856,0.034214202,0.0043235165,0.0043235165,0.034214202,0.08956856,0.16469465,0.2526219,0.3463559,0.4398513,0.52854633,0.6094785,0.6811103,0.74301076,0.7955099,0.83939457,0.87567645,0.9054295,0.9296903,0.9494025,0.96539223,0.9783639,0.9889058,0.9975027,1.0045493,1.0103643,1.0152035,1.0192711,1.0227299,1.0257086,1.0283091,1.0306122,1.0326812,1.0345663,1.0363067,1.0379337,1.0394715,1.0409392,1.042352,1.0437216,1.0450573,1.0463665,1.0476546,1.0489265,1.0501854,1.0514344,1.0526754,1.0539101,1.05514,1.0563661,1.0575892,1.0588099,1.0600288],[1.0600381,1.0588192,1.0575985,1.0563754,1.0551493,1.0539194,1.0526847,1.0514437,1.0501947,1.0489358,1.0476639,1.0463758,1.0450666,1.0437309,1.0423613,1.0409485,1.0394808,1.037943,1.036316,1.0345756,1.0326905,1.0306215,1.0283184,1.0257179,1.0227392,1.0192804,1.0152128,1.0103736,1.0045586,0.99751204,0.98891515,0.9783732,0.9654016,0.94941187,0.92969966,0.90543884,0.8756858,0.8394039,0.79551923,0.7430201,0.6811197,0.60948783,0.5285557,0.4398607,0.34636527,0.25263128,0.16470401,0.08957792,0.03422356,0.004332876,0.004332876,0.03422356,0.08957792,0.16470401,0.25263128,0.34636527,0.4398607,0.5285557,0.60948783,0.6811197,0.7430201,0.79551923,0.8394039,0.8756858,0.90543884,0.92969966,0.94941187,0.9654016,0.9783732,0.98891515,0.99751204,1.0045586,1.0103736,1.0152128,1.0192804,1.0227392,1.0257179,1.0283184,1.0306215,1.0326905,1.0345756,1.036316,1.037943,1.0394808,1.0409485,1.0423613,1.0437309,1.0450666,1.0463758,1.0476639,1.0489358,1.0501947,1.0514437,1.0526847,1.0539194,1.0551493,1.0563754,1.0575985,1.0588192,1.0600381],[1.060049,1.0588301,1.0576094,1.0563864,1.0551603,1.0539304,1.0526956,1.0514547,1.0502057,1.0489467,1.0476749,1.0463867,1.0450776,1.0437418,1.0423722,1.0409595,1.0394918,1.037954,1.036327,1.0345865,1.0327015,1.0306325,1.0283294,1.0257288,1.0227501,1.0192914,1.0152237,1.0103846,1.0045695,0.99752307,0.9889262,0.97838426,0.9654126,0.9494229,0.9297107,0.90544987,0.87569684,0.83941495,0.79553026,0.74303114,0.6811307,0.60949886,0.5285667,0.4398717,0.34637627,0.25264227,0.164715,0.089588925,0.03423456,0.0043438766,0.0043438766,0.03423456,0.089588925,0.164715,0.25264227,0.34637627,0.4398717,0.5285667,0.60949886,0.6811307,0.74303114,0.79553026,0.83941495,0.87569684,0.90544987,0.9297107,0.9494229,0.9654126,0.97838426,0.9889262,0.99752307,1.0045695,1.0103846,1.0152237,1.0192914,1.0227501,1.0257288,1.0283294,1.0306325,1.0327015,1.0345865,1.036327,1.037954,1.0394918,1.0409595,1.0423722,1.0437418,1.0450776,1.0463867,1.0476749,1.0489467,1.0502057,1.0514547,1.0526956,1.0539304,1.0551603,1.0563864,1.0576094,1.0588301,1.060049],[1.060062,1.0588431,1.0576224,1.0563993,1.0551733,1.0539434,1.0527086,1.0514677,1.0502187,1.0489597,1.0476879,1.0463997,1.0450906,1.0437548,1.0423852,1.0409725,1.0395048,1.037967,1.03634,1.0345995,1.0327145,1.0306455,1.0283424,1.0257418,1.0227631,1.0193044,1.0152367,1.0103976,1.0045825,0.997536,0.9889391,0.9783972,0.96542555,0.94943583,0.9297236,0.9054628,0.8757098,0.8394279,0.7955432,0.7430441,0.68114364,0.6095118,0.52857965,0.43988463,0.3463892,0.2526552,0.16472794,0.08960185,0.03424749,0.004356807,0.004356807,0.03424749,0.08960185,0.16472794,0.2526552,0.3463892,0.43988463,0.52857965,0.6095118,0.68114364,0.7430441,0.7955432,0.8394279,0.8757098,0.9054628,0.9297236,0.94943583,0.96542555,0.9783972,0.9889391,0.997536,1.0045825,1.0103976,1.0152367,1.0193044,1.0227631,1.0257418,1.0283424,1.0306455,1.0327145,1.0345995,1.03634,1.037967,1.0395048,1.0409725,1.0423852,1.0437548,1.0450906,1.0463997,1.0476879,1.0489597,1.0502187,1.0514677,1.0527086,1.0539434,1.0551733,1.0563993,1.0576224,1.0588431,1.060062],[1.0600772,1.0588583,1.0576376,1.0564145,1.0551884,1.0539585,1.0527238,1.0514828,1.0502338,1.0489749,1.047703,1.0464149,1.0451057,1.04377,1.0424004,1.0409876,1.0395199,1.0379821,1.0363551,1.0346147,1.0327296,1.0306606,1.0283575,1.025757,1.0227783,1.0193195,1.0152519,1.0104127,1.0045977,0.9975512,0.9889543,0.9784124,0.96544075,0.949451,0.9297388,0.905478,0.875725,0.8394431,0.7955584,0.7430593,0.68115884,0.609527,0.52859485,0.4398998,0.34640437,0.25267038,0.16474314,0.08961705,0.03426269,0.004372005,0.004372005,0.03426269,0.08961705,0.16474314,0.25267038,0.34640437,0.4398998,0.52859485,0.609527,0.68115884,0.7430593,0.7955584,0.8394431,0.875725,0.905478,0.9297388,0.949451,0.96544075,0.9784124,0.9889543,0.9975512,1.0045977,1.0104127,1.0152519,1.0193195,1.0227783,1.025757,1.0283575,1.0306606,1.0327296,1.0346147,1.0363551,1.0379821,1.0395199,1.0409876,1.0424004,1.04377,1.0451057,1.0464149,1.047703,1.0489749,1.0502338,1.0514828,1.0527238,1.0539585,1.0551884,1.0564145,1.0576376,1.0588583,1.0600772],[1.0600951,1.0588762,1.0576555,1.0564324,1.0552063,1.0539764,1.0527416,1.0515007,1.0502517,1.0489928,1.0477209,1.0464327,1.0451236,1.0437878,1.0424182,1.0410055,1.0395378,1.038,1.036373,1.0346326,1.0327475,1.0306785,1.0283754,1.0257748,1.0227962,1.0193374,1.0152698,1.0104306,1.0046155,0.997569,0.9889721,0.9784302,0.9654586,0.94946885,0.92975664,0.9054958,0.8757428,0.8394609,0.7955762,0.7430771,0.68117666,0.6095448,0.5286127,0.43991768,0.34642226,0.25268826,0.164761,0.08963491,0.034280553,0.0043898677,0.0043898677,0.034280553,0.08963491,0.164761,0.25268826,0.34642226,0.43991768,0.5286127,0.6095448,0.68117666,0.7430771,0.7955762,0.8394609,0.8757428,0.9054958,0.92975664,0.94946885,0.9654586,0.9784302,0.9889721,0.997569,1.0046155,1.0104306,1.0152698,1.0193374,1.0227962,1.0257748,1.0283754,1.0306785,1.0327475,1.0346326,1.036373,1.038,1.0395378,1.0410055,1.0424182,1.0437878,1.0451236,1.0464327,1.0477209,1.0489928,1.0502517,1.0515007,1.0527416,1.0539764,1.0552063,1.0564324,1.0576555,1.0588762,1.0600951],[1.060116,1.0588971,1.0576764,1.0564533,1.0552273,1.0539974,1.0527626,1.0515217,1.0502727,1.0490137,1.0477419,1.0464537,1.0451446,1.0438088,1.0424392,1.0410265,1.0395588,1.038021,1.036394,1.0346535,1.0327685,1.0306995,1.0283964,1.0257958,1.0228171,1.0193584,1.0152907,1.0104516,1.0046365,0.99759007,0.98899317,0.97845125,0.9654796,0.9494899,0.9297777,0.90551686,0.87576383,0.83948195,0.79559726,0.74309814,0.6811977,0.60956585,0.5286337,0.43993866,0.34644324,0.25270924,0.164782,0.089655906,0.03430155,0.0044108634,0.0044108634,0.03430155,0.089655906,0.164782,0.25270924,0.34644324,0.43993866,0.5286337,0.60956585,0.6811977,0.74309814,0.79559726,0.83948195,0.87576383,0.90551686,0.9297777,0.9494899,0.9654796,0.97845125,0.98899317,0.99759007,1.0046365,1.0104516,1.0152907,1.0193584,1.0228171,1.0257958,1.0283964,1.0306995,1.0327685,1.0346535,1.036394,1.038021,1.0395588,1.0410265,1.0424392,1.0438088,1.0451446,1.0464537,1.0477419,1.0490137,1.0502727,1.0515217,1.0527626,1.0539974,1.0552273,1.0564533,1.0576764,1.0588971,1.060116],[1.0601407,1.0589218,1.0577011,1.056478,1.055252,1.0540221,1.0527873,1.0515463,1.0502974,1.0490384,1.0477666,1.0464784,1.0451692,1.0438335,1.0424639,1.0410511,1.0395834,1.0380456,1.0364187,1.0346782,1.0327932,1.0307242,1.028421,1.0258205,1.0228418,1.0193831,1.0153154,1.0104762,1.0046612,0.99761474,0.98901784,0.9784759,0.9655043,0.94951457,0.92980236,0.90554154,0.8757885,0.8395066,0.79562193,0.7431228,0.6812224,0.60959053,0.5286584,0.43996334,0.3464679,0.25273392,0.16480668,0.08968059,0.034326226,0.0044355406,0.0044355406,0.034326226,0.08968059,0.16480668,0.25273392,0.3464679,0.43996334,0.5286584,0.60959053,0.6812224,0.7431228,0.79562193,0.8395066,0.8757885,0.90554154,0.92980236,0.94951457,0.9655043,0.9784759,0.98901784,0.99761474,1.0046612,1.0104762,1.0153154,1.0193831,1.0228418,1.0258205,1.028421,1.0307242,1.0327932,1.0346782,1.0364187,1.0380456,1.0395834,1.0410511,1.0424639,1.0438335,1.0451692,1.0464784,1.0477666,1.0490384,1.0502974,1.0515463,1.0527873,1.0540221,1.055252,1.056478,1.0577011,1.0589218,1.0601407],[1.0601698,1.0589509,1.0577302,1.0565071,1.055281,1.0540512,1.0528164,1.0515754,1.0503265,1.0490675,1.0477957,1.0465075,1.0451983,1.0438626,1.042493,1.0410802,1.0396125,1.0380747,1.0364478,1.0347073,1.0328223,1.0307533,1.0284501,1.0258496,1.0228709,1.0194122,1.0153445,1.0105053,1.0046903,0.9976437,0.9890468,0.9785049,0.96553326,0.94954354,0.9298313,0.9055705,0.8758175,0.8395356,0.7956509,0.7431518,0.68125135,0.6096195,0.52868736,0.43999237,0.34649694,0.25276294,0.16483568,0.08970959,0.03435523,0.004464545,0.004464545,0.03435523,0.08970959,0.16483568,0.25276294,0.34649694,0.43999237,0.52868736,0.6096195,0.68125135,0.7431518,0.7956509,0.8395356,0.8758175,0.9055705,0.9298313,0.94954354,0.96553326,0.9785049,0.9890468,0.9976437,1.0046903,1.0105053,1.0153445,1.0194122,1.0228709,1.0258496,1.0284501,1.0307533,1.0328223,1.0347073,1.0364478,1.0380747,1.0396125,1.0410802,1.042493,1.0438626,1.0451983,1.0465075,1.0477957,1.0490675,1.0503265,1.0515754,1.0528164,1.0540512,1.055281,1.0565071,1.0577302,1.0589509,1.0601698],[1.0602039,1.058985,1.0577643,1.0565412,1.0553151,1.0540853,1.0528505,1.0516095,1.0503606,1.0491016,1.0478297,1.0465416,1.0452324,1.0438967,1.0425271,1.0411143,1.0396466,1.0381088,1.0364819,1.0347414,1.0328563,1.0307873,1.0284842,1.0258837,1.022905,1.0194463,1.0153786,1.0105394,1.0047244,0.9976778,0.9890809,0.978539,0.96556735,0.9495776,0.9298654,0.9056046,0.8758516,0.8395697,0.795685,0.7431859,0.68128544,0.6096536,0.52872145,0.44002643,0.346531,0.252797,0.16486977,0.08974368,0.03438932,0.0044986345,0.0044986345,0.03438932,0.08974368,0.16486977,0.252797,0.346531,0.44002643,0.52872145,0.6096536,0.68128544,0.7431859,0.795685,0.8395697,0.8758516,0.9056046,0.9298654,0.9495776,0.96556735,0.978539,0.9890809,0.9976778,1.0047244,1.0105394,1.0153786,1.0194463,1.022905,1.0258837,1.0284842,1.0307873,1.0328563,1.0347414,1.0364819,1.0381088,1.0396466,1.0411143,1.0425271,1.0438967,1.0452324,1.0465416,1.0478297,1.0491016,1.0503606,1.0516095,1.0528505,1.0540853,1.0553151,1.0565412,1.0577643,1.058985,1.0602039],[1.060244,1.059025,1.0578043,1.0565813,1.0553552,1.0541253,1.0528905,1.0516496,1.0504006,1.0491416,1.0478698,1.0465816,1.0452725,1.0439367,1.0425671,1.0411544,1.0396867,1.0381489,1.0365219,1.0347815,1.0328964,1.0308274,1.0285243,1.0259237,1.022945,1.0194863,1.0154186,1.0105795,1.0047644,0.99771786,0.98912096,0.97857904,0.9656074,0.9496177,0.9299055,0.90564466,0.8758916,0.83960974,0.79572505,0.74322593,0.6813255,0.60969365,0.5287615,0.44006652,0.3465711,0.2528371,0.16490984,0.08978375,0.034429386,0.0045387014,0.0045387014,0.034429386,0.08978375,0.16490984,0.2528371,0.3465711,0.44006652,0.5287615,0.60969365,0.6813255,0.74322593,0.79572505,0.83960974,0.8758916,0.90564466,0.9299055,0.9496177,0.9656074,0.97857904,0.98912096,0.99771786,1.0047644,1.0105795,1.0154186,1.0194863,1.022945,1.0259237,1.0285243,1.0308274,1.0328964,1.0347815,1.0365219,1.0381489,1.0396867,1.0411544,1.0425671,1.0439367,1.0452725,1.0465816,1.0478698,1.0491416,1.0504006,1.0516496,1.0528905,1.0541253,1.0553552,1.0565813,1.0578043,1.059025,1.060244],[1.060291,1.0590721,1.0578514,1.0566283,1.0554023,1.0541724,1.0529376,1.0516967,1.0504477,1.0491887,1.0479169,1.0466287,1.0453196,1.0439838,1.0426142,1.0412015,1.0397338,1.038196,1.036569,1.0348285,1.0329435,1.0308745,1.0285714,1.0259708,1.0229921,1.0195334,1.0154657,1.0106266,1.0048115,0.99776495,0.98916805,0.97862613,0.9656545,0.9496648,0.92995256,0.90569174,0.8759387,0.83965683,0.79577214,0.743273,0.6813726,0.60974073,0.5288086,0.4401136,0.34661818,0.25288418,0.16495693,0.08983084,0.034476478,0.004585792,0.004585792,0.034476478,0.08983084,0.16495693,0.25288418,0.34661818,0.4401136,0.5288086,0.60974073,0.6813726,0.743273,0.79577214,0.83965683,0.8759387,0.90569174,0.92995256,0.9496648,0.9656545,0.97862613,0.98916805,0.99776495,1.0048115,1.0106266,1.0154657,1.0195334,1.0229921,1.0259708,1.0285714,1.0308745,1.0329435,1.0348285,1.036569,1.038196,1.0397338,1.0412015,1.0426142,1.0439838,1.0453196,1.0466287,1.0479169,1.0491887,1.0504477,1.0516967,1.0529376,1.0541724,1.0554023,1.0566283,1.0578514,1.0590721,1.060291],[1.0603464,1.0591274,1.0579067,1.0566837,1.0554576,1.0542277,1.0529929,1.051752,1.050503,1.049244,1.0479722,1.046684,1.0453749,1.0440391,1.0426695,1.0412568,1.0397891,1.0382513,1.0366243,1.0348839,1.0329988,1.0309298,1.0286267,1.0260261,1.0230474,1.0195887,1.015521,1.0106819,1.0048668,0.9978203,0.9892234,0.9786815,0.96570987,0.94972014,0.93000793,0.9057471,0.8759941,0.8397122,0.7958275,0.7433284,0.68142796,0.6097961,0.52886397,0.44016895,0.34667352,0.25293952,0.16501227,0.08988618,0.03453182,0.0046411366,0.0046411366,0.03453182,0.08988618,0.16501227,0.25293952,0.34667352,0.44016895,0.52886397,0.6097961,0.68142796,0.7433284,0.7958275,0.8397122,0.8759941,0.9057471,0.93000793,0.94972014,0.96570987,0.9786815,0.9892234,0.9978203,1.0048668,1.0106819,1.015521,1.0195887,1.0230474,1.0260261,1.0286267,1.0309298,1.0329988,1.0348839,1.0366243,1.0382513,1.0397891,1.0412568,1.0426695,1.0440391,1.0453749,1.046684,1.0479722,1.049244,1.050503,1.051752,1.0529929,1.0542277,1.0554576,1.0566837,1.0579067,1.0591274,1.0603464],[1.0604115,1.0591925,1.0579718,1.0567487,1.0555227,1.0542928,1.053058,1.051817,1.0505681,1.0493091,1.0480373,1.0467491,1.04544,1.0441042,1.0427346,1.0413219,1.0398542,1.0383164,1.0366894,1.034949,1.0330639,1.0309949,1.0286918,1.0260912,1.0231125,1.0196538,1.0155861,1.010747,1.0049319,0.99788535,0.98928845,0.97874653,0.9657749,0.9497852,0.93007296,0.90581214,0.8760591,0.83977723,0.79589254,0.7433934,0.681493,0.60986114,0.528929,0.440234,0.34673858,0.25300458,0.16507731,0.089951225,0.034596868,0.0047061816,0.0047061816,0.034596868,0.089951225,0.16507731,0.25300458,0.34673858,0.440234,0.528929,0.60986114,0.681493,0.7433934,0.79589254,0.83977723,0.8760591,0.90581214,0.93007296,0.9497852,0.9657749,0.97874653,0.98928845,0.99788535,1.0049319,1.010747,1.0155861,1.0196538,1.0231125,1.0260912,1.0286918,1.0309949,1.0330639,1.034949,1.0366894,1.0383164,1.0398542,1.0413219,1.0427346,1.0441042,1.04544,1.0467491,1.0480373,1.0493091,1.0505681,1.051817,1.053058,1.0542928,1.0555227,1.0567487,1.0579718,1.0591925,1.0604115],[1.0604879,1.059269,1.0580482,1.0568252,1.0555991,1.0543692,1.0531344,1.0518935,1.0506445,1.0493855,1.0481137,1.0468255,1.0455164,1.0441806,1.042811,1.0413983,1.0399306,1.0383928,1.0367658,1.0350254,1.0331403,1.0310713,1.0287682,1.0261676,1.023189,1.0197302,1.0156626,1.0108234,1.0050083,0.9979618,0.9893649,0.978823,0.96585137,0.94986165,0.93014944,0.9058886,0.8761356,0.8398537,0.795969,0.7434699,0.68156946,0.6099376,0.52900547,0.44031045,0.34681502,0.25308102,0.16515376,0.09002767,0.03467331,0.004782625,0.004782625,0.03467331,0.09002767,0.16515376,0.25308102,0.34681502,0.44031045,0.52900547,0.6099376,0.68156946,0.7434699,0.795969,0.8398537,0.8761356,0.9058886,0.93014944,0.94986165,0.96585137,0.978823,0.9893649,0.9979618,1.0050083,1.0108234,1.0156626,1.0197302,1.023189,1.0261676,1.0287682,1.0310713,1.0331403,1.0350254,1.0367658,1.0383928,1.0399306,1.0413983,1.042811,1.0441806,1.0455164,1.0468255,1.0481137,1.0493855,1.0506445,1.0518935,1.0531344,1.0543692,1.0555991,1.0568252,1.0580482,1.059269,1.0604879],[1.0605778,1.0593588,1.0581381,1.056915,1.055689,1.0544591,1.0532243,1.0519834,1.0507344,1.0494754,1.0482036,1.0469154,1.0456063,1.0442705,1.0429009,1.0414882,1.0400205,1.0384827,1.0368557,1.0351152,1.0332302,1.0311612,1.0288581,1.0262575,1.0232788,1.0198201,1.0157524,1.0109133,1.0050982,0.99805164,0.98945475,0.97891283,0.9659412,0.94995147,0.93023926,0.90597844,0.8762254,0.8399435,0.79605883,0.7435597,0.6816593,0.61002743,0.5290953,0.44040027,0.34690484,0.25317085,0.1652436,0.09011751,0.034763146,0.0048724622,0.0048724622,0.034763146,0.09011751,0.1652436,0.25317085,0.34690484,0.44040027,0.5290953,0.61002743,0.6816593,0.7435597,0.79605883,0.8399435,0.8762254,0.90597844,0.93023926,0.94995147,0.9659412,0.97891283,0.98945475,0.99805164,1.0050982,1.0109133,1.0157524,1.0198201,1.0232788,1.0262575,1.0288581,1.0311612,1.0332302,1.0351152,1.0368557,1.0384827,1.0400205,1.0414882,1.0429009,1.0442705,1.0456063,1.0469154,1.0482036,1.0494754,1.0507344,1.0519834,1.0532243,1.0544591,1.055689,1.056915,1.0581381,1.0593588,1.0605778],[1.0606833,1.0594643,1.0582436,1.0570205,1.0557945,1.0545646,1.0533298,1.0520889,1.0508399,1.0495809,1.0483091,1.0470209,1.0457118,1.044376,1.0430064,1.0415937,1.040126,1.0385882,1.0369612,1.0352207,1.0333357,1.0312667,1.0289636,1.026363,1.0233843,1.0199256,1.0158579,1.0110188,1.0052037,0.9981572,0.9895603,0.9790184,0.96604675,0.950057,0.9303448,0.906084,0.876331,0.8400491,0.7961644,0.7436653,0.68176484,0.610133,0.52920085,0.44050586,0.34701043,0.25327644,0.16534917,0.09022308,0.034868725,0.0049780374,0.0049780374,0.034868725,0.09022308,0.16534917,0.25327644,0.34701043,0.44050586,0.52920085,0.610133,0.68176484,0.7436653,0.7961644,0.8400491,0.876331,0.906084,0.9303448,0.950057,0.96604675,0.9790184,0.9895603,0.9981572,1.0052037,1.0110188,1.0158579,1.0199256,1.0233843,1.026363,1.0289636,1.0312667,1.0333357,1.0352207,1.0369612,1.0385882,1.040126,1.0415937,1.0430064,1.044376,1.0457118,1.0470209,1.0483091,1.0495809,1.0508399,1.0520889,1.0533298,1.0545646,1.0557945,1.0570205,1.0582436,1.0594643,1.0606833],[1.0608073,1.0595884,1.0583677,1.0571446,1.0559186,1.0546887,1.0534539,1.052213,1.050964,1.049705,1.0484332,1.047145,1.0458359,1.0445001,1.0431305,1.0417178,1.0402501,1.0387123,1.0370853,1.0353448,1.0334598,1.0313908,1.0290877,1.0264871,1.0235084,1.0200497,1.015982,1.0111428,1.0053278,0.9982813,0.9896844,0.9791425,0.96617085,0.9501811,0.9304689,0.9062081,0.87645507,0.8401732,0.7962885,0.7437894,0.68188894,0.6102571,0.52932495,0.44062993,0.3471345,0.2534005,0.16547324,0.09034715,0.034992788,0.0051021036,0.0051021036,0.034992788,0.09034715,0.16547324,0.2534005,0.3471345,0.44062993,0.52932495,0.6102571,0.68188894,0.7437894,0.7962885,0.8401732,0.87645507,0.9062081,0.9304689,0.9501811,0.96617085,0.9791425,0.9896844,0.9982813,1.0053278,1.0111428,1.015982,1.0200497,1.0235084,1.0264871,1.0290877,1.0313908,1.0334598,1.0353448,1.0370853,1.0387123,1.0402501,1.0417178,1.0431305,1.0445001,1.0458359,1.047145,1.0484332,1.049705,1.050964,1.052213,1.0534539,1.0546887,1.0559186,1.0571446,1.0583677,1.0595884,1.0608073],[1.0609531,1.0597342,1.0585135,1.0572904,1.0560644,1.0548345,1.0535997,1.0523587,1.0511098,1.0498508,1.048579,1.0472908,1.0459816,1.0446459,1.0432763,1.0418636,1.0403959,1.038858,1.0372311,1.0354906,1.0336056,1.0315366,1.0292335,1.0266329,1.0236542,1.0201955,1.0161278,1.0112886,1.0054736,0.9984271,0.9898302,0.9792883,0.96631664,0.9503269,0.9306147,0.9063539,0.87660086,0.840319,0.7964343,0.74393517,0.68203473,0.6104029,0.52947074,0.4407757,0.34728026,0.25354627,0.16561903,0.090492934,0.035138577,0.0052478914,0.0052478914,0.035138577,0.090492934,0.16561903,0.25354627,0.34728026,0.4407757,0.52947074,0.6104029,0.68203473,0.74393517,0.7964343,0.840319,0.87660086,0.9063539,0.9306147,0.9503269,0.96631664,0.9792883,0.9898302,0.9984271,1.0054736,1.0112886,1.0161278,1.0201955,1.0236542,1.0266329,1.0292335,1.0315366,1.0336056,1.0354906,1.0372311,1.038858,1.0403959,1.0418636,1.0432763,1.0446459,1.0459816,1.0472908,1.048579,1.0498508,1.0511098,1.0523587,1.0535997,1.0548345,1.0560644,1.0572904,1.0585135,1.0597342,1.0609531],[1.0611244,1.0599055,1.0586848,1.0574617,1.0562357,1.0550058,1.053771,1.05253,1.0512811,1.0500221,1.0487503,1.0474621,1.046153,1.0448172,1.0434476,1.0420349,1.0405672,1.0390294,1.0374024,1.0356619,1.0337769,1.0317079,1.0294048,1.0268042,1.0238255,1.0203668,1.0162991,1.01146,1.0056449,0.9985984,0.9900015,0.9794596,0.96648794,0.9504982,0.930786,0.9065252,0.87677217,0.8404903,0.7966056,0.7441065,0.68220603,0.6105742,0.52964205,0.440947,0.34745157,0.25371757,0.16579033,0.090664245,0.035309885,0.0054191984,0.0054191984,0.035309885,0.090664245,0.16579033,0.25371757,0.34745157,0.440947,0.52964205,0.6105742,0.68220603,0.7441065,0.7966056,0.8404903,0.87677217,0.9065252,0.930786,0.9504982,0.96648794,0.9794596,0.9900015,0.9985984,1.0056449,1.01146,1.0162991,1.0203668,1.0238255,1.0268042,1.0294048,1.0317079,1.0337769,1.0356619,1.0374024,1.0390294,1.0405672,1.0420349,1.0434476,1.0448172,1.046153,1.0474621,1.0487503,1.0500221,1.0512811,1.05253,1.053771,1.0550058,1.0562357,1.0574617,1.0586848,1.0599055,1.0611244],[1.0613257,1.0601068,1.058886,1.057663,1.0564369,1.055207,1.0539722,1.0527313,1.0514823,1.0502234,1.0489515,1.0476633,1.0463542,1.0450184,1.0436488,1.0422361,1.0407684,1.0392306,1.0376036,1.0358632,1.0339781,1.0319091,1.029606,1.0270054,1.0240268,1.020568,1.0165004,1.0116612,1.0058461,0.9987997,0.9902028,0.97966087,0.9666892,0.9506995,0.9309873,0.9067265,0.87697345,0.84069157,0.7968069,0.74430776,0.6824073,0.6107755,0.52984333,0.44114828,0.34765285,0.25391886,0.16599162,0.09086552,0.035511162,0.0056204787,0.0056204787,0.035511162,0.09086552,0.16599162,0.25391886,0.34765285,0.44114828,0.52984333,0.6107755,0.6824073,0.74430776,0.7968069,0.84069157,0.87697345,0.9067265,0.9309873,0.9506995,0.9666892,0.97966087,0.9902028,0.9987997,1.0058461,1.0116612,1.0165004,1.020568,1.0240268,1.0270054,1.029606,1.0319091,1.0339781,1.0358632,1.0376036,1.0392306,1.0407684,1.0422361,1.0436488,1.0450184,1.0463542,1.0476633,1.0489515,1.0502234,1.0514823,1.0527313,1.0539722,1.055207,1.0564369,1.057663,1.058886,1.0601068,1.0613257],[1.0615622,1.0603433,1.0591226,1.0578995,1.0566734,1.0554435,1.0542088,1.0529678,1.0517188,1.0504599,1.049188,1.0478998,1.0465907,1.045255,1.0438854,1.0424726,1.0410049,1.0394671,1.0378401,1.0360997,1.0342146,1.0321456,1.0298425,1.027242,1.0242633,1.0208045,1.0167369,1.0118977,1.0060827,0.99903613,0.99043924,0.9798973,0.9669257,0.95093596,0.93122375,0.90696293,0.8772099,0.840928,0.7970433,0.7445442,0.6826438,0.6110119,0.5300798,0.44138476,0.34788933,0.25415534,0.1662281,0.091102004,0.035747647,0.005856961,0.005856961,0.035747647,0.091102004,0.1662281,0.25415534,0.34788933,0.44138476,0.5300798,0.6110119,0.6826438,0.7445442,0.7970433,0.840928,0.8772099,0.90696293,0.93122375,0.95093596,0.9669257,0.9798973,0.99043924,0.99903613,1.0060827,1.0118977,1.0167369,1.0208045,1.0242633,1.027242,1.0298425,1.0321456,1.0342146,1.0360997,1.0378401,1.0394671,1.0410049,1.0424726,1.0438854,1.045255,1.0465907,1.0478998,1.049188,1.0504599,1.0517188,1.0529678,1.0542088,1.0554435,1.0566734,1.0578995,1.0591226,1.0603433,1.0615622],[1.06184,1.0606211,1.0594004,1.0581774,1.0569513,1.0557214,1.0544866,1.0532457,1.0519967,1.0507377,1.0494659,1.0481777,1.0468686,1.0455328,1.0441632,1.0427505,1.0412828,1.039745,1.038118,1.0363775,1.0344925,1.0324235,1.0301204,1.0275198,1.0245411,1.0210824,1.0170147,1.0121756,1.0063605,0.99931395,0.99071705,0.98017514,0.9672035,0.9512138,0.93150157,0.90724075,0.8774877,0.84120584,0.79732114,0.744822,0.6829216,0.61128974,0.5303576,0.44166258,0.34816715,0.25443316,0.16650592,0.09137983,0.03602547,0.006134782,0.006134782,0.03602547,0.09137983,0.16650592,0.25443316,0.34816715,0.44166258,0.5303576,0.61128974,0.6829216,0.744822,0.79732114,0.84120584,0.8774877,0.90724075,0.93150157,0.9512138,0.9672035,0.98017514,0.99071705,0.99931395,1.0063605,1.0121756,1.0170147,1.0210824,1.0245411,1.0275198,1.0301204,1.0324235,1.0344925,1.0363775,1.038118,1.039745,1.0412828,1.0427505,1.0441632,1.0455328,1.0468686,1.0481777,1.0494659,1.0507377,1.0519967,1.0532457,1.0544866,1.0557214,1.0569513,1.0581774,1.0594004,1.0606211,1.06184],[1.0621663,1.0609474,1.0597267,1.0585036,1.0572776,1.0560477,1.0548129,1.0535719,1.052323,1.051064,1.0497922,1.048504,1.0471948,1.0458591,1.0444895,1.0430768,1.041609,1.0400712,1.0384443,1.0367038,1.0348188,1.0327498,1.0304466,1.0278461,1.0248674,1.0214087,1.017341,1.0125018,1.0066868,0.99964035,0.99104345,0.98050153,0.9675299,0.9515402,0.93182796,0.90756714,0.8778141,0.84153223,0.79764754,0.7451484,0.683248,0.61161613,0.530684,0.44198895,0.34849352,0.25475952,0.16683227,0.09170619,0.036351822,0.006461138,0.006461138,0.036351822,0.09170619,0.16683227,0.25475952,0.34849352,0.44198895,0.530684,0.61161613,0.683248,0.7451484,0.79764754,0.84153223,0.8778141,0.90756714,0.93182796,0.9515402,0.9675299,0.98050153,0.99104345,0.99964035,1.0066868,1.0125018,1.017341,1.0214087,1.0248674,1.0278461,1.0304466,1.0327498,1.0348188,1.0367038,1.0384443,1.0400712,1.041609,1.0430768,1.0444895,1.0458591,1.0471948,1.048504,1.0497922,1.051064,1.052323,1.0535719,1.0548129,1.0560477,1.0572776,1.0585036,1.0597267,1.0609474,1.0621663],[1.0625497,1.0613308,1.0601101,1.058887,1.0576609,1.056431,1.0551963,1.0539553,1.0527064,1.0514474,1.0501755,1.0488874,1.0475782,1.0462425,1.0448729,1.0434601,1.0419924,1.0404546,1.0388277,1.0370872,1.0352021,1.0331331,1.03083,1.0282295,1.0252508,1.021792,1.0177244,1.0128852,1.0070702,1.0000236,0.99142677,0.98088485,0.9679132,0.9519235,0.9322113,0.90795046,0.87819743,0.84191555,0.79803085,0.74553174,0.6836313,0.61199945,0.5310673,0.4423723,0.34887686,0.25514287,0.1672156,0.09208951,0.03673515,0.0068444656,0.0068444656,0.03673515,0.09208951,0.1672156,0.25514287,0.34887686,0.4423723,0.5310673,0.61199945,0.6836313,0.74553174,0.79803085,0.84191555,0.87819743,0.90795046,0.9322113,0.9519235,0.9679132,0.98088485,0.99142677,1.0000236,1.0070702,1.0128852,1.0177244,1.021792,1.0252508,1.0282295,1.03083,1.0331331,1.0352021,1.0370872,1.0388277,1.0404546,1.0419924,1.0434601,1.0448729,1.0462425,1.0475782,1.0488874,1.0501755,1.0514474,1.0527064,1.0539553,1.0551963,1.056431,1.0576609,1.058887,1.0601101,1.0613308,1.0625497],[1.0629998,1.0617809,1.0605602,1.0593371,1.0581111,1.0568812,1.0556464,1.0544055,1.0531565,1.0518975,1.0506257,1.0493375,1.0480283,1.0466926,1.045323,1.0439103,1.0424426,1.0409048,1.0392778,1.0375373,1.0356523,1.0335833,1.0312802,1.0286796,1.0257009,1.0222422,1.0181745,1.0133353,1.0075203,1.0004739,0.99187696,0.98133504,0.9683634,0.9523737,0.9326615,0.90840065,0.8786476,0.84236574,0.79848105,0.74598193,0.6840815,0.61244965,0.5315175,0.44282246,0.34932703,0.25559303,0.1676658,0.092539705,0.03718534,0.0072946576,0.0072946576,0.03718534,0.092539705,0.1676658,0.25559303,0.34932703,0.44282246,0.5315175,0.61244965,0.6840815,0.74598193,0.79848105,0.84236574,0.8786476,0.90840065,0.9326615,0.9523737,0.9683634,0.98133504,0.99187696,1.0004739,1.0075203,1.0133353,1.0181745,1.0222422,1.0257009,1.0286796,1.0312802,1.0335833,1.0356523,1.0375373,1.0392778,1.0409048,1.0424426,1.0439103,1.045323,1.0466926,1.0480283,1.0493375,1.0506257,1.0518975,1.0531565,1.0544055,1.0556464,1.0568812,1.0581111,1.0593371,1.0605602,1.0617809,1.0629998],[1.0635285,1.0623096,1.0610889,1.0598658,1.0586398,1.0574099,1.0561751,1.0549341,1.0536852,1.0524262,1.0511544,1.0498662,1.048557,1.0472213,1.0458517,1.044439,1.0429713,1.0414335,1.0398065,1.038066,1.036181,1.034112,1.0318089,1.0292083,1.0262296,1.0227709,1.0187032,1.013864,1.008049,1.0010024,0.9924056,0.9818637,0.96889204,0.9529023,0.9331901,0.9089293,0.87917626,0.8428944,0.7990097,0.74651057,0.6846101,0.6129783,0.53204614,0.44335112,0.3498557,0.2561217,0.16819443,0.09306835,0.037713982,0.007823298,0.007823298,0.037713982,0.09306835,0.16819443,0.2561217,0.3498557,0.44335112,0.53204614,0.6129783,0.6846101,0.74651057,0.7990097,0.8428944,0.87917626,0.9089293,0.9331901,0.9529023,0.96889204,0.9818637,0.9924056,1.0010024,1.008049,1.013864,1.0187032,1.0227709,1.0262296,1.0292083,1.0318089,1.034112,1.036181,1.038066,1.0398065,1.0414335,1.0429713,1.044439,1.0458517,1.0472213,1.048557,1.0498662,1.0511544,1.0524262,1.0536852,1.0549341,1.0561751,1.0574099,1.0586398,1.0598658,1.0610889,1.0623096,1.0635285],[1.0641491,1.0629302,1.0617095,1.0604864,1.0592604,1.0580305,1.0567957,1.0555547,1.0543058,1.0530468,1.051775,1.0504868,1.0491776,1.0478419,1.0464723,1.0450596,1.0435919,1.042054,1.0404271,1.0386866,1.0368016,1.0347326,1.0324295,1.0298289,1.0268502,1.0233915,1.0193238,1.0144846,1.0086696,1.0016232,0.99302626,0.98248434,0.9695127,0.953523,0.9338108,0.90954995,0.8797969,0.84351504,0.79963034,0.7471312,0.6852308,0.61359894,0.5326668,0.44397175,0.35047632,0.25674233,0.16881508,0.093688995,0.038334634,0.008443948,0.008443948,0.038334634,0.093688995,0.16881508,0.25674233,0.35047632,0.44397175,0.5326668,0.61359894,0.6852308,0.7471312,0.79963034,0.84351504,0.8797969,0.90954995,0.9338108,0.953523,0.9695127,0.98248434,0.99302626,1.0016232,1.0086696,1.0144846,1.0193238,1.0233915,1.0268502,1.0298289,1.0324295,1.0347326,1.0368016,1.0386866,1.0404271,1.042054,1.0435919,1.0450596,1.0464723,1.0478419,1.0491776,1.0504868,1.051775,1.0530468,1.0543058,1.0555547,1.0567957,1.0580305,1.0592604,1.0604864,1.0617095,1.0629302,1.0641491],[1.0648777,1.0636588,1.0624381,1.061215,1.059989,1.0587591,1.0575243,1.0562834,1.0550344,1.0537754,1.0525036,1.0512154,1.0499063,1.0485705,1.0472009,1.0457882,1.0443205,1.0427827,1.0411557,1.0394152,1.0375302,1.0354612,1.0331581,1.0305575,1.0275788,1.0241201,1.0200524,1.0152133,1.0093982,1.0023516,0.99375474,0.9832128,0.9702412,0.95425147,0.93453926,0.91027844,0.8805254,0.8442435,0.80035883,0.7478597,0.6859593,0.61432743,0.5333953,0.4447003,0.35120487,0.25747088,0.16954361,0.09441753,0.039063167,0.009172481,0.009172481,0.039063167,0.09441753,0.16954361,0.25747088,0.35120487,0.4447003,0.5333953,0.61432743,0.6859593,0.7478597,0.80035883,0.8442435,0.8805254,0.91027844,0.93453926,0.95425147,0.9702412,0.9832128,0.99375474,1.0023516,1.0093982,1.0152133,1.0200524,1.0241201,1.0275788,1.0305575,1.0331581,1.0354612,1.0375302,1.0394152,1.0411557,1.0427827,1.0443205,1.0457882,1.0472009,1.0485705,1.0499063,1.0512154,1.0525036,1.0537754,1.0550344,1.0562834,1.0575243,1.0587591,1.059989,1.061215,1.0624381,1.0636588,1.0648777],[1.0657327,1.0645138,1.0632931,1.06207,1.060844,1.0596141,1.0583793,1.0571383,1.0558894,1.0546304,1.0533586,1.0520704,1.0507612,1.0494255,1.0480559,1.0466431,1.0451754,1.0436376,1.0420107,1.0402702,1.0383852,1.0363162,1.034013,1.0314125,1.0284338,1.0249751,1.0209074,1.0160682,1.0102532,1.0032066,0.9946097,0.9840678,0.97109616,0.95510644,0.9353942,0.9111334,0.8813804,0.8450985,0.8012138,0.7487147,0.68681425,0.6151824,0.53425026,0.44555527,0.35205984,0.25832582,0.17039858,0.09527249,0.03991813,0.010027443,0.010027443,0.03991813,0.09527249,0.17039858,0.25832582,0.35205984,0.44555527,0.53425026,0.6151824,0.68681425,0.7487147,0.8012138,0.8450985,0.8813804,0.9111334,0.9353942,0.95510644,0.97109616,0.9840678,0.9946097,1.0032066,1.0102532,1.0160682,1.0209074,1.0249751,1.0284338,1.0314125,1.034013,1.0363162,1.0383852,1.0402702,1.0420107,1.0436376,1.0451754,1.0466431,1.0480559,1.0494255,1.0507612,1.0520704,1.0533586,1.0546304,1.0558894,1.0571383,1.0583793,1.0596141,1.060844,1.06207,1.0632931,1.0645138,1.0657327],[1.0667357,1.0655168,1.0642961,1.063073,1.061847,1.0606171,1.0593823,1.0581414,1.0568924,1.0556334,1.0543616,1.0530734,1.0517642,1.0504285,1.0490589,1.0476462,1.0461785,1.0446407,1.0430137,1.0412732,1.0393882,1.0373192,1.0350161,1.0324155,1.0294368,1.0259781,1.0219104,1.0170712,1.0112562,1.0042096,0.9956128,0.9850709,0.97209924,0.9561095,0.9363973,0.9121365,0.88238347,0.8461016,0.8022169,0.7497178,0.68781734,0.6161855,0.53525335,0.4465583,0.35306287,0.25932887,0.17140163,0.09627554,0.040921178,0.011030493,0.011030493,0.040921178,0.09627554,0.17140163,0.25932887,0.35306287,0.4465583,0.53525335,0.6161855,0.68781734,0.7497178,0.8022169,0.8461016,0.88238347,0.9121365,0.9363973,0.9561095,0.97209924,0.9850709,0.9956128,1.0042096,1.0112562,1.0170712,1.0219104,1.0259781,1.0294368,1.0324155,1.0350161,1.0373192,1.0393882,1.0412732,1.0430137,1.0446407,1.0461785,1.0476462,1.0490589,1.0504285,1.0517642,1.0530734,1.0543616,1.0556334,1.0568924,1.0581414,1.0593823,1.0606171,1.061847,1.063073,1.0642961,1.0655168,1.0667357],[1.0679121,1.0666932,1.0654725,1.0642494,1.0630233,1.0617934,1.0605587,1.0593177,1.0580688,1.0568098,1.0555379,1.0542498,1.0529406,1.0516049,1.0502353,1.0488225,1.0473548,1.045817,1.04419,1.0424496,1.0405645,1.0384955,1.0361924,1.0335919,1.0306132,1.0271544,1.0230868,1.0182476,1.0124326,1.0053861,0.9967892,0.9862473,0.97327566,0.95728594,0.93757373,0.9133129,0.8835599,0.847278,0.8033933,0.7508942,0.68899375,0.6173619,0.53642976,0.4477347,0.35423928,0.2605053,0.17257804,0.097451955,0.04209759,0.012206907,0.012206907,0.04209759,0.097451955,0.17257804,0.2605053,0.35423928,0.4477347,0.53642976,0.6173619,0.68899375,0.7508942,0.8033933,0.847278,0.8835599,0.9133129,0.93757373,0.95728594,0.97327566,0.9862473,0.9967892,1.0053861,1.0124326,1.0182476,1.0230868,1.0271544,1.0306132,1.0335919,1.0361924,1.0384955,1.0405645,1.0424496,1.04419,1.045817,1.0473548,1.0488225,1.0502353,1.0516049,1.0529406,1.0542498,1.0555379,1.0568098,1.0580688,1.0593177,1.0605587,1.0617934,1.0630233,1.0642494,1.0654725,1.0666932,1.0679121],[1.0692914,1.0680724,1.0668517,1.0656286,1.0644026,1.0631727,1.0619379,1.060697,1.059448,1.058189,1.0569172,1.055629,1.0543199,1.0529841,1.0516145,1.0502018,1.0487341,1.0471963,1.0455693,1.0438288,1.0419438,1.0398748,1.0375717,1.0349711,1.0319924,1.0285337,1.024466,1.0196269,1.0138118,1.0067652,0.9981684,0.9876265,0.97465485,0.95866513,0.9389529,0.9146921,0.8849391,0.8486572,0.8047725,0.7522734,0.69037294,0.6187411,0.53780895,0.44911394,0.3556185,0.2618845,0.17395726,0.09883116,0.0434768,0.013586117,0.013586117,0.0434768,0.09883116,0.17395726,0.2618845,0.3556185,0.44911394,0.53780895,0.6187411,0.69037294,0.7522734,0.8047725,0.8486572,0.8849391,0.9146921,0.9389529,0.95866513,0.97465485,0.9876265,0.9981684,1.0067652,1.0138118,1.0196269,1.024466,1.0285337,1.0319924,1.0349711,1.0375717,1.0398748,1.0419438,1.0438288,1.0455693,1.0471963,1.0487341,1.0502018,1.0516145,1.0529841,1.0543199,1.055629,1.0569172,1.058189,1.059448,1.060697,1.0619379,1.0631727,1.0644026,1.0656286,1.0668517,1.0680724,1.0692914],[1.0709076,1.0696887,1.068468,1.0672449,1.0660188,1.0647889,1.0635542,1.0623132,1.0610642,1.0598053,1.0585334,1.0572453,1.0559361,1.0546004,1.0532308,1.051818,1.0503503,1.0488125,1.0471855,1.0454451,1.04356,1.041491,1.0391879,1.0365874,1.0336087,1.0301499,1.0260823,1.0212431,1.0154281,1.0083815,0.99978465,0.98924273,0.9762711,0.9602814,0.94056916,0.91630834,0.8865553,0.85027343,0.80638874,0.7538896,0.6919892,0.62035733,0.5394252,0.45073017,0.35723475,0.26350075,0.17557348,0.1004474,0.045093037,0.015202352,0.015202352,0.045093037,0.1004474,0.17557348,0.26350075,0.35723475,0.45073017,0.5394252,0.62035733,0.6919892,0.7538896,0.80638874,0.85027343,0.8865553,0.91630834,0.94056916,0.9602814,0.9762711,0.98924273,0.99978465,1.0083815,1.0154281,1.0212431,1.0260823,1.0301499,1.0336087,1.0365874,1.0391879,1.041491,1.04356,1.0454451,1.0471855,1.0488125,1.0503503,1.051818,1.0532308,1.0546004,1.0559361,1.0572453,1.0585334,1.0598053,1.0610642,1.0623132,1.0635542,1.0647889,1.0660188,1.0672449,1.068468,1.0696887,1.0709076],[1.0728006,1.0715817,1.070361,1.0691379,1.0679119,1.066682,1.0654472,1.0642062,1.0629573,1.0616983,1.0604265,1.0591383,1.0578291,1.0564934,1.0551238,1.053711,1.0522434,1.0507056,1.0490786,1.0473381,1.0454531,1.0433841,1.041081,1.0384804,1.0355017,1.032043,1.0279753,1.0231361,1.0173211,1.0102745,1.0016776,0.9911357,0.9781641,0.96217436,0.94246215,0.9182013,0.8884483,0.8521664,0.8082817,0.7557826,0.69388217,0.6222503,0.5413182,0.4526232,0.35912776,0.26539376,0.1774665,0.102340415,0.04698605,0.017095365,0.017095365,0.04698605,0.102340415,0.1774665,0.26539376,0.35912776,0.4526232,0.5413182,0.6222503,0.69388217,0.7557826,0.8082817,0.8521664,0.8884483,0.9182013,0.94246215,0.96217436,0.9781641,0.9911357,1.0016776,1.0102745,1.0173211,1.0231361,1.0279753,1.032043,1.0355017,1.0384804,1.041081,1.0433841,1.0454531,1.0473381,1.0490786,1.0507056,1.0522434,1.053711,1.0551238,1.0564934,1.0578291,1.0591383,1.0604265,1.0616983,1.0629573,1.0642062,1.0654472,1.066682,1.0679119,1.0691379,1.070361,1.0715817,1.0728006],[1.0750164,1.0737975,1.0725768,1.0713537,1.0701276,1.0688977,1.067663,1.066422,1.065173,1.0639141,1.0626422,1.061354,1.0600449,1.0587091,1.0573395,1.0559268,1.0544591,1.0529213,1.0512943,1.0495539,1.0476688,1.0455998,1.0432967,1.0406961,1.0377175,1.0342587,1.0301911,1.0253519,1.0195369,1.0124904,1.0038935,0.9933516,0.98037994,0.9643902,0.944678,0.9204172,0.89066416,0.8543823,0.8104976,0.75799847,0.696098,0.6244662,0.54353404,0.454839,0.36134356,0.26760957,0.17968233,0.10455623,0.049201876,0.01931119,0.01931119,0.049201876,0.10455623,0.17968233,0.26760957,0.36134356,0.454839,0.54353404,0.6244662,0.696098,0.75799847,0.8104976,0.8543823,0.89066416,0.9204172,0.944678,0.9643902,0.98037994,0.9933516,1.0038935,1.0124904,1.0195369,1.0253519,1.0301911,1.0342587,1.0377175,1.0406961,1.0432967,1.0455998,1.0476688,1.0495539,1.0512943,1.0529213,1.0544591,1.0559268,1.0573395,1.0587091,1.0600449,1.061354,1.0626422,1.0639141,1.065173,1.066422,1.067663,1.0688977,1.0701276,1.0713537,1.0725768,1.0737975,1.0750164],[1.0776082,1.0763893,1.0751686,1.0739455,1.0727195,1.0714896,1.0702548,1.0690138,1.0677649,1.0665059,1.0652341,1.0639459,1.0626367,1.061301,1.0599314,1.0585186,1.057051,1.0555131,1.0538862,1.0521457,1.0502607,1.0481917,1.0458885,1.043288,1.0403093,1.0368506,1.0327829,1.0279437,1.0221287,1.0150821,1.0064852,0.99594337,0.9829717,0.966982,0.9472698,0.923009,0.89325595,0.85697407,0.8130894,0.76059026,0.6986898,0.62705797,0.5461258,0.4574308,0.36393538,0.27020139,0.18227413,0.10714804,0.05179368,0.021902993,0.021902993,0.05179368,0.10714804,0.18227413,0.27020139,0.36393538,0.4574308,0.5461258,0.62705797,0.6986898,0.76059026,0.8130894,0.85697407,0.89325595,0.923009,0.9472698,0.966982,0.9829717,0.99594337,1.0064852,1.0150821,1.0221287,1.0279437,1.0327829,1.0368506,1.0403093,1.043288,1.0458885,1.0481917,1.0502607,1.0521457,1.0538862,1.0555131,1.057051,1.0585186,1.0599314,1.061301,1.0626367,1.0639459,1.0652341,1.0665059,1.0677649,1.0690138,1.0702548,1.0714896,1.0727195,1.0739455,1.0751686,1.0763893,1.0776082],[1.0806373,1.0794184,1.0781977,1.0769746,1.0757486,1.0745187,1.0732839,1.072043,1.070794,1.069535,1.0682632,1.066975,1.0656658,1.0643301,1.0629605,1.0615478,1.06008,1.0585423,1.0569153,1.0551748,1.0532898,1.0512208,1.0489177,1.0463171,1.0433384,1.0398797,1.035812,1.0309728,1.0251578,1.0181112,1.0095143,0.9989724,0.9860008,0.97001106,0.95029885,0.926038,0.896285,0.8600031,0.8161184,0.7636193,0.70171887,0.630087,0.5491549,0.46045986,0.36696443,0.27323043,0.18530318,0.11017709,0.054822735,0.02493205,0.02493205,0.054822735,0.11017709,0.18530318,0.27323043,0.36696443,0.46045986,0.5491549,0.630087,0.70171887,0.7636193,0.8161184,0.8600031,0.896285,0.926038,0.95029885,0.97001106,0.9860008,0.9989724,1.0095143,1.0181112,1.0251578,1.0309728,1.035812,1.0398797,1.0433384,1.0463171,1.0489177,1.0512208,1.0532898,1.0551748,1.0569153,1.0585423,1.06008,1.0615478,1.0629605,1.0643301,1.0656658,1.066975,1.0682632,1.069535,1.070794,1.072043,1.0732839,1.0745187,1.0757486,1.0769746,1.0781977,1.0794184,1.0806373],[1.0841738,1.0829549,1.0817342,1.0805111,1.079285,1.0780551,1.0768204,1.0755794,1.0743304,1.0730715,1.0717996,1.0705115,1.0692023,1.0678666,1.066497,1.0650842,1.0636165,1.0620787,1.0604517,1.0587113,1.0568262,1.0547572,1.0524541,1.0498536,1.0468749,1.0434161,1.0393485,1.0345093,1.0286943,1.0216478,1.0130509,1.002509,0.98953736,0.97354764,0.9538354,0.9295746,0.8998216,0.8635397,0.819655,0.7671559,0.70525545,0.6336236,0.55269146,0.46399644,0.370501,0.27676702,0.18883976,0.113713674,0.05835931,0.028468627,0.028468627,0.05835931,0.113713674,0.18883976,0.27676702,0.370501,0.46399644,0.55269146,0.6336236,0.70525545,0.7671559,0.819655,0.8635397,0.8998216,0.9295746,0.9538354,0.97354764,0.98953736,1.002509,1.0130509,1.0216478,1.0286943,1.0345093,1.0393485,1.0434161,1.0468749,1.0498536,1.0524541,1.0547572,1.0568262,1.0587113,1.0604517,1.0620787,1.0636165,1.0650842,1.066497,1.0678666,1.0692023,1.0705115,1.0717996,1.0730715,1.0743304,1.0755794,1.0768204,1.0780551,1.079285,1.0805111,1.0817342,1.0829549,1.0841738],[1.0882983,1.0870794,1.0858587,1.0846356,1.0834095,1.0821797,1.0809449,1.0797039,1.078455,1.077196,1.0759242,1.074636,1.0733268,1.0719911,1.0706215,1.0692087,1.067741,1.0662032,1.0645763,1.0628358,1.0609508,1.0588818,1.0565786,1.0539781,1.0509994,1.0475407,1.043473,1.0386338,1.0328188,1.0257722,1.0171753,1.0066334,0.99366176,0.97767204,0.95795983,0.933699,0.903946,0.8676641,0.8237794,0.7712803,0.70937985,0.637748,0.55681586,0.46812084,0.3746254,0.28089142,0.19296417,0.117838085,0.06248372,0.032593034,0.032593034,0.06248372,0.117838085,0.19296417,0.28089142,0.3746254,0.46812084,0.55681586,0.637748,0.70937985,0.7712803,0.8237794,0.8676641,0.903946,0.933699,0.95795983,0.97767204,0.99366176,1.0066334,1.0171753,1.0257722,1.0328188,1.0386338,1.043473,1.0475407,1.0509994,1.0539781,1.0565786,1.0588818,1.0609508,1.0628358,1.0645763,1.0662032,1.067741,1.0692087,1.0706215,1.0719911,1.0733268,1.074636,1.0759242,1.077196,1.078455,1.0797039,1.0809449,1.0821797,1.0834095,1.0846356,1.0858587,1.0870794,1.0882983],[1.0931017,1.0918828,1.0906621,1.089439,1.088213,1.0869831,1.0857483,1.0845073,1.0832584,1.0819994,1.0807276,1.0794394,1.0781302,1.0767945,1.0754249,1.0740122,1.0725445,1.0710067,1.0693797,1.0676392,1.0657542,1.0636852,1.061382,1.0587815,1.0558028,1.0523441,1.0482764,1.0434372,1.0376222,1.0305758,1.0219789,1.0114369,0.9984653,0.9824756,0.96276337,0.93850255,0.9087495,0.87246764,0.82858294,0.7760838,0.7141834,0.64255154,0.5616194,0.47292435,0.37942892,0.28569493,0.19776769,0.12264159,0.06728724,0.03739655,0.03739655,0.06728724,0.12264159,0.19776769,0.28569493,0.37942892,0.47292435,0.5616194,0.64255154,0.7141834,0.7760838,0.82858294,0.87246764,0.9087495,0.93850255,0.96276337,0.9824756,0.9984653,1.0114369,1.0219789,1.0305758,1.0376222,1.0434372,1.0482764,1.0523441,1.0558028,1.0587815,1.061382,1.0636852,1.0657542,1.0676392,1.0693797,1.0710067,1.0725445,1.0740122,1.0754249,1.0767945,1.0781302,1.0794394,1.0807276,1.0819994,1.0832584,1.0845073,1.0857483,1.0869831,1.088213,1.089439,1.0906621,1.0918828,1.0931017],[1.0986875,1.0974686,1.0962479,1.0950248,1.0937988,1.0925689,1.0913341,1.0900931,1.0888442,1.0875852,1.0863134,1.0850252,1.083716,1.0823803,1.0810107,1.079598,1.0781302,1.0765924,1.0749655,1.073225,1.07134,1.069271,1.0669678,1.0643673,1.0613886,1.0579299,1.0538622,1.049023,1.043208,1.0361614,1.0275645,1.0170226,1.004051,0.9880613,0.9683491,0.9440883,0.91433525,0.87805337,0.8341687,0.78166956,0.7197691,0.6481373,0.56720513,0.47851008,0.38501465,0.29128066,0.2033534,0.12822732,0.07287296,0.042982273,0.042982273,0.07287296,0.12822732,0.2033534,0.29128066,0.38501465,0.47851008,0.56720513,0.6481373,0.7197691,0.78166956,0.8341687,0.87805337,0.91433525,0.9440883,0.9683491,0.9880613,1.004051,1.0170226,1.0275645,1.0361614,1.043208,1.049023,1.0538622,1.0579299,1.0613886,1.0643673,1.0669678,1.069271,1.07134,1.073225,1.0749655,1.0765924,1.0781302,1.079598,1.0810107,1.0823803,1.083716,1.0850252,1.0863134,1.0875852,1.0888442,1.0900931,1.0913341,1.0925689,1.0937988,1.0950248,1.0962479,1.0974686,1.0986875],[1.1051711,1.1039522,1.1027315,1.1015084,1.1002823,1.0990524,1.0978177,1.0965767,1.0953277,1.0940688,1.0927969,1.0915087,1.0901996,1.0888638,1.0874943,1.0860815,1.0846138,1.083076,1.081449,1.0797086,1.0778235,1.0757545,1.0734514,1.0708508,1.0678722,1.0644134,1.0603458,1.0555066,1.0496916,1.042645,1.0340481,1.0235062,1.0105345,0.99454486,0.97483265,0.9505718,0.9208188,0.88453686,0.8406522,0.78815305,0.7262527,0.65462077,0.5736886,0.48499364,0.3914982,0.2977642,0.20983696,0.13471088,0.07935651,0.049465824,0.049465824,0.07935651,0.13471088,0.20983696,0.2977642,0.3914982,0.48499364,0.5736886,0.65462077,0.7262527,0.78815305,0.8406522,0.88453686,0.9208188,0.9505718,0.97483265,0.99454486,1.0105345,1.0235062,1.0340481,1.042645,1.0496916,1.0555066,1.0603458,1.0644134,1.0678722,1.0708508,1.0734514,1.0757545,1.0778235,1.0797086,1.081449,1.083076,1.0846138,1.0860815,1.0874943,1.0888638,1.0901996,1.0915087,1.0927969,1.0940688,1.0953277,1.0965767,1.0978177,1.0990524,1.1002823,1.1015084,1.1027315,1.1039522,1.1051711],[1.1126809,1.111462,1.1102413,1.1090182,1.1077921,1.1065623,1.1053275,1.1040865,1.1028376,1.1015786,1.1003067,1.0990186,1.0977094,1.0963737,1.0950041,1.0935913,1.0921236,1.0905858,1.0889589,1.0872184,1.0853333,1.0832644,1.0809612,1.0783607,1.075382,1.0719233,1.0678556,1.0630164,1.0572014,1.0501549,1.041558,1.0310161,1.0180445,1.0020547,0.98234254,0.9580817,0.9283287,0.8920468,0.8481621,0.795663,0.73376256,0.6621307,0.5811986,0.49250352,0.3990081,0.3052741,0.21734686,0.14222077,0.08686641,0.056975722,0.056975722,0.08686641,0.14222077,0.21734686,0.3052741,0.3990081,0.49250352,0.5811986,0.6621307,0.73376256,0.795663,0.8481621,0.8920468,0.9283287,0.9580817,0.98234254,1.0020547,1.0180445,1.0310161,1.041558,1.0501549,1.0572014,1.0630164,1.0678556,1.0719233,1.075382,1.0783607,1.0809612,1.0832644,1.0853333,1.0872184,1.0889589,1.0905858,1.0921236,1.0935913,1.0950041,1.0963737,1.0977094,1.0990186,1.1003067,1.1015786,1.1028376,1.1040865,1.1053275,1.1065623,1.1077921,1.1090182,1.1102413,1.111462,1.1126809],[1.1213585,1.1201396,1.1189189,1.1176958,1.1164697,1.1152399,1.1140051,1.1127641,1.1115152,1.1102562,1.1089844,1.1076962,1.106387,1.1050513,1.1036817,1.1022689,1.1008012,1.0992634,1.0976365,1.095896,1.094011,1.091942,1.0896388,1.0870383,1.0840596,1.0806009,1.0765332,1.071694,1.065879,1.0588324,1.0502355,1.0396936,1.026722,1.0107323,0.9910201,0.96675926,0.93700624,0.90072435,0.85683966,0.80434054,0.7424401,0.67080826,0.5898761,0.50118107,0.40768564,0.31395164,0.2260244,0.15089831,0.09554395,0.065653265,0.065653265,0.09554395,0.15089831,0.2260244,0.31395164,0.40768564,0.50118107,0.5898761,0.67080826,0.7424401,0.80434054,0.85683966,0.90072435,0.93700624,0.96675926,0.9910201,1.0107323,1.026722,1.0396936,1.0502355,1.0588324,1.065879,1.071694,1.0765332,1.0806009,1.0840596,1.0870383,1.0896388,1.091942,1.094011,1.095896,1.0976365,1.0992634,1.1008012,1.1022689,1.1036817,1.1050513,1.106387,1.1076962,1.1089844,1.1102562,1.1115152,1.1127641,1.1140051,1.1152399,1.1164697,1.1176958,1.1189189,1.1201396,1.1213585],[1.1313571,1.1301382,1.1289175,1.1276944,1.1264683,1.1252384,1.1240036,1.1227627,1.1215137,1.1202548,1.1189829,1.1176947,1.1163856,1.1150498,1.1136802,1.1122675,1.1107998,1.109262,1.107635,1.1058946,1.1040095,1.1019405,1.0996374,1.0970368,1.0940582,1.0905994,1.0865318,1.0816926,1.0758775,1.068831,1.0602341,1.0496922,1.0367205,1.0207309,1.0010186,0.9767578,0.9470048,0.9107229,0.8668382,0.8143391,0.75243866,0.6808068,0.5998747,0.5111796,0.4176842,0.32395023,0.23602296,0.16089687,0.10554251,0.07565183,0.07565183,0.10554251,0.16089687,0.23602296,0.32395023,0.4176842,0.5111796,0.5998747,0.6808068,0.75243866,0.8143391,0.8668382,0.9107229,0.9470048,0.9767578,1.0010186,1.0207309,1.0367205,1.0496922,1.0602341,1.068831,1.0758775,1.0816926,1.0865318,1.0905994,1.0940582,1.0970368,1.0996374,1.1019405,1.1040095,1.1058946,1.107635,1.109262,1.1107998,1.1122675,1.1136802,1.1150498,1.1163856,1.1176947,1.1189829,1.1202548,1.1215137,1.1227627,1.1240036,1.1252384,1.1264683,1.1276944,1.1289175,1.1301382,1.1313571],[1.1428405,1.1416216,1.1404009,1.1391778,1.1379517,1.1367218,1.1354871,1.1342461,1.1329972,1.1317382,1.1304663,1.1291782,1.127869,1.1265333,1.1251637,1.1237509,1.1222832,1.1207454,1.1191185,1.117378,1.1154929,1.113424,1.1111208,1.1085203,1.1055416,1.1020828,1.0980152,1.093176,1.087361,1.0803144,1.0717175,1.0611756,1.048204,1.0322143,1.0125021,0.98824126,0.9584882,0.92220634,0.87832165,0.82582253,0.7639221,0.69229025,0.6113581,0.5226631,0.42916766,0.33543366,0.24750641,0.17238033,0.11702596,0.08713528,0.08713528,0.11702596,0.17238033,0.24750641,0.33543366,0.42916766,0.5226631,0.6113581,0.69229025,0.7639221,0.82582253,0.87832165,0.92220634,0.9584882,0.98824126,1.0125021,1.0322143,1.048204,1.0611756,1.0717175,1.0803144,1.087361,1.093176,1.0980152,1.1020828,1.1055416,1.1085203,1.1111208,1.113424,1.1154929,1.117378,1.1191185,1.1207454,1.1222832,1.1237509,1.1251637,1.1265333,1.127869,1.1291782,1.1304663,1.1317382,1.1329972,1.1342461,1.1354871,1.1367218,1.1379517,1.1391778,1.1404009,1.1416216,1.1428405],[1.1559803,1.1547614,1.1535407,1.1523176,1.1510916,1.1498617,1.1486269,1.147386,1.146137,1.144878,1.1436062,1.142318,1.1410089,1.1396731,1.1383035,1.1368908,1.1354231,1.1338853,1.1322583,1.1305178,1.1286328,1.1265638,1.1242607,1.1216601,1.1186814,1.1152227,1.111155,1.1063159,1.1005008,1.0934542,1.0848573,1.0743154,1.0613438,1.0453541,1.0256419,1.001381,0.97162807,0.9353461,0.8914615,0.8389623,0.77706194,0.70543003,0.6244979,0.5358029,0.44230747,0.34857348,0.26064622,0.18552014,0.13016577,0.10027509,0.10027509,0.13016577,0.18552014,0.26064622,0.34857348,0.44230747,0.5358029,0.6244979,0.70543003,0.77706194,0.8389623,0.8914615,0.9353461,0.97162807,1.001381,1.0256419,1.0453541,1.0613438,1.0743154,1.0848573,1.0934542,1.1005008,1.1063159,1.111155,1.1152227,1.1186814,1.1216601,1.1242607,1.1265638,1.1286328,1.1305178,1.1322583,1.1338853,1.1354231,1.1368908,1.1383035,1.1396731,1.1410089,1.142318,1.1436062,1.144878,1.146137,1.147386,1.1486269,1.1498617,1.1510916,1.1523176,1.1535407,1.1547614,1.1559803],[1.1709516,1.1697327,1.168512,1.1672889,1.1660628,1.164833,1.1635982,1.1623572,1.1611083,1.1598493,1.1585774,1.1572893,1.1559801,1.1546444,1.1532748,1.151862,1.1503943,1.1488565,1.1472296,1.1454891,1.143604,1.141535,1.1392319,1.1366314,1.1336527,1.130194,1.1261263,1.1212871,1.1154721,1.1084255,1.0998286,1.0892867,1.076315,1.0603254,1.0406132,1.0163523,0.9865993,0.95031744,0.90643275,0.85393363,0.7920332,0.72040135,0.6394692,0.55077416,0.45727873,0.36354476,0.27561748,0.2004914,0.14513704,0.11524636,0.11524636,0.14513704,0.2004914,0.27561748,0.36354476,0.45727873,0.55077416,0.6394692,0.72040135,0.7920332,0.85393363,0.90643275,0.95031744,0.9865993,1.0163523,1.0406132,1.0603254,1.076315,1.0892867,1.0998286,1.1084255,1.1154721,1.1212871,1.1261263,1.130194,1.1336527,1.1366314,1.1392319,1.141535,1.143604,1.1454891,1.1472296,1.1488565,1.1503943,1.151862,1.1532748,1.1546444,1.1559801,1.1572893,1.1585774,1.1598493,1.1611083,1.1623572,1.1635982,1.164833,1.1660628,1.1672889,1.168512,1.1697327,1.1709516],[1.1879271,1.1867082,1.1854875,1.1842644,1.1830384,1.1818085,1.1805737,1.1793327,1.1780838,1.1768248,1.175553,1.1742648,1.1729556,1.1716199,1.1702503,1.1688375,1.1673698,1.165832,1.1642051,1.1624646,1.1605796,1.1585106,1.1562074,1.1536069,1.1506282,1.1471695,1.1431018,1.1382626,1.1324476,1.125401,1.1168041,1.1062622,1.0932906,1.0773009,1.0575887,1.0333278,1.0035748,0.9672929,0.92340827,0.8709091,0.8090087,0.7373768,0.65644467,0.5677497,0.47425425,0.38052025,0.292593,0.21746692,0.16211255,0.13222186,0.13222186,0.16211255,0.21746692,0.292593,0.38052025,0.47425425,0.5677497,0.65644467,0.7373768,0.8090087,0.8709091,0.92340827,0.9672929,1.0035748,1.0333278,1.0575887,1.0773009,1.0932906,1.1062622,1.1168041,1.125401,1.1324476,1.1382626,1.1431018,1.1471695,1.1506282,1.1536069,1.1562074,1.1585106,1.1605796,1.1624646,1.1642051,1.165832,1.1673698,1.1688375,1.1702503,1.1716199,1.1729556,1.1742648,1.175553,1.1768248,1.1780838,1.1793327,1.1805737,1.1818085,1.1830384,1.1842644,1.1854875,1.1867082,1.1879271],[1.2070696,1.2058507,1.20463,1.2034069,1.2021809,1.200951,1.1997162,1.1984752,1.1972263,1.1959673,1.1946955,1.1934073,1.1920981,1.1907624,1.1893928,1.18798,1.1865124,1.1849746,1.1833476,1.1816071,1.1797221,1.1776531,1.17535,1.1727494,1.1697707,1.166312,1.1622443,1.1574051,1.1515901,1.1445435,1.1359466,1.1254047,1.1124331,1.0964434,1.0767312,1.0524703,1.0227174,0.9864354,0.9425508,0.8900516,0.8281512,0.7565193,0.6755872,0.5868922,0.49339676,0.39966276,0.3117355,0.23660943,0.18125506,0.15136437,0.15136437,0.18125506,0.23660943,0.3117355,0.39966276,0.49339676,0.5868922,0.6755872,0.7565193,0.8281512,0.8900516,0.9425508,0.9864354,1.0227174,1.0524703,1.0767312,1.0964434,1.1124331,1.1254047,1.1359466,1.1445435,1.1515901,1.1574051,1.1622443,1.166312,1.1697707,1.1727494,1.17535,1.1776531,1.1797221,1.1816071,1.1833476,1.1849746,1.1865124,1.18798,1.1893928,1.1907624,1.1920981,1.1934073,1.1946955,1.1959673,1.1972263,1.1984752,1.1997162,1.200951,1.2021809,1.2034069,1.20463,1.2058507,1.2070696],[1.2285224,1.2273035,1.2260828,1.2248597,1.2236336,1.2224038,1.221169,1.219928,1.2186791,1.2174201,1.2161483,1.2148601,1.2135509,1.2122152,1.2108456,1.2094328,1.2079651,1.2064273,1.2048004,1.2030599,1.2011749,1.1991059,1.1968027,1.1942022,1.1912235,1.1877648,1.1836971,1.1788579,1.1730429,1.1659964,1.1573995,1.1468576,1.133886,1.1178962,1.098184,1.0739232,1.0441701,1.0078883,0.9640036,0.9115045,0.84960407,0.7779722,0.6970401,0.60834503,0.5148496,0.42111564,0.33318835,0.25806227,0.20270792,0.17281723,0.17281723,0.20270792,0.25806227,0.33318835,0.42111564,0.5148496,0.60834503,0.6970401,0.7779722,0.84960407,0.9115045,0.9640036,1.0078883,1.0441701,1.0739232,1.098184,1.1178962,1.133886,1.1468576,1.1573995,1.1659964,1.1730429,1.1788579,1.1836971,1.1877648,1.1912235,1.1942022,1.1968027,1.1991059,1.2011749,1.2030599,1.2048004,1.2064273,1.2079651,1.2094328,1.2108456,1.2122152,1.2135509,1.2148601,1.2161483,1.2174201,1.2186791,1.219928,1.221169,1.2224038,1.2236336,1.2248597,1.2260828,1.2273035,1.2285224],[1.2523983,1.2511793,1.2499586,1.2487355,1.2475095,1.2462796,1.2450448,1.2438039,1.2425549,1.2412959,1.2400241,1.2387359,1.2374268,1.236091,1.2347214,1.2333087,1.231841,1.2303032,1.2286762,1.2269357,1.2250507,1.2229817,1.2206786,1.218078,1.2150993,1.2116406,1.2075729,1.2027338,1.1969187,1.1898721,1.1812752,1.1707333,1.1577617,1.141772,1.1220598,1.097799,1.068046,1.031764,0.9878794,0.9353802,0.87347984,0.80184793,0.7209158,0.6322208,0.5387254,0.44499138,0.35706413,0.28193805,0.22658367,0.19669299,0.19669299,0.22658367,0.28193805,0.35706413,0.44499138,0.5387254,0.6322208,0.7209158,0.80184793,0.87347984,0.9353802,0.9878794,1.031764,1.068046,1.097799,1.1220598,1.141772,1.1577617,1.1707333,1.1812752,1.1898721,1.1969187,1.2027338,1.2075729,1.2116406,1.2150993,1.218078,1.2206786,1.2229817,1.2250507,1.2269357,1.2286762,1.2303032,1.231841,1.2333087,1.2347214,1.236091,1.2374268,1.2387359,1.2400241,1.2412959,1.2425549,1.2438039,1.2450448,1.2462796,1.2475095,1.2487355,1.2499586,1.2511793,1.2523983],[1.2787662,1.2775472,1.2763265,1.2751034,1.2738774,1.2726475,1.2714127,1.2701718,1.2689228,1.2676638,1.266392,1.2651038,1.2637947,1.2624589,1.2610893,1.2596766,1.2582089,1.2566711,1.2550441,1.2533036,1.2514186,1.2493496,1.2470465,1.2444459,1.2414672,1.2380085,1.2339408,1.2291017,1.2232866,1.21624,1.2076432,1.1971012,1.1841296,1.1681399,1.1484277,1.1241668,1.0944139,1.0581319,1.0142473,0.9617482,0.89984775,0.8282159,0.74728376,0.6585887,0.5650933,0.4713593,0.38343203,0.30830595,0.2529516,0.2230609,0.2230609,0.2529516,0.30830595,0.38343203,0.4713593,0.5650933,0.6585887,0.74728376,0.8282159,0.89984775,0.9617482,1.0142473,1.0581319,1.0944139,1.1241668,1.1484277,1.1681399,1.1841296,1.1971012,1.2076432,1.21624,1.2232866,1.2291017,1.2339408,1.2380085,1.2414672,1.2444459,1.2470465,1.2493496,1.2514186,1.2533036,1.2550441,1.2566711,1.2582089,1.2596766,1.2610893,1.2624589,1.2637947,1.2651038,1.266392,1.2676638,1.2689228,1.2701718,1.2714127,1.2726475,1.2738774,1.2751034,1.2763265,1.2775472,1.2787662],[1.3076391,1.3064202,1.3051995,1.3039764,1.3027503,1.3015205,1.3002857,1.2990447,1.2977958,1.2965368,1.295265,1.2939768,1.2926676,1.2913319,1.2899623,1.2885495,1.2870818,1.285544,1.2839171,1.2821766,1.2802916,1.2782226,1.2759194,1.2733189,1.2703402,1.2668815,1.2628138,1.2579746,1.2521596,1.2451131,1.2365162,1.2259743,1.2130027,1.1970129,1.1773007,1.1530399,1.1232868,1.087005,1.0431203,0.9906212,0.9287208,0.8570889,0.7761568,0.68746173,0.5939663,0.50023234,0.41230506,0.33717898,0.28182462,0.25193393,0.25193393,0.28182462,0.33717898,0.41230506,0.50023234,0.5939663,0.68746173,0.7761568,0.8570889,0.9287208,0.9906212,1.0431203,1.087005,1.1232868,1.1530399,1.1773007,1.1970129,1.2130027,1.2259743,1.2365162,1.2451131,1.2521596,1.2579746,1.2628138,1.2668815,1.2703402,1.2733189,1.2759194,1.2782226,1.2802916,1.2821766,1.2839171,1.285544,1.2870818,1.2885495,1.2899623,1.2913319,1.2926676,1.2939768,1.295265,1.2965368,1.2977958,1.2990447,1.3002857,1.3015205,1.3027503,1.3039764,1.3051995,1.3064202,1.3076391],[1.3389618,1.3377429,1.3365222,1.3352991,1.3340731,1.3328432,1.3316084,1.3303674,1.3291185,1.3278595,1.3265877,1.3252995,1.3239903,1.3226546,1.321285,1.3198723,1.3184046,1.3168668,1.3152398,1.3134993,1.3116143,1.3095453,1.3072422,1.3046416,1.3016629,1.2982042,1.2941365,1.2892973,1.2834823,1.2764357,1.2678388,1.2572969,1.2443253,1.2283356,1.2086234,1.1843625,1.1546096,1.1183276,1.074443,1.0219438,0.96004343,0.8884115,0.8074794,0.7187844,0.62528896,0.53155494,0.44362772,0.36850163,0.31314728,0.2832566,0.2832566,0.31314728,0.36850163,0.44362772,0.53155494,0.62528896,0.7187844,0.8074794,0.8884115,0.96004343,1.0219438,1.074443,1.1183276,1.1546096,1.1843625,1.2086234,1.2283356,1.2443253,1.2572969,1.2678388,1.2764357,1.2834823,1.2892973,1.2941365,1.2982042,1.3016629,1.3046416,1.3072422,1.3095453,1.3116143,1.3134993,1.3152398,1.3168668,1.3184046,1.3198723,1.321285,1.3226546,1.3239903,1.3252995,1.3265877,1.3278595,1.3291185,1.3303674,1.3316084,1.3328432,1.3340731,1.3352991,1.3365222,1.3377429,1.3389618],[1.3725997,1.3713808,1.3701601,1.368937,1.367711,1.3664811,1.3652463,1.3640053,1.3627564,1.3614974,1.3602256,1.3589374,1.3576282,1.3562925,1.3549229,1.3535101,1.3520424,1.3505046,1.3488777,1.3471372,1.3452522,1.3431832,1.34088,1.3382795,1.3353008,1.3318421,1.3277744,1.3229352,1.3171202,1.3100736,1.3014767,1.2909348,1.2779632,1.2619735,1.2422613,1.2180004,1.1882474,1.1519656,1.1080809,1.0555818,0.9936813,0.92204946,0.8411173,0.75242233,0.65892684,0.5651929,0.47726563,0.40213954,0.3467852,0.3168945,0.3168945,0.3467852,0.40213954,0.47726563,0.5651929,0.65892684,0.75242233,0.8411173,0.92204946,0.9936813,1.0555818,1.1080809,1.1519656,1.1882474,1.2180004,1.2422613,1.2619735,1.2779632,1.2909348,1.3014767,1.3100736,1.3171202,1.3229352,1.3277744,1.3318421,1.3353008,1.3382795,1.34088,1.3431832,1.3452522,1.3471372,1.3488777,1.3505046,1.3520424,1.3535101,1.3549229,1.3562925,1.3576282,1.3589374,1.3602256,1.3614974,1.3627564,1.3640053,1.3652463,1.3664811,1.367711,1.368937,1.3701601,1.3713808,1.3725997],[1.4083338,1.407115,1.4058943,1.4046712,1.403445,1.4022152,1.4009805,1.3997395,1.3984904,1.3972316,1.3959596,1.3946714,1.3933623,1.3920267,1.390657,1.3892443,1.3877766,1.3862388,1.3846118,1.3828714,1.3809862,1.3789172,1.3766141,1.3740137,1.3710349,1.3675761,1.3635085,1.3586693,1.3528543,1.3458078,1.3372109,1.326669,1.3136973,1.2977076,1.2779953,1.2537346,1.2239816,1.1876997,1.143815,1.0913159,1.0294154,0.9577836,0.87685144,0.7881564,0.694661,0.600927,0.5129998,0.43787366,0.3825193,0.35262862,0.35262862,0.3825193,0.43787366,0.5129998,0.600927,0.694661,0.7881564,0.87685144,0.9577836,1.0294154,1.0913159,1.143815,1.1876997,1.2239816,1.2537346,1.2779953,1.2977076,1.3136973,1.326669,1.3372109,1.3458078,1.3528543,1.3586693,1.3635085,1.3675761,1.3710349,1.3740137,1.3766141,1.3789172,1.3809862,1.3828714,1.3846118,1.3862388,1.3877766,1.3892443,1.390657,1.3920267,1.3933623,1.3946714,1.3959596,1.3972316,1.3984904,1.3997395,1.4009805,1.4022152,1.403445,1.4046712,1.4058943,1.407115,1.4083338],[1.4458596,1.4446406,1.4434199,1.4421968,1.4409708,1.4397409,1.4385061,1.4372652,1.4360162,1.4347572,1.4334854,1.4321972,1.430888,1.4295523,1.4281827,1.42677,1.4253023,1.4237645,1.4221375,1.420397,1.418512,1.416443,1.4141399,1.4115393,1.4085606,1.4051019,1.4010342,1.396195,1.39038,1.3833334,1.3747365,1.3641946,1.351223,1.3352333,1.3155211,1.2912602,1.2615073,1.2252253,1.1813407,1.1288415,1.0669411,0.99530923,0.9143771,0.8256821,0.7321867,0.63845265,0.5505254,0.47539935,0.420045,0.3901543,0.3901543,0.420045,0.47539935,0.5505254,0.63845265,0.7321867,0.8256821,0.9143771,0.99530923,1.0669411,1.1288415,1.1813407,1.2252253,1.2615073,1.2912602,1.3155211,1.3352333,1.351223,1.3641946,1.3747365,1.3833334,1.39038,1.396195,1.4010342,1.4051019,1.4085606,1.4115393,1.4141399,1.416443,1.418512,1.420397,1.4221375,1.4237645,1.4253023,1.42677,1.4281827,1.4295523,1.430888,1.4321972,1.4334854,1.4347572,1.4360162,1.4372652,1.4385061,1.4397409,1.4409708,1.4421968,1.4434199,1.4446406,1.4458596],[1.4847922,1.4835733,1.4823526,1.4811295,1.4799035,1.4786736,1.4774388,1.4761978,1.4749489,1.4736899,1.4724181,1.4711299,1.4698207,1.468485,1.4671154,1.4657027,1.464235,1.4626971,1.4610702,1.4593297,1.4574447,1.4553757,1.4530725,1.450472,1.4474933,1.4440346,1.4399669,1.4351277,1.4293127,1.4222662,1.4136693,1.4031274,1.3901558,1.374166,1.3544538,1.330193,1.30044,1.2641581,1.2202734,1.1677743,1.1058738,1.034242,0.9533099,0.86461484,0.7711194,0.67738545,0.58945817,0.51433206,0.45897773,0.42908704,0.42908704,0.45897773,0.51433206,0.58945817,0.67738545,0.7711194,0.86461484,0.9533099,1.034242,1.1058738,1.1677743,1.2202734,1.2641581,1.30044,1.330193,1.3544538,1.374166,1.3901558,1.4031274,1.4136693,1.4222662,1.4293127,1.4351277,1.4399669,1.4440346,1.4474933,1.450472,1.4530725,1.4553757,1.4574447,1.4593297,1.4610702,1.4626971,1.464235,1.4657027,1.4671154,1.468485,1.4698207,1.4711299,1.4724181,1.4736899,1.4749489,1.4761978,1.4774388,1.4786736,1.4799035,1.4811295,1.4823526,1.4835733,1.4847922],[1.5246806,1.5234617,1.522241,1.5210179,1.5197918,1.518562,1.5173272,1.5160862,1.5148373,1.5135783,1.5123065,1.5110183,1.5097091,1.5083734,1.5070038,1.505591,1.5041233,1.5025855,1.5009586,1.4992181,1.497333,1.495264,1.4929609,1.4903604,1.4873817,1.483923,1.4798553,1.4750161,1.4692011,1.4621546,1.4535577,1.4430158,1.4300442,1.4140544,1.3943422,1.3700814,1.3403283,1.3040464,1.2601618,1.2076626,1.1457622,1.0741303,0.9931982,0.9045032,0.81100774,0.7172738,0.6293465,0.55422044,0.49886608,0.4689754,0.4689754,0.49886608,0.55422044,0.6293465,0.7172738,0.81100774,0.9045032,0.9931982,1.0741303,1.1457622,1.2076626,1.2601618,1.3040464,1.3403283,1.3700814,1.3943422,1.4140544,1.4300442,1.4430158,1.4535577,1.4621546,1.4692011,1.4750161,1.4798553,1.483923,1.4873817,1.4903604,1.4929609,1.495264,1.497333,1.4992181,1.5009586,1.5025855,1.5041233,1.505591,1.5070038,1.5083734,1.5097091,1.5110183,1.5123065,1.5135783,1.5148373,1.5160862,1.5173272,1.518562,1.5197918,1.5210179,1.522241,1.5234617,1.5246806],[1.5650257,1.5638068,1.5625861,1.561363,1.5601369,1.558907,1.5576723,1.5564313,1.5551823,1.5539234,1.5526515,1.5513633,1.5500542,1.5487185,1.5473489,1.5459361,1.5444684,1.5429306,1.5413036,1.5395632,1.5376781,1.5356091,1.533306,1.5307055,1.5277268,1.524268,1.5202004,1.5153612,1.5095462,1.5024996,1.4939027,1.4833608,1.4703891,1.4543995,1.4346873,1.4104264,1.3806734,1.3443916,1.3005068,1.2480078,1.1861073,1.1144755,1.0335433,0.9448483,0.8513528,0.75761884,0.66969156,0.5945655,0.53921115,0.50932044,0.50932044,0.53921115,0.5945655,0.66969156,0.75761884,0.8513528,0.9448483,1.0335433,1.1144755,1.1861073,1.2480078,1.3005068,1.3443916,1.3806734,1.4104264,1.4346873,1.4543995,1.4703891,1.4833608,1.4939027,1.5024996,1.5095462,1.5153612,1.5202004,1.524268,1.5277268,1.5307055,1.533306,1.5356091,1.5376781,1.5395632,1.5413036,1.5429306,1.5444684,1.5459361,1.5473489,1.5487185,1.5500542,1.5513633,1.5526515,1.5539234,1.5551823,1.5564313,1.5576723,1.558907,1.5601369,1.561363,1.5625861,1.5638068,1.5650257],[1.6053052,1.6040862,1.6028655,1.6016424,1.6004164,1.5991864,1.5979517,1.5967107,1.5954618,1.5942028,1.592931,1.5916429,1.5903337,1.5889978,1.5876284,1.5862155,1.5847478,1.58321,1.581583,1.5798426,1.5779576,1.5758886,1.5735855,1.5709848,1.5680063,1.5645475,1.5604799,1.5556407,1.5498257,1.5427791,1.5341822,1.5236403,1.5106686,1.494679,1.4749668,1.4507059,1.4209528,1.384671,1.3407862,1.2882872,1.2263868,1.1547549,1.0738227,0.9851277,0.8916323,0.7978983,0.7099711,0.63484496,0.5794906,0.5495999,0.5495999,0.5794906,0.63484496,0.7099711,0.7978983,0.8916323,0.9851277,1.0738227,1.1547549,1.2263868,1.2882872,1.3407862,1.384671,1.4209528,1.4507059,1.4749668,1.494679,1.5106686,1.5236403,1.5341822,1.5427791,1.5498257,1.5556407,1.5604799,1.5645475,1.5680063,1.5709848,1.5735855,1.5758886,1.5779576,1.5798426,1.581583,1.58321,1.5847478,1.5862155,1.5876284,1.5889978,1.5903337,1.5916429,1.592931,1.5942028,1.5954618,1.5967107,1.5979517,1.5991864,1.6004164,1.6016424,1.6028655,1.6040862,1.6053052],[1.645,1.643781,1.6425602,1.6413372,1.6401112,1.6388812,1.6376464,1.6364055,1.6351566,1.6338975,1.6326258,1.6313376,1.6300285,1.6286926,1.6273232,1.6259103,1.6244426,1.6229048,1.6212778,1.6195374,1.6176524,1.6155834,1.6132803,1.6106796,1.6077011,1.6042423,1.6001747,1.5953355,1.5895205,1.5824739,1.573877,1.5633351,1.5503634,1.5343738,1.5146616,1.4904007,1.4606476,1.4243658,1.380481,1.327982,1.2660816,1.1944497,1.1135175,1.0248225,0.9313271,0.8375931,0.74966586,0.67453974,0.6191854,0.5892947,0.5892947,0.6191854,0.67453974,0.74966586,0.8375931,0.9313271,1.0248225,1.1135175,1.1944497,1.2660816,1.327982,1.380481,1.4243658,1.4606476,1.4904007,1.5146616,1.5343738,1.5503634,1.5633351,1.573877,1.5824739,1.5895205,1.5953355,1.6001747,1.6042423,1.6077011,1.6106796,1.6132803,1.6155834,1.6176524,1.6195374,1.6212778,1.6229048,1.6244426,1.6259103,1.6273232,1.6286926,1.6300285,1.6313376,1.6326258,1.6338975,1.6351566,1.6364055,1.6376464,1.6388812,1.6401112,1.6413372,1.6425602,1.643781,1.645],[1.6836209,1.6824019,1.6811812,1.6799581,1.6787322,1.6775022,1.6762674,1.6750264,1.6737776,1.6725185,1.6712468,1.6699586,1.6686494,1.6673136,1.6659441,1.6645312,1.6630635,1.6615257,1.6598988,1.6581583,1.6562734,1.6542044,1.6519012,1.6493006,1.646322,1.6428633,1.6387956,1.6339564,1.6281414,1.6210948,1.6124979,1.601956,1.5889844,1.5729947,1.5532825,1.5290216,1.4992685,1.4629867,1.419102,1.3666029,1.3047025,1.2330706,1.1521385,1.0634434,0.96994805,0.876214,0.7882868,0.7131607,0.65780634,0.6279156,0.6279156,0.65780634,0.7131607,0.7882868,0.876214,0.96994805,1.0634434,1.1521385,1.2330706,1.3047025,1.3666029,1.419102,1.4629867,1.4992685,1.5290216,1.5532825,1.5729947,1.5889844,1.601956,1.6124979,1.6210948,1.6281414,1.6339564,1.6387956,1.6428633,1.646322,1.6493006,1.6519012,1.6542044,1.6562734,1.6581583,1.6598988,1.6615257,1.6630635,1.6645312,1.6659441,1.6673136,1.6686494,1.6699586,1.6712468,1.6725185,1.6737776,1.6750264,1.6762674,1.6775022,1.6787322,1.6799581,1.6811812,1.6824019,1.6836209],[1.7207317,1.7195128,1.7182921,1.717069,1.715843,1.7146131,1.7133783,1.7121373,1.7108884,1.7096294,1.7083576,1.7070694,1.7057602,1.7044245,1.7030549,1.7016422,1.7001745,1.6986367,1.6970097,1.6952692,1.6933842,1.6913152,1.689012,1.6864115,1.6834328,1.6799741,1.6759064,1.6710672,1.6652522,1.6582057,1.6496089,1.6390669,1.6260953,1.6101055,1.5903933,1.5661325,1.5363795,1.5000975,1.4562129,1.4037137,1.3418133,1.2701814,1.1892493,1.1005543,1.0070589,0.9133249,0.8253976,0.75027156,0.6949172,0.6650265,0.6650265,0.6949172,0.75027156,0.8253976,0.9133249,1.0070589,1.1005543,1.1892493,1.2701814,1.3418133,1.4037137,1.4562129,1.5000975,1.5363795,1.5661325,1.5903933,1.6101055,1.6260953,1.6390669,1.6496089,1.6582057,1.6652522,1.6710672,1.6759064,1.6799741,1.6834328,1.6864115,1.689012,1.6913152,1.6933842,1.6952692,1.6970097,1.6986367,1.7001745,1.7016422,1.7030549,1.7044245,1.7057602,1.7070694,1.7083576,1.7096294,1.7108884,1.7121373,1.7133783,1.7146131,1.715843,1.717069,1.7182921,1.7195128,1.7207317],[1.7559671,1.7547482,1.7535275,1.7523044,1.7510784,1.7498485,1.7486137,1.7473727,1.7461238,1.7448648,1.743593,1.7423048,1.7409956,1.7396599,1.7382903,1.7368776,1.7354099,1.733872,1.7322451,1.7305046,1.7286196,1.7265506,1.7242475,1.7216469,1.7186682,1.7152095,1.7111418,1.7063026,1.7004876,1.6934412,1.6848443,1.6743023,1.6613307,1.6453409,1.6256287,1.601368,1.5716149,1.5353329,1.4914483,1.4389491,1.3770487,1.3054168,1.2244847,1.1357898,1.0422943,0.9485603,0.860633,0.78550696,0.7301526,0.7002619,0.7002619,0.7301526,0.78550696,0.860633,0.9485603,1.0422943,1.1357898,1.2244847,1.3054168,1.3770487,1.4389491,1.4914483,1.5353329,1.5716149,1.601368,1.6256287,1.6453409,1.6613307,1.6743023,1.6848443,1.6934412,1.7004876,1.7063026,1.7111418,1.7152095,1.7186682,1.7216469,1.7242475,1.7265506,1.7286196,1.7305046,1.7322451,1.733872,1.7354099,1.7368776,1.7382903,1.7396599,1.7409956,1.7423048,1.743593,1.7448648,1.7461238,1.7473727,1.7486137,1.7498485,1.7510784,1.7523044,1.7535275,1.7547482,1.7559671],[1.7890434,1.7878244,1.7866037,1.7853806,1.7841547,1.7829247,1.7816899,1.7804489,1.7792001,1.777941,1.7766693,1.7753811,1.7740719,1.7727361,1.7713666,1.7699537,1.768486,1.7669482,1.7653213,1.7635808,1.7616959,1.7596269,1.7573237,1.7547231,1.7517445,1.7482858,1.7442181,1.7393789,1.7335639,1.7265173,1.7179204,1.7073785,1.6944069,1.6784172,1.658705,1.6344441,1.604691,1.5684092,1.5245245,1.4720254,1.410125,1.3384931,1.257561,1.1688659,1.0753706,0.9816365,0.8937093,0.8185832,0.76322883,0.7333381,0.7333381,0.76322883,0.8185832,0.8937093,0.9816365,1.0753706,1.1688659,1.257561,1.3384931,1.410125,1.4720254,1.5245245,1.5684092,1.604691,1.6344441,1.658705,1.6784172,1.6944069,1.7073785,1.7179204,1.7265173,1.7335639,1.7393789,1.7442181,1.7482858,1.7517445,1.7547231,1.7573237,1.7596269,1.7616959,1.7635808,1.7653213,1.7669482,1.768486,1.7699537,1.7713666,1.7727361,1.7740719,1.7753811,1.7766693,1.777941,1.7792001,1.7804489,1.7816899,1.7829247,1.7841547,1.7853806,1.7866037,1.7878244,1.7890434],[1.819763,1.818544,1.8173233,1.8161002,1.8148742,1.8136443,1.8124095,1.8111686,1.8099196,1.8086606,1.8073888,1.8061006,1.8047915,1.8034557,1.8020861,1.8006734,1.7992057,1.7976679,1.7960409,1.7943004,1.7924154,1.7903464,1.7880433,1.7854427,1.782464,1.7790053,1.7749376,1.7700984,1.7642834,1.757237,1.7486401,1.7380981,1.7251265,1.7091367,1.6894245,1.6651638,1.6354107,1.5991287,1.5552441,1.5027449,1.4408445,1.3692126,1.2882805,1.1995856,1.1060901,1.012356,0.9244288,0.84930277,0.7939484,0.7640577,0.7640577,0.7939484,0.84930277,0.9244288,1.012356,1.1060901,1.1995856,1.2882805,1.3692126,1.4408445,1.5027449,1.5552441,1.5991287,1.6354107,1.6651638,1.6894245,1.7091367,1.7251265,1.7380981,1.7486401,1.757237,1.7642834,1.7700984,1.7749376,1.7790053,1.782464,1.7854427,1.7880433,1.7903464,1.7924154,1.7943004,1.7960409,1.7976679,1.7992057,1.8006734,1.8020861,1.8034557,1.8047915,1.8061006,1.8073888,1.8086606,1.8099196,1.8111686,1.8124095,1.8136443,1.8148742,1.8161002,1.8173233,1.818544,1.819763],[1.8480121,1.8467932,1.8455725,1.8443494,1.8431233,1.8418934,1.8406587,1.8394177,1.8381687,1.8369098,1.8356379,1.8343498,1.8330406,1.8317049,1.8303353,1.8289225,1.8274548,1.825917,1.82429,1.8225496,1.8206645,1.8185955,1.8162924,1.8136919,1.8107132,1.8072544,1.8031868,1.7983476,1.7925326,1.785486,1.7768891,1.7663472,1.7533755,1.7373859,1.7176737,1.6934128,1.6636598,1.627378,1.5834932,1.5309942,1.4690937,1.3974619,1.3165298,1.2278347,1.1343392,1.0406053,0.95267797,0.8775519,0.82219756,0.79230684,0.79230684,0.82219756,0.8775519,0.95267797,1.0406053,1.1343392,1.2278347,1.3165298,1.3974619,1.4690937,1.5309942,1.5834932,1.627378,1.6636598,1.6934128,1.7176737,1.7373859,1.7533755,1.7663472,1.7768891,1.785486,1.7925326,1.7983476,1.8031868,1.8072544,1.8107132,1.8136919,1.8162924,1.8185955,1.8206645,1.8225496,1.82429,1.825917,1.8274548,1.8289225,1.8303353,1.8317049,1.8330406,1.8343498,1.8356379,1.8369098,1.8381687,1.8394177,1.8406587,1.8418934,1.8431233,1.8443494,1.8455725,1.8467932,1.8480121],[1.8737533,1.8725343,1.8713136,1.8700905,1.8688645,1.8676345,1.8663998,1.8651588,1.86391,1.8626509,1.8613791,1.860091,1.8587818,1.857446,1.8560765,1.8546636,1.8531959,1.8516581,1.8500311,1.8482907,1.8464057,1.8443367,1.8420336,1.839433,1.8364544,1.8329957,1.828928,1.8240888,1.8182738,1.8112272,1.8026303,1.7920884,1.7791167,1.7631271,1.7434149,1.719154,1.6894009,1.6531191,1.6092343,1.5567353,1.4948349,1.423203,1.3422709,1.2535758,1.1600804,1.0663464,0.9784192,0.9032931,0.8479387,0.818048,0.818048,0.8479387,0.9032931,0.9784192,1.0663464,1.1600804,1.2535758,1.3422709,1.423203,1.4948349,1.5567353,1.6092343,1.6531191,1.6894009,1.719154,1.7434149,1.7631271,1.7791167,1.7920884,1.8026303,1.8112272,1.8182738,1.8240888,1.828928,1.8329957,1.8364544,1.839433,1.8420336,1.8443367,1.8464057,1.8482907,1.8500311,1.8516581,1.8531959,1.8546636,1.8560765,1.857446,1.8587818,1.860091,1.8613791,1.8626509,1.86391,1.8651588,1.8663998,1.8676345,1.8688645,1.8700905,1.8713136,1.8725343,1.8737533],[1.897015,1.8957961,1.8945754,1.8933523,1.8921262,1.8908963,1.8896616,1.8884206,1.8871716,1.8859127,1.8846408,1.8833526,1.8820435,1.8807077,1.8793381,1.8779254,1.8764577,1.8749199,1.8732929,1.8715525,1.8696674,1.8675984,1.8652953,1.8626947,1.859716,1.8562573,1.8521897,1.8473505,1.8415354,1.8344889,1.825892,1.81535,1.8023784,1.7863888,1.7666765,1.7424157,1.7126627,1.6763809,1.6324961,1.5799971,1.5180966,1.4464648,1.3655326,1.2768376,1.1833421,1.0896082,1.0016809,0.9265548,0.87120044,0.8413097,0.8413097,0.87120044,0.9265548,1.0016809,1.0896082,1.1833421,1.2768376,1.3655326,1.4464648,1.5180966,1.5799971,1.6324961,1.6763809,1.7126627,1.7424157,1.7666765,1.7863888,1.8023784,1.81535,1.825892,1.8344889,1.8415354,1.8473505,1.8521897,1.8562573,1.859716,1.8626947,1.8652953,1.8675984,1.8696674,1.8715525,1.8732929,1.8749199,1.8764577,1.8779254,1.8793381,1.8807077,1.8820435,1.8833526,1.8846408,1.8859127,1.8871716,1.8884206,1.8896616,1.8908963,1.8921262,1.8933523,1.8945754,1.8957961,1.897015],[1.9178782,1.9166594,1.9154387,1.9142156,1.9129894,1.9117596,1.9105248,1.9092839,1.9080348,1.906776,1.905504,1.9042158,1.9029067,1.901571,1.9002013,1.8987887,1.897321,1.8957832,1.8941562,1.8924158,1.8905306,1.8884616,1.8861585,1.883558,1.8805792,1.8771205,1.8730528,1.8682137,1.8623986,1.8553522,1.8467553,1.8362134,1.8232417,1.8072519,1.7875397,1.763279,1.733526,1.697244,1.6533594,1.6008602,1.5389597,1.467328,1.3863958,1.2977008,1.2042054,1.1104714,1.0225441,0.94741803,0.8920637,0.86217296,0.86217296,0.8920637,0.94741803,1.0225441,1.1104714,1.2042054,1.2977008,1.3863958,1.467328,1.5389597,1.6008602,1.6533594,1.697244,1.733526,1.763279,1.7875397,1.8072519,1.8232417,1.8362134,1.8467553,1.8553522,1.8623986,1.8682137,1.8730528,1.8771205,1.8805792,1.883558,1.8861585,1.8884616,1.8905306,1.8924158,1.8941562,1.8957832,1.897321,1.8987887,1.9002013,1.901571,1.9029067,1.9042158,1.905504,1.906776,1.9080348,1.9092839,1.9105248,1.9117596,1.9129894,1.9142156,1.9154387,1.9166594,1.9178782],[1.9364645,1.9352456,1.9340249,1.9328018,1.9315758,1.9303459,1.9291111,1.9278702,1.9266212,1.9253622,1.9240904,1.9228022,1.921493,1.9201573,1.9187877,1.917375,1.9159073,1.9143695,1.9127425,1.911002,1.909117,1.907048,1.9047449,1.9021443,1.8991656,1.8957069,1.8916392,1.8868,1.880985,1.8739386,1.8653417,1.8547997,1.8418281,1.8258383,1.8061261,1.7818654,1.7521123,1.7158303,1.6719457,1.6194465,1.5575461,1.4859142,1.4049821,1.3162872,1.2227917,1.1290576,1.0411304,0.9660044,0.91065,0.8807593,0.8807593,0.91065,0.9660044,1.0411304,1.1290576,1.2227917,1.3162872,1.4049821,1.4859142,1.5575461,1.6194465,1.6719457,1.7158303,1.7521123,1.7818654,1.8061261,1.8258383,1.8418281,1.8547997,1.8653417,1.8739386,1.880985,1.8868,1.8916392,1.8957069,1.8991656,1.9021443,1.9047449,1.907048,1.909117,1.911002,1.9127425,1.9143695,1.9159073,1.917375,1.9187877,1.9201573,1.921493,1.9228022,1.9240904,1.9253622,1.9266212,1.9278702,1.9291111,1.9303459,1.9315758,1.9328018,1.9340249,1.9352456,1.9364645],[1.9529232,1.9517043,1.9504836,1.9492605,1.9480344,1.9468045,1.9455698,1.9443288,1.9430798,1.9418209,1.940549,1.9392608,1.9379517,1.936616,1.9352463,1.9338336,1.9323659,1.9308281,1.9292011,1.9274607,1.9255756,1.9235066,1.9212035,1.918603,1.9156243,1.9121655,1.9080979,1.9032587,1.8974437,1.8903971,1.8818002,1.8712583,1.8582866,1.842297,1.8225847,1.7983239,1.7685709,1.7322891,1.6884043,1.6359053,1.5740048,1.502373,1.4214408,1.3327458,1.2392503,1.1455164,1.057589,0.982463,0.92710865,0.8972179,0.8972179,0.92710865,0.982463,1.057589,1.1455164,1.2392503,1.3327458,1.4214408,1.502373,1.5740048,1.6359053,1.6884043,1.7322891,1.7685709,1.7983239,1.8225847,1.842297,1.8582866,1.8712583,1.8818002,1.8903971,1.8974437,1.9032587,1.9080979,1.9121655,1.9156243,1.918603,1.9212035,1.9235066,1.9255756,1.9274607,1.9292011,1.9308281,1.9323659,1.9338336,1.9352463,1.936616,1.9379517,1.9392608,1.940549,1.9418209,1.9430798,1.9443288,1.9455698,1.9468045,1.9480344,1.9492605,1.9504836,1.9517043,1.9529232],[1.9674202,1.9662013,1.9649806,1.9637575,1.9625314,1.9613016,1.9600668,1.9588258,1.9575769,1.9563179,1.955046,1.9537579,1.9524487,1.951113,1.9497434,1.9483306,1.9468629,1.9453251,1.9436982,1.9419577,1.9400727,1.9380037,1.9357005,1.9331,1.9301213,1.9266626,1.9225949,1.9177557,1.9119407,1.9048941,1.8962972,1.8857553,1.8727837,1.856794,1.8370818,1.8128209,1.783068,1.7467861,1.7029014,1.6504023,1.5885018,1.51687,1.4359379,1.3472428,1.2537473,1.1600134,1.0720861,0.99696004,0.9416057,0.911715,0.911715,0.9416057,0.99696004,1.0720861,1.1600134,1.2537473,1.3472428,1.4359379,1.51687,1.5885018,1.6504023,1.7029014,1.7467861,1.783068,1.8128209,1.8370818,1.856794,1.8727837,1.8857553,1.8962972,1.9048941,1.9119407,1.9177557,1.9225949,1.9266626,1.9301213,1.9331,1.9357005,1.9380037,1.9400727,1.9419577,1.9436982,1.9453251,1.9468629,1.9483306,1.9497434,1.951113,1.9524487,1.9537579,1.955046,1.9563179,1.9575769,1.9588258,1.9600668,1.9613016,1.9625314,1.9637575,1.9649806,1.9662013,1.9674202],[1.9801296,1.9789107,1.97769,1.9764669,1.9752408,1.974011,1.9727762,1.9715352,1.9702863,1.9690273,1.9677554,1.9664673,1.9651581,1.9638224,1.9624528,1.96104,1.9595723,1.9580345,1.9564075,1.9546671,1.952782,1.950713,1.9484099,1.9458094,1.9428307,1.939372,1.9353043,1.9304651,1.9246501,1.9176035,1.9090066,1.8984647,1.885493,1.8695034,1.8497912,1.8255303,1.7957773,1.7594955,1.7156107,1.6631117,1.6012112,1.5295794,1.4486473,1.3599522,1.2664567,1.1727228,1.0847955,1.0096694,0.95431507,0.92442435,0.92442435,0.95431507,1.0096694,1.0847955,1.1727228,1.2664567,1.3599522,1.4486473,1.5295794,1.6012112,1.6631117,1.7156107,1.7594955,1.7957773,1.8255303,1.8497912,1.8695034,1.885493,1.8984647,1.9090066,1.9176035,1.9246501,1.9304651,1.9353043,1.939372,1.9428307,1.9458094,1.9484099,1.950713,1.952782,1.9546671,1.9564075,1.9580345,1.9595723,1.96104,1.9624528,1.9638224,1.9651581,1.9664673,1.9677554,1.9690273,1.9702863,1.9715352,1.9727762,1.974011,1.9752408,1.9764669,1.97769,1.9789107,1.9801296],[1.991226,1.9900072,1.9887865,1.9875634,1.9863372,1.9851074,1.9838727,1.9826317,1.9813826,1.9801238,1.9788518,1.9775636,1.9762545,1.9749188,1.9735491,1.9721365,1.9706688,1.969131,1.967504,1.9657636,1.9638784,1.9618094,1.9595063,1.9569058,1.953927,1.9504683,1.9464006,1.9415615,1.9357464,1.9287,1.9201031,1.9095612,1.8965895,1.8805997,1.8608875,1.8366268,1.8068738,1.7705919,1.7267072,1.674208,1.6123075,1.5406758,1.4597436,1.3710486,1.2775532,1.1838192,1.095892,1.0207658,0.9654115,0.93552077,0.93552077,0.9654115,1.0207658,1.095892,1.1838192,1.2775532,1.3710486,1.4597436,1.5406758,1.6123075,1.674208,1.7267072,1.7705919,1.8068738,1.8366268,1.8608875,1.8805997,1.8965895,1.9095612,1.9201031,1.9287,1.9357464,1.9415615,1.9464006,1.9504683,1.953927,1.9569058,1.9595063,1.9618094,1.9638784,1.9657636,1.967504,1.969131,1.9706688,1.9721365,1.9735491,1.9749188,1.9762545,1.9775636,1.9788518,1.9801238,1.9813826,1.9826317,1.9838727,1.9851074,1.9863372,1.9875634,1.9887865,1.9900072,1.991226],[2.0008793,1.9996604,1.9984397,1.9972166,1.9959905,1.9947606,1.9935259,1.9922849,1.9910359,1.989777,1.9885051,1.987217,1.9859078,1.984572,1.9832025,1.9817897,1.980322,1.9787842,1.9771572,1.9754168,1.9735317,1.9714627,1.9691596,1.966559,1.9635804,1.9601216,1.956054,1.9512148,1.9453998,1.9383533,1.9297564,1.9192145,1.9062428,1.8902531,1.8705409,1.8462801,1.816527,1.7802451,1.7363604,1.6838613,1.6219609,1.550329,1.4693968,1.3807019,1.2872064,1.1934724,1.1055452,1.0304191,0.97506475,0.94517404,0.94517404,0.97506475,1.0304191,1.1055452,1.1934724,1.2872064,1.3807019,1.4693968,1.550329,1.6219609,1.6838613,1.7363604,1.7802451,1.816527,1.8462801,1.8705409,1.8902531,1.9062428,1.9192145,1.9297564,1.9383533,1.9453998,1.9512148,1.956054,1.9601216,1.9635804,1.966559,1.9691596,1.9714627,1.9735317,1.9754168,1.9771572,1.9787842,1.980322,1.9817897,1.9832025,1.984572,1.9859078,1.987217,1.9885051,1.989777,1.9910359,1.9922849,1.9935259,1.9947606,1.9959905,1.9972166,1.9984397,1.9996604,2.0008793],[2.009251,2.008032,2.0068114,2.0055883,2.004362,2.0031323,2.0018976,2.0006566,1.9994076,1.9981487,1.9968768,1.9955887,1.9942795,1.9929438,1.9915742,1.9901614,1.9886937,1.9871559,1.985529,1.9837885,1.9819034,1.9798344,1.9775313,1.9749308,1.9719521,1.9684933,1.9644257,1.9595865,1.9537715,1.9467249,1.938128,1.9275861,1.9146144,1.8986248,1.8789126,1.8546517,1.8248987,1.7886169,1.7447321,1.6922331,1.6303326,1.5587008,1.4777687,1.3890736,1.2955781,1.2018442,1.1139169,1.0387908,0.98343647,0.95354575,0.95354575,0.98343647,1.0387908,1.1139169,1.2018442,1.2955781,1.3890736,1.4777687,1.5587008,1.6303326,1.6922331,1.7447321,1.7886169,1.8248987,1.8546517,1.8789126,1.8986248,1.9146144,1.9275861,1.938128,1.9467249,1.9537715,1.9595865,1.9644257,1.9684933,1.9719521,1.9749308,1.9775313,1.9798344,1.9819034,1.9837885,1.985529,1.9871559,1.9886937,1.9901614,1.9915742,1.9929438,1.9942795,1.9955887,1.9968768,1.9981487,1.9994076,2.0006566,2.0018976,2.0031323,2.004362,2.0055883,2.0068114,2.008032,2.009251],[2.0164917,2.0152726,2.014052,2.0128288,2.0116029,2.0103729,2.009138,2.0078971,2.0066483,2.0053892,2.0041175,2.0028293,2.0015202,2.0001843,1.9988148,1.997402,1.9959342,1.9943964,1.9927695,1.991029,1.9891441,1.9870751,1.984772,1.9821713,1.9791927,1.975734,1.9716663,1.9668272,1.9610121,1.9539655,1.9453686,1.9348267,1.9218551,1.9058654,1.8861532,1.8618923,1.8321393,1.7958574,1.7519727,1.6994736,1.6375732,1.5659413,1.4850092,1.3963141,1.3028188,1.2090847,1.1211575,1.0460314,0.99067706,0.96078634,0.96078634,0.99067706,1.0460314,1.1211575,1.2090847,1.3028188,1.3963141,1.4850092,1.5659413,1.6375732,1.6994736,1.7519727,1.7958574,1.8321393,1.8618923,1.8861532,1.9058654,1.9218551,1.9348267,1.9453686,1.9539655,1.9610121,1.9668272,1.9716663,1.975734,1.9791927,1.9821713,1.984772,1.9870751,1.9891441,1.991029,1.9927695,1.9943964,1.9959342,1.997402,1.9988148,2.0001843,2.0015202,2.0028293,2.0041175,2.0053892,2.0066483,2.0078971,2.009138,2.0103729,2.0116029,2.0128288,2.014052,2.0152726,2.0164917],[2.0227392,2.0215201,2.0202994,2.0190763,2.0178504,2.0166206,2.0153856,2.014145,2.0128958,2.0116367,2.010365,2.0090768,2.0077677,2.006432,2.0050623,2.0036497,2.002182,2.0006442,1.9990171,1.9972767,1.9953916,1.9933226,1.9910195,1.9884189,1.9854403,1.9819815,1.9779139,1.9730747,1.9672596,1.9602132,1.9516163,1.9410744,1.9281027,1.912113,1.8924007,1.86814,1.8383869,1.802105,1.7582203,1.7057211,1.6438208,1.5721889,1.4912567,1.4025618,1.3090663,1.2153323,1.127405,1.052279,0.99692464,0.9670339,0.9670339,0.99692464,1.052279,1.127405,1.2153323,1.3090663,1.4025618,1.4912567,1.5721889,1.6438208,1.7057211,1.7582203,1.802105,1.8383869,1.86814,1.8924007,1.912113,1.9281027,1.9410744,1.9516163,1.9602132,1.9672596,1.9730747,1.9779139,1.9819815,1.9854403,1.9884189,1.9910195,1.9933226,1.9953916,1.9972767,1.9990171,2.0006442,2.002182,2.0036497,2.0050623,2.006432,2.0077677,2.0090768,2.010365,2.0116367,2.0128958,2.014145,2.0153856,2.0166206,2.0178504,2.0190763,2.0202994,2.0215201,2.0227392],[2.028119,2.0269,2.0256793,2.0244563,2.02323,2.0220003,2.0207655,2.0195246,2.0182757,2.0170166,2.0157447,2.0144567,2.0131474,2.0118117,2.0104423,2.0090294,2.0075617,2.006024,2.004397,2.0026565,2.0007715,1.9987024,1.9963993,1.9937987,1.99082,1.9873613,1.9832937,1.9784545,1.9726394,1.9655929,1.956996,1.946454,1.9334824,1.9174927,1.8977805,1.8735197,1.8437667,1.8074849,1.7636001,1.711101,1.6492006,1.5775688,1.4966366,1.4079416,1.3144461,1.2207122,1.1327848,1.0576588,1.0023044,0.9724137,0.9724137,1.0023044,1.0576588,1.1327848,1.2207122,1.3144461,1.4079416,1.4966366,1.5775688,1.6492006,1.711101,1.7636001,1.8074849,1.8437667,1.8735197,1.8977805,1.9174927,1.9334824,1.946454,1.956996,1.9655929,1.9726394,1.9784545,1.9832937,1.9873613,1.99082,1.9937987,1.9963993,1.9987024,2.0007715,2.0026565,2.004397,2.006024,2.0075617,2.0090294,2.0104423,2.0118117,2.0131474,2.0144567,2.0157447,2.0170166,2.0182757,2.0195246,2.0207655,2.0220003,2.02323,2.0244563,2.0256793,2.0269,2.028119],[2.0327435,2.0315247,2.030304,2.0290809,2.027855,2.026625,2.0253901,2.0241492,2.0229,2.0216413,2.0203695,2.019081,2.0177722,2.0164363,2.0150666,2.013654,2.0121863,2.0106485,2.0090215,2.007281,2.005396,2.003327,2.0010238,1.9984233,1.9954447,1.9919859,1.9879183,1.9830791,1.977264,1.9702175,1.9616206,1.9510787,1.938107,1.9221174,1.9024051,1.8781443,1.8483913,1.8121095,1.7682247,1.7157257,1.6538252,1.5821934,1.5012612,1.4125662,1.3190707,1.2253368,1.1374094,1.0622834,1.006929,0.9770383,0.9770383,1.006929,1.0622834,1.1374094,1.2253368,1.3190707,1.4125662,1.5012612,1.5821934,1.6538252,1.7157257,1.7682247,1.8121095,1.8483913,1.8781443,1.9024051,1.9221174,1.938107,1.9510787,1.9616206,1.9702175,1.977264,1.9830791,1.9879183,1.9919859,1.9954447,1.9984233,2.0010238,2.003327,2.005396,2.007281,2.0090215,2.0106485,2.0121863,2.013654,2.0150666,2.0164363,2.0177722,2.019081,2.0203695,2.0216413,2.0229,2.0241492,2.0253901,2.026625,2.027855,2.0290809,2.030304,2.0315247,2.0327435],[2.036713,2.0354939,2.0342731,2.03305,2.031824,2.030594,2.0293593,2.0281184,2.0268695,2.0256104,2.0243387,2.0230505,2.0217414,2.0204055,2.019036,2.0176232,2.0161555,2.0146177,2.0129907,2.0112503,2.0093653,2.0072963,2.0049932,2.0023925,1.999414,1.9959552,1.9918876,1.9870484,1.9812334,1.9741868,1.9655899,1.955048,1.9420763,1.9260867,1.9063745,1.8821136,1.8523605,1.8160787,1.7721939,1.7196949,1.6577945,1.5861626,1.5052304,1.4165354,1.32304,1.229306,1.1413788,1.0662526,1.0108982,0.9810076,0.9810076,1.0108982,1.0662526,1.1413788,1.229306,1.32304,1.4165354,1.5052304,1.5861626,1.6577945,1.7196949,1.7721939,1.8160787,1.8523605,1.8821136,1.9063745,1.9260867,1.9420763,1.955048,1.9655899,1.9741868,1.9812334,1.9870484,1.9918876,1.9959552,1.999414,2.0023925,2.0049932,2.0072963,2.0093653,2.0112503,2.0129907,2.0146177,2.0161555,2.0176232,2.019036,2.0204055,2.0217414,2.0230505,2.0243387,2.0256104,2.0268695,2.0281184,2.0293593,2.030594,2.031824,2.03305,2.0342731,2.0354939,2.036713],[2.0401154,2.0388966,2.0376759,2.0364528,2.0352266,2.0339968,2.032762,2.031521,2.030272,2.0290132,2.0277412,2.026453,2.0251439,2.0238082,2.0224385,2.021026,2.0195582,2.0180204,2.0163934,2.014653,2.0127678,2.0106988,2.0083957,2.0057952,2.0028164,1.9993577,1.99529,1.9904509,1.9846358,1.9775894,1.9689925,1.9584506,1.9454789,1.9294891,1.9097769,1.8855162,1.8557632,1.8194813,1.7755966,1.7230974,1.661197,1.5895652,1.508633,1.419938,1.3264426,1.2327086,1.1447814,1.0696552,1.0143008,0.98441017,0.98441017,1.0143008,1.0696552,1.1447814,1.2327086,1.3264426,1.419938,1.508633,1.5895652,1.661197,1.7230974,1.7755966,1.8194813,1.8557632,1.8855162,1.9097769,1.9294891,1.9454789,1.9584506,1.9689925,1.9775894,1.9846358,1.9904509,1.99529,1.9993577,2.0028164,2.0057952,2.0083957,2.0106988,2.0127678,2.014653,2.0163934,2.0180204,2.0195582,2.021026,2.0224385,2.0238082,2.0251439,2.026453,2.0277412,2.0290132,2.030272,2.031521,2.032762,2.0339968,2.0352266,2.0364528,2.0376759,2.0388966,2.0401154],[2.0430288,2.04181,2.0405893,2.0393662,2.03814,2.0369103,2.0356755,2.0344346,2.0331855,2.0319266,2.0306547,2.0293665,2.0280573,2.0267217,2.025352,2.0239394,2.0224717,2.0209339,2.019307,2.0175664,2.0156813,2.0136123,2.0113091,2.0087087,2.00573,2.0022712,1.9982035,1.9933643,1.9875493,1.9805028,1.971906,1.961364,1.9483924,1.9324026,1.9126904,1.8884296,1.8586767,1.8223947,1.7785101,1.7260109,1.6641104,1.5924786,1.5115465,1.4228514,1.3293561,1.235622,1.1476948,1.0725687,1.0172143,0.98732364,0.98732364,1.0172143,1.0725687,1.1476948,1.235622,1.3293561,1.4228514,1.5115465,1.5924786,1.6641104,1.7260109,1.7785101,1.8223947,1.8586767,1.8884296,1.9126904,1.9324026,1.9483924,1.961364,1.971906,1.9805028,1.9875493,1.9933643,1.9982035,2.0022712,2.00573,2.0087087,2.0113091,2.0136123,2.0156813,2.0175664,2.019307,2.0209339,2.0224717,2.0239394,2.025352,2.0267217,2.0280573,2.0293665,2.0306547,2.0319266,2.0331855,2.0344346,2.0356755,2.0369103,2.03814,2.0393662,2.0405893,2.04181,2.0430288],[2.0455213,2.0443025,2.0430818,2.0418587,2.0406327,2.0394027,2.038168,2.036927,2.035678,2.034419,2.0331473,2.031859,2.03055,2.0292141,2.0278444,2.0264318,2.024964,2.0234263,2.0217993,2.0200589,2.0181737,2.0161047,2.0138016,2.0112011,2.0082226,2.0047636,2.0006962,1.9958569,1.9900419,1.9829953,1.9743984,1.9638565,1.9508848,1.9348952,1.915183,1.8909221,1.8611691,1.8248873,1.7810025,1.7285035,1.666603,1.5949712,1.514039,1.425344,1.3318485,1.2381146,1.1501873,1.0750612,1.0197068,0.9898161,0.9898161,1.0197068,1.0750612,1.1501873,1.2381146,1.3318485,1.425344,1.514039,1.5949712,1.666603,1.7285035,1.7810025,1.8248873,1.8611691,1.8909221,1.915183,1.9348952,1.9508848,1.9638565,1.9743984,1.9829953,1.9900419,1.9958569,2.0006962,2.0047636,2.0082226,2.0112011,2.0138016,2.0161047,2.0181737,2.0200589,2.0217993,2.0234263,2.024964,2.0264318,2.0278444,2.0292141,2.03055,2.031859,2.0331473,2.034419,2.035678,2.036927,2.038168,2.0394027,2.0406327,2.0418587,2.0430818,2.0443025,2.0455213],[2.0476518,2.046433,2.0452123,2.0439892,2.042763,2.041533,2.0402985,2.0390573,2.0378084,2.0365496,2.0352776,2.0339894,2.0326803,2.0313444,2.029975,2.028562,2.0270944,2.0255566,2.0239296,2.0221891,2.0203042,2.0182352,2.015932,2.0133314,2.0103528,2.006894,2.0028265,1.9979873,1.9921722,1.9851258,1.9765289,1.965987,1.9530153,1.9370255,1.9173133,1.8930526,1.8632995,1.8270175,1.7831329,1.7306337,1.6687334,1.5971014,1.5161693,1.4274744,1.3339789,1.2402449,1.1523176,1.0771916,1.0218372,0.9919465,0.9919465,1.0218372,1.0771916,1.1523176,1.2402449,1.3339789,1.4274744,1.5161693,1.5971014,1.6687334,1.7306337,1.7831329,1.8270175,1.8632995,1.8930526,1.9173133,1.9370255,1.9530153,1.965987,1.9765289,1.9851258,1.9921722,1.9979873,2.0028265,2.006894,2.0103528,2.0133314,2.015932,2.0182352,2.0203042,2.0221891,2.0239296,2.0255566,2.0270944,2.028562,2.029975,2.0313444,2.0326803,2.0339894,2.0352776,2.0365496,2.0378084,2.0390573,2.0402985,2.041533,2.042763,2.0439892,2.0452123,2.046433,2.0476518],[2.0494716,2.0482526,2.0470319,2.0458088,2.0445828,2.0433528,2.042118,2.040877,2.0396283,2.0383692,2.0370975,2.0358093,2.0345001,2.0331643,2.0317948,2.030382,2.0289142,2.0273764,2.0257494,2.024009,2.022124,2.020055,2.017752,2.0151513,2.0121727,2.008714,2.0046463,1.9998071,1.9939921,1.9869455,1.9783486,1.9678067,1.954835,1.9388454,1.9191332,1.8948723,1.8651192,1.8288374,1.7849526,1.7324536,1.6705532,1.5989213,1.5179892,1.4292941,1.3357987,1.2420647,1.1541375,1.0790113,1.023657,0.9937663,0.9937663,1.023657,1.0790113,1.1541375,1.2420647,1.3357987,1.4292941,1.5179892,1.5989213,1.6705532,1.7324536,1.7849526,1.8288374,1.8651192,1.8948723,1.9191332,1.9388454,1.954835,1.9678067,1.9783486,1.9869455,1.9939921,1.9998071,2.0046463,2.008714,2.0121727,2.0151513,2.017752,2.020055,2.022124,2.024009,2.0257494,2.0273764,2.0289142,2.030382,2.0317948,2.0331643,2.0345001,2.0358093,2.0370975,2.0383692,2.0396283,2.040877,2.042118,2.0433528,2.0445828,2.0458088,2.0470319,2.0482526,2.0494716],[2.051025,2.049806,2.0485854,2.0473623,2.0461364,2.0449064,2.0436716,2.0424306,2.0411816,2.0399227,2.038651,2.0373626,2.0360537,2.0347178,2.033348,2.0319355,2.0304677,2.02893,2.027303,2.0255625,2.0236773,2.0216084,2.0193052,2.0167048,2.0137262,2.0102673,2.0061998,2.0013604,1.9955455,1.9884989,1.979902,1.9693601,1.9563885,1.9403988,1.9206866,1.8964257,1.8666728,1.8303909,1.7865062,1.7340071,1.6721066,1.6004748,1.5195427,1.4308476,1.3373522,1.2436182,1.1556909,1.0805649,1.0252105,0.9953198,0.9953198,1.0252105,1.0805649,1.1556909,1.2436182,1.3373522,1.4308476,1.5195427,1.6004748,1.6721066,1.7340071,1.7865062,1.8303909,1.8666728,1.8964257,1.9206866,1.9403988,1.9563885,1.9693601,1.979902,1.9884989,1.9955455,2.0013604,2.0061998,2.0102673,2.0137262,2.0167048,2.0193052,2.0216084,2.0236773,2.0255625,2.027303,2.02893,2.0304677,2.0319355,2.033348,2.0347178,2.0360537,2.0373626,2.038651,2.0399227,2.0411816,2.0424306,2.0436716,2.0449064,2.0461364,2.0473623,2.0485854,2.049806,2.051025],[2.0523505,2.0511317,2.049911,2.048688,2.0474617,2.046232,2.0449972,2.0437562,2.0425072,2.0412483,2.0399764,2.0386882,2.037379,2.0360434,2.0346737,2.033261,2.0317934,2.0302556,2.0286286,2.0268881,2.025003,2.022934,2.0206308,2.0180304,2.0150516,2.0115929,2.0075252,2.002686,1.996871,1.9898245,1.9812276,1.9706857,1.9577141,1.9417243,1.9220121,1.8977513,1.8679984,1.8317164,1.7878318,1.7353326,1.6734321,1.6018003,1.5208682,1.4321731,1.3386778,1.2449437,1.1570165,1.0818903,1.026536,0.99664533,0.99664533,1.026536,1.0818903,1.1570165,1.2449437,1.3386778,1.4321731,1.5208682,1.6018003,1.6734321,1.7353326,1.7878318,1.8317164,1.8679984,1.8977513,1.9220121,1.9417243,1.9577141,1.9706857,1.9812276,1.9898245,1.996871,2.002686,2.0075252,2.0115929,2.0150516,2.0180304,2.0206308,2.022934,2.025003,2.0268881,2.0286286,2.0302556,2.0317934,2.033261,2.0346737,2.0360434,2.037379,2.0386882,2.0399764,2.0412483,2.0425072,2.0437562,2.0449972,2.046232,2.0474617,2.048688,2.049911,2.0511317,2.0523505],[2.053481,2.052262,2.0510414,2.0498183,2.048592,2.0473623,2.0461276,2.0448866,2.0436378,2.0423787,2.0411067,2.0398188,2.0385094,2.0371737,2.0358043,2.0343914,2.0329237,2.031386,2.029759,2.0280185,2.0261335,2.0240645,2.0217614,2.0191607,2.016182,2.0127234,2.0086555,2.0038166,1.9980015,1.9909549,1.982358,1.9718161,1.9588444,1.9428548,1.9231426,1.8988817,1.8691287,1.8328469,1.7889621,1.7364631,1.6745626,1.6029308,1.5219986,1.4333036,1.3398081,1.2460742,1.1581469,1.0830208,1.0276664,0.99777573,0.99777573,1.0276664,1.0830208,1.1581469,1.2460742,1.3398081,1.4333036,1.5219986,1.6029308,1.6745626,1.7364631,1.7889621,1.8328469,1.8691287,1.8988817,1.9231426,1.9428548,1.9588444,1.9718161,1.982358,1.9909549,1.9980015,2.0038166,2.0086555,2.0127234,2.016182,2.0191607,2.0217614,2.0240645,2.0261335,2.0280185,2.029759,2.031386,2.0329237,2.0343914,2.0358043,2.0371737,2.0385094,2.0398188,2.0411067,2.0423787,2.0436378,2.0448866,2.0461276,2.0473623,2.048592,2.0498183,2.0510414,2.052262,2.053481],[2.0544448,2.053226,2.0520053,2.0507822,2.049556,2.048326,2.0470915,2.0458503,2.0446014,2.0433426,2.0420706,2.0407825,2.0394733,2.0381374,2.036768,2.035355,2.0338874,2.0323496,2.0307226,2.0289822,2.0270972,2.0250282,2.022725,2.0201244,2.0171459,2.0136871,2.0096195,2.0047803,1.9989653,1.9919188,1.9833219,1.97278,1.9598083,1.9438186,1.9241064,1.8998456,1.8700925,1.8338106,1.7899259,1.7374268,1.6755264,1.6038945,1.5229623,1.4342674,1.3407719,1.2470379,1.1591107,1.0839846,1.0286303,0.99873954,0.99873954,1.0286303,1.0839846,1.1591107,1.2470379,1.3407719,1.4342674,1.5229623,1.6038945,1.6755264,1.7374268,1.7899259,1.8338106,1.8700925,1.8998456,1.9241064,1.9438186,1.9598083,1.97278,1.9833219,1.9919188,1.9989653,2.0047803,2.0096195,2.0136871,2.0171459,2.0201244,2.022725,2.0250282,2.0270972,2.0289822,2.0307226,2.0323496,2.0338874,2.035355,2.036768,2.0381374,2.0394733,2.0407825,2.0420706,2.0433426,2.0446014,2.0458503,2.0470915,2.048326,2.049556,2.0507822,2.0520053,2.053226,2.0544448],[2.0552664,2.0540473,2.0528266,2.0516036,2.0503774,2.0491476,2.0479128,2.0466719,2.045423,2.044164,2.042892,2.041604,2.0402946,2.038959,2.0375896,2.0361767,2.034709,2.0331712,2.0315442,2.0298038,2.0279188,2.0258498,2.0235467,2.020946,2.0179672,2.0145087,2.0104408,2.005602,1.9997867,1.9927402,1.9841433,1.9736013,1.9606297,1.94464,1.9249278,1.900667,1.870914,1.8346322,1.7907474,1.7382483,1.6763479,1.6047161,1.5237839,1.4350889,1.3415934,1.2478595,1.1599321,1.0848061,1.0294517,0.999561,0.999561,1.0294517,1.0848061,1.1599321,1.2478595,1.3415934,1.4350889,1.5237839,1.6047161,1.6763479,1.7382483,1.7907474,1.8346322,1.870914,1.900667,1.9249278,1.94464,1.9606297,1.9736013,1.9841433,1.9927402,1.9997867,2.005602,2.0104408,2.0145087,2.0179672,2.020946,2.0235467,2.0258498,2.0279188,2.0298038,2.0315442,2.0331712,2.034709,2.0361767,2.0375896,2.038959,2.0402946,2.041604,2.042892,2.044164,2.045423,2.0466719,2.0479128,2.0491476,2.0503774,2.0516036,2.0528266,2.0540473,2.0552664],[2.0559661,2.0547473,2.0535266,2.0523036,2.0510774,2.0498476,2.0486128,2.0473719,2.0461228,2.044864,2.043592,2.0423038,2.0409946,2.039659,2.0382893,2.0368767,2.035409,2.0338712,2.0322442,2.0305037,2.0286186,2.0265496,2.0242465,2.021646,2.0186672,2.0152085,2.0111408,2.0063016,2.0004866,1.9934402,1.9848433,1.9743013,1.9613297,1.9453399,1.9256277,1.901367,1.871614,1.835332,1.7914474,1.7389482,1.6770477,1.6054159,1.5244838,1.4357888,1.3422934,1.2485594,1.1606321,1.085506,1.0301516,1.000261,1.000261,1.0301516,1.085506,1.1606321,1.2485594,1.3422934,1.4357888,1.5244838,1.6054159,1.6770477,1.7389482,1.7914474,1.835332,1.871614,1.901367,1.9256277,1.9453399,1.9613297,1.9743013,1.9848433,1.9934402,2.0004866,2.0063016,2.0111408,2.0152085,2.0186672,2.021646,2.0242465,2.0265496,2.0286186,2.0305037,2.0322442,2.0338712,2.035409,2.0368767,2.0382893,2.039659,2.0409946,2.0423038,2.043592,2.044864,2.0461228,2.0473719,2.0486128,2.0498476,2.0510774,2.0523036,2.0535266,2.0547473,2.0559661],[2.0565624,2.0553436,2.054123,2.0528998,2.0516737,2.0504436,2.049209,2.047968,2.046719,2.0454602,2.0441883,2.0429,2.041591,2.040255,2.0388856,2.0374727,2.036005,2.0344672,2.0328403,2.0310998,2.0292149,2.0271459,2.0248427,2.022242,2.0192635,2.0158048,2.011737,2.006898,2.001083,1.9940364,1.9854395,1.9748976,1.961926,1.9459362,1.926224,1.9019632,1.8722101,1.8359282,1.7920436,1.7395444,1.677644,1.6060121,1.52508,1.436385,1.3428895,1.2491555,1.1612283,1.0861022,1.0307479,1.0008572,1.0008572,1.0307479,1.0861022,1.1612283,1.2491555,1.3428895,1.436385,1.52508,1.6060121,1.677644,1.7395444,1.7920436,1.8359282,1.8722101,1.9019632,1.926224,1.9459362,1.961926,1.9748976,1.9854395,1.9940364,2.001083,2.006898,2.011737,2.0158048,2.0192635,2.022242,2.0248427,2.0271459,2.0292149,2.0310998,2.0328403,2.0344672,2.036005,2.0374727,2.0388856,2.040255,2.041591,2.0429,2.0441883,2.0454602,2.046719,2.047968,2.049209,2.0504436,2.0516737,2.0528998,2.054123,2.0553436,2.0565624],[2.0570703,2.0558515,2.0546308,2.0534077,2.0521815,2.0509515,2.049717,2.0484757,2.047227,2.045968,2.044696,2.043408,2.0420988,2.040763,2.0393934,2.0379806,2.0365129,2.034975,2.033348,2.0316076,2.0297227,2.0276537,2.0253506,2.02275,2.0197713,2.0163126,2.012245,2.0074058,2.0015907,1.9945443,1.9859474,1.9754055,1.9624338,1.946444,1.9267318,1.9024711,1.872718,1.836436,1.7925514,1.7400522,1.6781518,1.6065199,1.5255878,1.4368929,1.3433974,1.2496634,1.1617361,1.0866101,1.0312557,1.0013651,1.0013651,1.0312557,1.0866101,1.1617361,1.2496634,1.3433974,1.4368929,1.5255878,1.6065199,1.6781518,1.7400522,1.7925514,1.836436,1.872718,1.9024711,1.9267318,1.946444,1.9624338,1.9754055,1.9859474,1.9945443,2.0015907,2.0074058,2.012245,2.0163126,2.0197713,2.02275,2.0253506,2.0276537,2.0297227,2.0316076,2.033348,2.034975,2.0365129,2.0379806,2.0393934,2.040763,2.0420988,2.043408,2.044696,2.045968,2.047227,2.0484757,2.049717,2.0509515,2.0521815,2.0534077,2.0546308,2.0558515,2.0570703],[2.0575027,2.056284,2.0550632,2.0538402,2.052614,2.0513842,2.0501494,2.0489085,2.0476594,2.0464005,2.0451286,2.0438404,2.0425313,2.0411956,2.039826,2.0384133,2.0369456,2.0354078,2.0337808,2.0320404,2.0301552,2.0280862,2.025783,2.0231826,2.0202038,2.016745,2.0126774,2.0078382,2.0020232,1.9949768,1.9863799,1.975838,1.9628663,1.9468765,1.9271643,1.9029036,1.8731506,1.8368686,1.792984,1.7404848,1.6785843,1.6069525,1.5260204,1.4373254,1.34383,1.250096,1.1621687,1.0870426,1.0316882,1.0017976,1.0017976,1.0316882,1.0870426,1.1621687,1.250096,1.34383,1.4373254,1.5260204,1.6069525,1.6785843,1.7404848,1.792984,1.8368686,1.8731506,1.9029036,1.9271643,1.9468765,1.9628663,1.975838,1.9863799,1.9949768,2.0020232,2.0078382,2.0126774,2.016745,2.0202038,2.0231826,2.025783,2.0280862,2.0301552,2.0320404,2.0337808,2.0354078,2.0369456,2.0384133,2.039826,2.0411956,2.0425313,2.0438404,2.0451286,2.0464005,2.0476594,2.0489085,2.0501494,2.0513842,2.052614,2.0538402,2.0550632,2.056284,2.0575027]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":\"darkorange\",\"size\":9},\"mode\":\"markers\",\"name\":\"\\u003cbr\\u003eMinimum\\u003cbr\\u003e\",\"showlegend\":true,\"x\":[0],\"y\":[-10],\"z\":[0.00004539787187241018],\"type\":\"scatter3d\"},{\"marker\":{\"color\":\"darkorange\",\"size\":14},\"mode\":\"markers\",\"showlegend\":false,\"x\":[0],\"y\":[-10],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,0.45],\"y\":[0.0,1.0]},\"aspectmode\":\"cube\"},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.55,1.0],\"range\":[-6,6]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"range\":[-10,6]},\"annotations\":[{\"font\":{\"size\":20},\"showarrow\":false,\"text\":\"3D plot\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":20},\"showarrow\":false,\"text\":\"2D plot\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"font\":{\"size\":40}},\"margin\":{\"l\":60,\"r\":60,\"t\":60,\"b\":60},\"height\":450,\"width\":1000},                        {\"displayModeBar\": false, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c4ea8d13-cbd7-4747-9782-82f3271767e3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def pathological_curve_loss(x: Tensor, y: Tensor):\n",
        "    # Example of a pathological curvature. There are many more possible, feel free to experiment here!\n",
        "    x_loss = t.tanh(x) ** 2 + 0.01 * t.abs(x)\n",
        "    y_loss = t.sigmoid(y)\n",
        "    return x_loss + y_loss\n",
        "\n",
        "\n",
        "plot_fn(pathological_curve_loss, min_points=[(0, \"y_min\")])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzC0FpMqIT1R"
      },
      "source": [
        "In terms of optimization, you can image that `x` and `y` are weight parameters, and the curvature represents the loss surface over the space of `x` and `y`. Note that in typical networks, we have many, many more parameters than two, and such curvatures can occur in multi-dimensional spaces as well.\n",
        "\n",
        "Ideally, our optimization algorithm would find the center of the ravine and focuses on optimizing the parameters towards the direction of `y`. However, if we encounter a point along the ridges, the gradient is much greater in `x` than `y`, and we might end up jumping from one side to the other. Due to the large gradients, we would have to reduce our learning rate slowing down learning significantly.\n",
        "\n",
        "To test our algorithms, we can implement a simple function to train two parameters on such a surface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k620h1l3IT1S"
      },
      "source": [
        "### Exercise - implement `opt_fn_with_sgd`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵🔵🔵⚪\n",
        ">\n",
        "> You should spend up to 15-20 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Implement the `opt_fn_with_sgd` function using `torch.optim.SGD`. This function optimizes parameters `(x, y)` (which represent coordinates at which we evaluate a function) using gradient descent on that function value. In other words, this should look just like your optimization loops in previous days' material, except rather than passing in `model.parameters()` to your optimizer, you pass in `(xy,)` (because it needs to be an iterable of parameters, not just a single parameter).\n",
        "\n",
        "Remember, your update steps `optimizer.step()` will automatically change the values of `xy` inplace - this means that you shouldn't store past values like `xy_list.append(xy)` because then past elements of that list will be modified when `xy` is updated. Instead, you should use something like `xy_list.append(xy.detach().clone())` to make sure you're returning a copy of the tensor, which won't continue to be modified.\n",
        "\n",
        "We've also provided you with a function `plot_fn_with_points`, which plots a function as well as a list of points produced by functions like the one above. The code below starts from `(2.5, 2.5)` and adds the resulting trajectory of `(x, y)` coordinates to the contour plot. Does it find the minimum? Play with the learning rate and momentum a bit and see how close you can get within 100 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JXL0RoKEIT1S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "eb1d7ba3-4eb5-4e82-8599-b113f42b184c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-364818be0dd2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moptimizer_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mxys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_fn_with_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathological_curve_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"momentum\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{params=}, last point={xys[-1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-364818be0dd2>\u001b[0m in \u001b[0;36mopt_fn_with_sgd\u001b[0;34m(fn, xy, lr, momentum, n_iters)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def opt_fn_with_sgd(\n",
        "    fn: Callable, xy: Float[Tensor, \"2\"], lr=0.001, momentum=0.98, n_iters: int = 100\n",
        ") -> Float[Tensor, \"n_iters 2\"]:\n",
        "    \"\"\"\n",
        "    Optimize the a given function starting from the specified point.\n",
        "\n",
        "    xy: shape (2,). The (x, y) starting point.\n",
        "    n_iters: number of steps.\n",
        "    lr, momentum: parameters passed to the torch.optim.SGD optimizer.\n",
        "\n",
        "    Return: (n_iters+1, 2). The (x, y) values, from initial values pre-optimization to values after step `n_iters`.\n",
        "    \"\"\"\n",
        "    # Make sure tensor has requires_grad=True, otherwise it can't be optimized (more on this tomorrow!)\n",
        "    assert xy.requires_grad\n",
        "\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "points = []\n",
        "\n",
        "optimizer_list = [\n",
        "    (optim.SGD, {\"lr\": 0.1, \"momentum\": 0.0}),\n",
        "    (optim.SGD, {\"lr\": 0.02, \"momentum\": 0.99}),\n",
        "]\n",
        "\n",
        "for optimizer_class, params in optimizer_list:\n",
        "    xy = t.tensor([2.5, 2.5], requires_grad=True)\n",
        "    xys = opt_fn_with_sgd(pathological_curve_loss, xy=xy, lr=params[\"lr\"], momentum=params[\"momentum\"])\n",
        "    points.append((xys, optimizer_class, params))\n",
        "    print(f\"{params=}, last point={xys[-1]}\")\n",
        "\n",
        "plot_fn_with_points(pathological_curve_loss, points=points, min_points=[(0, \"y_min\")])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ1y7dggIT1T"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I'm not sure if my <code>opt_fn_with_sgd</code> is implemented properly.</summary>\n",
        "\n",
        "With a learning rate of `0.02` and momentum of `0.99`, my SGD was able to reach `[ 0.8110, -6.3344]` after 100 iterations.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm getting <code>Can't call numpy() on Tensor that requires grad</code>.</summary>\n",
        "\n",
        "This is a protective mechanism built into PyTorch. The idea is that once you convert your Tensor to NumPy, PyTorch can no longer track gradients, but you might not understand this and expect backprop to work on NumPy arrays.\n",
        "\n",
        "All you need to do to convince PyTorch you're a responsible adult is to call `detach()` on the tensor first, which returns a view that does not require grad and isn't part of the computation graph.\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def opt_fn_with_sgd(\n",
        "    fn: Callable, xy: Float[Tensor, \"2\"], lr=0.001, momentum=0.98, n_iters: int = 100\n",
        ") -> Float[Tensor, \"n_iters 2\"]:\n",
        "    \"\"\"\n",
        "    Optimize the a given function starting from the specified point.\n",
        "\n",
        "    xy: shape (2,). The (x, y) starting point.\n",
        "    n_iters: number of steps.\n",
        "    lr, momentum: parameters passed to the torch.optim.SGD optimizer.\n",
        "\n",
        "    Return: (n_iters+1, 2). The (x, y) values, from initial values pre-optimization to values after step `n_iters`.\n",
        "    \"\"\"\n",
        "    # Make sure tensor has requires_grad=True, otherwise it can't be optimized (more on this tomorrow!)\n",
        "    assert xy.requires_grad\n",
        "\n",
        "    optimizer = optim.SGD((xy,), lr=lr, momentum=momentum)\n",
        "\n",
        "    xy_list = [xy.detach().clone()]  # so that we don't unintentionally modify past values in `xy_list`\n",
        "\n",
        "    for i in range(n_iters):\n",
        "        fn(xy[0], xy[1]).backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        xy_list.append(xy.detach().clone())\n",
        "\n",
        "    return t.stack(xy_list)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZNwTCa_IT1U"
      },
      "source": [
        "## Build Your Own Optimizers\n",
        "\n",
        "Now let's build our own drop-in replacement for these three classes from `torch.optim`. For each of the exercises you'll have to translate pseudocode that we give you into actual code. If you want an extra challenge, you can try and work directly from the pseudocode in the PyTorch documentation page rather than what we give you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3dFcIUsIT1U"
      },
      "source": [
        "> **A warning regarding in-place operations**\n",
        ">\n",
        "> Be careful with expressions like `x = x + y` and `x += y`. They are NOT equivalent in Python.\n",
        ">\n",
        "> - The first one allocates a new `Tensor` of the appropriate size and adds `x` and `y` to it, then rebinds `x` to point to the new variable. The original `x` is not modified.\n",
        "> - The second one modifies the storage referred to by `x` to contain the sum of `x` and `y` - it is an \"in-place\" operation. `x.add_(y)` and `torch.add(x, y, out=x)` also work the same way.\n",
        ">\n",
        "> Another example: if `x` and `y` are the same shape, then `x = y` won't change the value of `x` inplace, but `x.copy_(y)` will (i.e. changing its values to the values of `y`).\n",
        ">\n",
        "> When you're updating parameters in your network you _should_ use inplace operations (because your `optimizer` was passed an iterable of parameters, and so defining a new parameter value via `theta = theta - step` will take it out of the optimizer's scope - it will continue to point to the old, unmodified version).\n",
        ">\n",
        "> However, be careful of using inplace operations where you shouldn't be - you don't want to accidentally do something like modify the gradients manually!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcvxGi5jIT1V"
      },
      "source": [
        "### Exercise - implement SGD\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴🔴⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 25-35 minutes on this exercise.\n",
        "> This is the first of several exercises like it. The first will probably take the longest.\n",
        "> ```\n",
        "\n",
        "First, you should implement stochastic gradient descent. It should be like the [PyTorch version](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD), but assume `nesterov=False`, `maximize=False`, and `dampening=0`. The pseudocode simplifies to:\n",
        "\n",
        "$\n",
        "b_0 \\leftarrow 0 \\\\\n",
        "\\text {for } t=1 \\text { to } \\ldots \\text { do } \\\\\n",
        "\\quad\\; g_t \\leftarrow \\nabla_\\theta f_t\\left(\\theta_{t-1}\\right) \\\\\n",
        "\\quad\\; \\text {if } \\lambda \\neq 0 \\\\\n",
        "\\quad\\;\\quad\\; g_t \\leftarrow g_t+\\lambda \\theta_{t-1} \\\\\n",
        "\\quad\\; \\text {if } \\mu \\neq 0 \\\\\n",
        "\\quad\\;\\quad\\; b_t \\leftarrow \\mu b_{t-1} + g_t \\\\\n",
        "\\quad\\;\\quad\\; g_t \\leftarrow b_t \\\\\n",
        "\\quad\\; \\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t\n",
        "$\n",
        "\n",
        "where $\\theta_t$ are the parameters, $g_t$ are the gradients (after being modified by operations like weight decay & momentum if necessary), and $b_t$ are the values we track to implement momentum.\n",
        "\n",
        "<details>\n",
        "<summary>Derivation of the simplified pseudocode</summary>\n",
        "\n",
        "We start by removing the \"if nesterov\" and \"if maximize\" sections, since we're not using either of those. We also substitute $\\tau=0$ since we're not using dampening. This gives us:\n",
        "\n",
        "$\n",
        "\\text {for } t=1 \\text { to } \\ldots \\text { do } \\\\\n",
        "\\quad\\; g_t \\leftarrow \\nabla_\\theta f_t\\left(\\theta_{t-1}\\right) \\\\\n",
        "\\quad\\; \\text {if } \\lambda \\neq 0 \\\\\n",
        "\\quad\\;\\quad\\; g_t \\leftarrow g_t+\\lambda \\theta_{t-1} \\\\\n",
        "\\quad\\; \\text {if } \\mu \\neq 0 \\\\\n",
        "\\quad\\;\\quad\\; \\text{if } t>1 \\\\\n",
        "\\quad\\;\\quad\\;\\quad\\; b_t \\leftarrow \\mu b_{t-1} + g_t \\\\\n",
        "\\quad\\;\\quad\\; else \\\\\n",
        "\\quad\\;\\quad\\;\\quad\\; b_t \\leftarrow g_t \\\\\n",
        "\\quad\\;\\quad\\; g_t \\leftarrow b_t \\\\\n",
        "\\quad\\; \\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t\n",
        "$\n",
        "\n",
        "Finally, we observe that we can set $b_0 = 0$ and then remove the special case handling of the $t=1$ case, which gives us the pseudocode above.\n",
        "\n",
        "</details>\n",
        "\n",
        "You should complete the `step` method below, which implements the algorithm described by the pseudocode above. Note that we've added the `torch.inference_mode` decorator to the `step` method, which is equivalent to using the context manager `with torch.inference_mode():`. This is similar to `torch.no_grad`; the difference between them isn't worth getting into here but in general know that `torch.inference_mode` is mostly preferred.\n",
        "\n",
        "The configurations used during `tests.test_sgd` will start simple (e.g. all parameters set to zero except `lr`) and gradually move to more complicated ones. This will help you track exactly where in your model the error is coming from.\n",
        "\n",
        "\n",
        "You should also read the `__init__` and `zero_grad` methods, making sure you understand how these work and what they are doing. Note that setting `grad=None` like the code below is treated as equivalent to setting `grad` equal to a tensor of zeros, i.e. the first time we're required to do an operation on the gradient it'll be replaced with this. Making it be `None` by default is the standard, so as to not use unnecessary memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LTjm3I2IT1W"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[t.nn.parameter.Parameter],\n",
        "        lr: float,\n",
        "        momentum: float = 0.0,\n",
        "        weight_decay: float = 0.0,\n",
        "    ):\n",
        "        \"\"\"Implements SGD with momentum.\n",
        "\n",
        "        Like the PyTorch version, but assume nesterov=False, maximize=False, and dampening=0\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
        "        \"\"\"\n",
        "        self.params = list(params)  # turn params into a list (it might be a generator, so iterating over it empties it)\n",
        "        self.lr = lr\n",
        "        self.mu = momentum\n",
        "        self.lmda = weight_decay\n",
        "\n",
        "        self.b = [t.zeros_like(p) for p in self.params]\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        \"\"\"Zeros all gradients of the parameters in `self.params`.\"\"\"\n",
        "        for param in self.params:\n",
        "            param.grad = None\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        \"\"\"Performs a single optimization step of the SGD algorithm.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"SGD(lr={self.lr}, momentum={self.mu}, weight_decay={self.lmda})\"\n",
        "\n",
        "\n",
        "tests.test_sgd(SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Kf_rpfIT1X"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "class SGD:\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[t.nn.parameter.Parameter],\n",
        "        lr: float,\n",
        "        momentum: float = 0.0,\n",
        "        weight_decay: float = 0.0,\n",
        "    ):\n",
        "        \"\"\"Implements SGD with momentum.\n",
        "\n",
        "        Like the PyTorch version, but assume nesterov=False, maximize=False, and dampening=0\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
        "        \"\"\"\n",
        "        self.params = list(params)  # turn params into a list (it might be a generator, so iterating over it empties it)\n",
        "        self.lr = lr\n",
        "        self.mu = momentum\n",
        "        self.lmda = weight_decay\n",
        "\n",
        "        self.b = [t.zeros_like(p) for p in self.params]\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        \"\"\"Zeros all gradients of the parameters in `self.params`.\"\"\"\n",
        "        for param in self.params:\n",
        "            param.grad = None\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        \"\"\"Performs a single optimization step of the SGD algorithm.\"\"\"\n",
        "        for b, theta in zip(self.b, self.params):\n",
        "            g = theta.grad\n",
        "            if self.lmda != 0:\n",
        "                g = g + self.lmda * theta  # this shouldn't be inplace since we don't want to modify theta.grad\n",
        "            if self.mu != 0:\n",
        "                b.copy_(self.mu * b + g)  # this does need to be inplace, since we're modifying the value in `self.b`\n",
        "                g = b\n",
        "            theta -= self.lr * g  # inplace operation, to modify params\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"SGD(lr={self.lr}, momentum={self.mu}, weight_decay={self.lmda})\"\n",
        "\n",
        "\n",
        "tests.test_sgd(SGD)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iIs7OjxIT1X"
      },
      "source": [
        "If you feel comfortable with this implementation, you can skim through the remaining ones, since there's diminishing marginal returns to be gained from doing the actual exercises. We still recommend you read the content on the optimizers before the actual exercises, because they contain useful theory to understand. If you want an extra challenge in the actual exercises, you can try and implement the optimization algorithms directly from the PyTorch documentation pseudocode rather than from the simplified pseudocode we give you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e558HS3cIT1Y"
      },
      "source": [
        "### RMSProp (and adaptive methods)\n",
        "\n",
        "From SGD, we'll move onto discussing **adaptive gradient descent methods**. These are methods which automatically adjust the learning rate of each parameter during training, based on the size of gradients at previous steps. In a sense this is similar to how momentum operates in SGD, but we don't tend to describe SGD plus momentum as an adaptive method. When discussing momentum, we usually think of the analogy of a ball rolling down a hill, and the ball's velocity accelerates until it reaches some terminal velocity. The momentum parameter $\\mu$ controls the terminal velocity: as $\\mu \\to 1$ the terminal velocity gets very high, which also means it can take a long time to adjust its speed when it enters new territory. In contrast, adaptive methods are better thought of as deliberate, conscious updates to the learning rate of parameters based on past values. They allow us to speed up when we need to, but without sacrificing our ability to adapt quickly when we enter new regimes.\n",
        "\n",
        "The first adaptive method we'll look at is **RMSprop**. This is actually the second main adaptive method that was proposed in the optimization literature, after AdaGrad (however the problem with AdaGrad is that it decays the learning rates too quickly - this is the problem that RMSprop solves). RMSprop is similar to SGD, with an added dynamic: **the size of parameter steps are scaled according to the variance of past gradients**, with higher variance leading to smaller steps. Intuitively, if you're in a very monotonic region of the loss landscape then you want to take larger steps (since you know where you're going and you just want to get there quickly), whereas if you're in a very noisy region and possibly oscillating around minima then you want to take smaller steps.\n",
        "\n",
        "One final note - when we're using non-adaptive methods like SGD we tend to have an inverse relationship between the learning rate and the batch size. Broadly speaking, this is because a larger batch size means our gradients will have smaller variance, and so we can safely use a larger learning rate. This generally isn't necessary for adaptive methods since the learning rates will be adjusted automatically during training based on the variance of our gradients - we don't need to manually scale them ourselves. Most commonly during optimization, we'll start with the default hyperparameters for whatever adaptive optimizer we're using, and then adjust from there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SViRuxeXIT1Z"
      },
      "source": [
        "### Exercise - implement RMSprop\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 15-25 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Below, you should implement RMSprop in the same way as you implemented SGD. The pseudocode is slightly more complicated, since we now have to track 2 variables: $b_t$ for applying the momentum effect, and $v_t$ for tracking the variance of past gradients (we've called these `b` and `v` below).\n",
        "\n",
        "[Here](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html) is a link to the PyTorch version, alternatively you can use our simplified pseudocode again:\n",
        "\n",
        "<details>\n",
        "<summary>Click here for the simplified pseudocode</summary>\n",
        "\n",
        "$\n",
        "b_0 \\leftarrow 0 \\\\\n",
        "\\text {for } t=1 \\text { to } \\ldots \\text { do } \\\\\n",
        "\\quad\\; g_t \\leftarrow \\nabla_\\theta f_t\\left(\\theta_{t-1}\\right) \\\\\n",
        "\\quad\\; \\text {if } \\lambda \\neq 0 \\\\\n",
        "\\quad\\;\\quad\\; g_t \\leftarrow g_t+\\lambda \\theta_{t-1} \\\\\n",
        "\\quad\\; v_t \\leftarrow \\alpha v_{t-1} + (1-\\alpha) g_t^2 \\\\\n",
        "\\quad\\; g_t \\leftarrow g_t / (\\sqrt{v_t} + \\epsilon) \\\\\n",
        "\\quad\\; \\text {if } \\mu \\neq 0 \\\\\n",
        "\\quad\\;\\quad\\; b_t \\leftarrow \\mu b_{t-1} + g_t \\\\\n",
        "\\quad\\;\\quad\\; g_t \\leftarrow b_t \\\\\n",
        "\\quad\\; \\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t\n",
        "$\n",
        "\n",
        "Note that we've reordered the pseudocode slightly differently to the PyTorch docs, so that we divide $g_t$ by $\\sqrt{v_t + \\epsilon}$ before applying momentum. Both ways are equivalent though.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvbsGBqkIT1Z"
      },
      "outputs": [],
      "source": [
        "class RMSprop:\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[t.nn.parameter.Parameter],\n",
        "        lr: float = 0.01,\n",
        "        alpha: float = 0.99,\n",
        "        eps: float = 1e-08,\n",
        "        weight_decay: float = 0.0,\n",
        "        momentum: float = 0.0,\n",
        "    ):\n",
        "        \"\"\"Implements RMSprop.\n",
        "\n",
        "        Like the PyTorch version, but assumes centered=False\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html\n",
        "        \"\"\"\n",
        "        self.params = list(params)  # turn params into a list (because it might be a generator)\n",
        "        self.lr = lr\n",
        "        self.eps = eps\n",
        "        self.mu = momentum\n",
        "        self.lmda = weight_decay\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.b = [t.zeros_like(p) for p in self.params]\n",
        "        self.v = [t.zeros_like(p) for p in self.params]\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        for p in self.params:\n",
        "            p.grad = None\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (\n",
        "            f\"RMSprop(lr={self.lr}, eps={self.eps}, momentum={self.mu}, weight_decay={self.lmda}, alpha={self.alpha})\"\n",
        "        )\n",
        "\n",
        "\n",
        "tests.test_rmsprop(RMSprop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkX086t4IT1a"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "class RMSprop:\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[t.nn.parameter.Parameter],\n",
        "        lr: float = 0.01,\n",
        "        alpha: float = 0.99,\n",
        "        eps: float = 1e-08,\n",
        "        weight_decay: float = 0.0,\n",
        "        momentum: float = 0.0,\n",
        "    ):\n",
        "        \"\"\"Implements RMSprop.\n",
        "\n",
        "        Like the PyTorch version, but assumes centered=False\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html\n",
        "        \"\"\"\n",
        "        self.params = list(params)  # turn params into a list (because it might be a generator)\n",
        "        self.lr = lr\n",
        "        self.eps = eps\n",
        "        self.mu = momentum\n",
        "        self.lmda = weight_decay\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.b = [t.zeros_like(p) for p in self.params]\n",
        "        self.v = [t.zeros_like(p) for p in self.params]\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        for p in self.params:\n",
        "            p.grad = None\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        for theta, b, v in zip(self.params, self.b, self.v):\n",
        "            g = theta.grad\n",
        "            if self.lmda != 0:\n",
        "                g = g + self.lmda * theta\n",
        "            v.copy_(self.alpha * v + (1 - self.alpha) * g.pow(2))  # inplace operation, to modify value in self.v\n",
        "            g = g / (v.sqrt() + self.eps)  # not inplace operation\n",
        "            if self.mu > 0:\n",
        "                b.copy_(self.mu * b + g)  # inplace operation, to modify value in self.b\n",
        "                g = b\n",
        "            theta -= self.lr * g  # inplace operation, to modify params\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (\n",
        "            f\"RMSprop(lr={self.lr}, eps={self.eps}, momentum={self.mu}, weight_decay={self.lmda}, alpha={self.alpha})\"\n",
        "        )\n",
        "\n",
        "\n",
        "tests.test_rmsprop(RMSprop)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7N05QOOIT1b"
      },
      "source": [
        "### Adam, and \"momentum\"\n",
        "\n",
        "We'll end by implementing Adam and AdamW, two of the most popular optimizers in deep learning. These combine the benefits of RMSprop and SGD with momentum: they have the same variance-based scaling as RMSprop, but they also have an update rule based on the first moment of gradients as well.\n",
        "\n",
        "There's an important clarification to make here - the first order adjustment of Adam is sometimes called momentum as a shorthand, but there's an important sense in which it isn't. The key difference is that SGD's momentum causes acceleration until we hit terminal velocity, which could be very large for $\\mu \\approx 1$. In contrast, Adam's momentum is an exponentially weighted moving average - the parameter $\\beta_1$ controls how quickly it adjusts (with a value closer to 1 meaning it adjust to newer values more slowly), but it doesn't change the terminal velocity in any sense. Mathematically, the difference between these two is minimal (all you'd need to do is take Adam's update rule $m_t \\leftarrow \\beta_1 m_{t-1} + (1-\\beta_1) g_t$ and change it to $m_t \\leftarrow \\beta_1 m_{t-1} + g_t$ for it to have the same qualitative behaviour as SGD), but this extra factor makes a lot of difference!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q13jeB5IT1b"
      },
      "source": [
        "### Exercise - implement Adam\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴🔴⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 15-20 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "This should just be an extension of your RMSprop implementation. You still have 2 variables to track, but now the variable $b_t$ for applying momentum has been replaced with $m_t$ for tracking the exponentially weighted moving average of first order moments.\n",
        "\n",
        "[Here's](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) a link to the PyTorch version, alternatively you can use the simplified pseudocode below:\n",
        "\n",
        "<details>\n",
        "<summary>Click here for the simplified pseudocode</summary>\n",
        "\n",
        "$\n",
        "\\text {for } t=1 \\text { to } \\ldots \\text { do } \\\\\n",
        "\\quad\\; g_t \\leftarrow \\nabla_\\theta f_t\\left(\\theta_{t-1}\\right) \\\\\n",
        "\\quad\\; \\text {if } \\lambda \\neq 0 \\\\\n",
        "\\quad\\;\\quad\\; g_t \\leftarrow g_t+\\lambda \\theta_{t-1} \\\\\n",
        "\\quad\\; m_t \\leftarrow \\beta_1 m_{t-1} + (1-\\beta_1) g_t \\\\\n",
        "\\quad\\; v_t \\leftarrow \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 \\\\\n",
        "\\quad\\; \\widehat{m_t} \\leftarrow m_t / (1 - \\beta_1^t) \\\\\n",
        "\\quad\\; \\widehat{v_t} \\leftarrow v_t / (1 - \\beta_2^t) \\\\\n",
        "\\quad\\; \\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t} / (\\sqrt{\\widehat{v_t}} + \\epsilon)\n",
        "$\n",
        "\n",
        "</details>\n",
        "\n",
        "Note - we center our first & second moment estimators by dividing by $1 - \\beta^t$, which means for this optimizer we do have to track the variable $t$ (make sure to remember to increment it after each use of the `step` function). We do this because Adam's exponentially weighted moving average would otherwise take a while to converge to the true mean (since its estimates initially behave like the truncated sum of a geometric series). We leave it as an exercise for the reader to derive this (hint - try assuming the expected value $\\mathbb{E}[g_t] = g_0$ is the same for all $t$, what does the expression $\\mathbb{E}[m_t]$ simplify to?)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN9D5RnbIT1c"
      },
      "outputs": [],
      "source": [
        "class Adam:\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[t.nn.parameter.Parameter],\n",
        "        lr: float = 0.001,\n",
        "        betas: tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-08,\n",
        "        weight_decay: float = 0.0,\n",
        "    ):\n",
        "        \"\"\"Implements Adam.\n",
        "\n",
        "        Like the PyTorch version, but assumes amsgrad=False and maximize=False\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
        "        \"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.beta1, self.beta2 = betas\n",
        "        self.eps = eps\n",
        "        self.lmda = weight_decay\n",
        "        self.t = 1\n",
        "\n",
        "        self.m = [t.zeros_like(p) for p in self.params]\n",
        "        self.v = [t.zeros_like(p) for p in self.params]\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        for p in self.params:\n",
        "            p.grad = None\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Adam(lr={self.lr}, beta1={self.beta1}, beta2={self.beta2}, eps={self.eps}, weight_decay={self.lmda})\"\n",
        "\n",
        "\n",
        "tests.test_adam(Adam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsXe7cWzIT1d"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "class Adam:\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[t.nn.parameter.Parameter],\n",
        "        lr: float = 0.001,\n",
        "        betas: tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-08,\n",
        "        weight_decay: float = 0.0,\n",
        "    ):\n",
        "        \"\"\"Implements Adam.\n",
        "\n",
        "        Like the PyTorch version, but assumes amsgrad=False and maximize=False\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
        "        \"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.beta1, self.beta2 = betas\n",
        "        self.eps = eps\n",
        "        self.lmda = weight_decay\n",
        "        self.t = 1\n",
        "\n",
        "        self.m = [t.zeros_like(p) for p in self.params]\n",
        "        self.v = [t.zeros_like(p) for p in self.params]\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        for p in self.params:\n",
        "            p.grad = None\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        for theta, m, v in zip(self.params, self.m, self.v):\n",
        "            g = theta.grad\n",
        "            if self.lmda != 0:\n",
        "                g = g + self.lmda * theta\n",
        "            m.copy_(self.beta1 * m + (1 - self.beta1) * g)\n",
        "            v.copy_(self.beta2 * v + (1 - self.beta2) * g.pow(2))\n",
        "            m_hat = m / (1 - self.beta1**self.t)\n",
        "            v_hat = v / (1 - self.beta2**self.t)\n",
        "            theta -= self.lr * m_hat / (v_hat.sqrt() + self.eps)\n",
        "        self.t += 1\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Adam(lr={self.lr}, beta1={self.beta1}, beta2={self.beta2}, eps={self.eps}, weight_decay={self.lmda})\"\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pba52K9IT1v"
      },
      "source": [
        "### Exercise - implement AdamW\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵🔵⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 10-15 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Finally, you'll adapt your Adam implementation to implement AdamW. This is a very small modification of the Adam update rule, where we apply weight decay in a different way (by modifying the weights $theta_t$ themselves, rather than modifying the gradients $g_t$ and then using those modified gradients in the first & second moment calculations). This means that, unlike with Adam, using weight decay is equivalent to having a Gaussian prior on the weights with mean zero (or alternatively, equivalent to L2 regularization). This is seen as the more \"correct\" way to implement weight decay, and so AdamW is now generally preferred over Adam.\n",
        "\n",
        "You can read more about this variant of Adam [here](https://arxiv.org/abs/1711.05101). The PyTorch docs are [here](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html), and the pseudocode is again provided for you below (but for this exercise we do recommend trying to go without it - having to work with more complex pseudocode and parse out the bits that actually matter is a useful exercise!).\n",
        "\n",
        "<details>\n",
        "<summary>Click here for the simplified pseudocode</summary>\n",
        "\n",
        "$\n",
        "\\text {for } t=1 \\text { to } \\ldots \\text { do } \\\\\n",
        "\\quad\\; g_t \\leftarrow \\nabla_\\theta f_t\\left(\\theta_{t-1}\\right) \\\\\n",
        "\\quad\\; \\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\lambda \\theta_{t-1} \\\\\n",
        "\\quad\\; m_t \\leftarrow \\beta_1 m_{t-1} + (1-\\beta_1) g_t \\\\\n",
        "\\quad\\; v_t \\leftarrow \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 \\\\\n",
        "\\quad\\; \\widehat{m_t} \\leftarrow m_t / (1 - \\beta_1^t) \\\\\n",
        "\\quad\\; \\widehat{v_t} \\leftarrow v_t / (1 - \\beta_2^t) \\\\\n",
        "\\quad\\; \\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t} / (\\sqrt{\\widehat{v_t}} + \\epsilon)\n",
        "$\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANlMs5I6IT1w"
      },
      "outputs": [],
      "source": [
        "class AdamW:\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[t.nn.parameter.Parameter],\n",
        "        lr: float = 0.001,\n",
        "        betas: tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-08,\n",
        "        weight_decay: float = 0.0,\n",
        "    ):\n",
        "        \"\"\"Implements Adam.\n",
        "\n",
        "        Like the PyTorch version, but assumes amsgrad=False and maximize=False\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
        "        \"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.beta1, self.beta2 = betas\n",
        "        self.eps = eps\n",
        "        self.lmda = weight_decay\n",
        "        self.t = 1\n",
        "\n",
        "        self.m = [t.zeros_like(p) for p in self.params]\n",
        "        self.v = [t.zeros_like(p) for p in self.params]\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        for p in self.params:\n",
        "            p.grad = None\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"AdamW(lr={self.lr}, beta1={self.beta1}, beta2={self.beta2}, eps={self.eps}, weight_decay={self.lmda})\"\n",
        "\n",
        "\n",
        "tests.test_adamw(AdamW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1uZszjWIT1y"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "class AdamW:\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[t.nn.parameter.Parameter],\n",
        "        lr: float = 0.001,\n",
        "        betas: tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-08,\n",
        "        weight_decay: float = 0.0,\n",
        "    ):\n",
        "        \"\"\"Implements Adam.\n",
        "\n",
        "        Like the PyTorch version, but assumes amsgrad=False and maximize=False\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
        "        \"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.beta1, self.beta2 = betas\n",
        "        self.eps = eps\n",
        "        self.lmda = weight_decay\n",
        "        self.t = 1\n",
        "\n",
        "        self.m = [t.zeros_like(p) for p in self.params]\n",
        "        self.v = [t.zeros_like(p) for p in self.params]\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        for p in self.params:\n",
        "            p.grad = None\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        for theta, m, v in zip(self.params, self.m, self.v):\n",
        "            g = theta.grad\n",
        "            theta *= 1 - self.lr * self.lmda\n",
        "            m.copy_(self.beta1 * m + (1 - self.beta1) * g)\n",
        "            v.copy_(self.beta2 * v + (1 - self.beta2) * g.pow(2))\n",
        "            m_hat = m / (1 - self.beta1**self.t)\n",
        "            v_hat = v / (1 - self.beta2**self.t)\n",
        "            theta -= self.lr * m_hat / (v_hat.sqrt() + self.eps)\n",
        "        self.t += 1\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"AdamW(lr={self.lr}, beta1={self.beta1}, beta2={self.beta2}, eps={self.eps}, weight_decay={self.lmda})\"\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WunMHqLgIT1z"
      },
      "source": [
        "## Plotting multiple optimisers\n",
        "\n",
        "Finally, we've provided some code which should allow you to plot more than one of your optimisers at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gflUVwuIT1z"
      },
      "source": [
        "### Exercise - experiment with different optimizers & params\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 20-30 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "We've given you a function below which works just like `opt_fn_with_sgd` from earlier, but takes in a general optimizer and hyperparameters (as a dictionary of keyword arguments like `lr` and `momentum`).\n",
        "\n",
        "You should use this function to play around with different optimizers and hyperparameters, comparing their performance. The code below gives one example of such a comparison, run it now and see what you get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xE-uNsFIT10"
      },
      "outputs": [],
      "source": [
        "def opt_fn(\n",
        "    fn: Callable,\n",
        "    xy: Tensor,\n",
        "    optimizer_class,\n",
        "    optimizer_hyperparams: dict,\n",
        "    n_iters: int = 100,\n",
        ") -> Tensor:\n",
        "    \"\"\"Optimize the a given function starting from the specified point.\n",
        "\n",
        "    optimizer_class: one of the optimizers you've defined, either SGD, RMSprop, or Adam\n",
        "    optimzer_kwargs: keyword arguments passed to your optimiser (e.g. lr and weight_decay)\n",
        "    \"\"\"\n",
        "    assert xy.requires_grad\n",
        "\n",
        "    optimizer = optimizer_class([xy], **optimizer_hyperparams)\n",
        "\n",
        "    xy_list = [xy.detach().clone()]  # so that we don't unintentionally modify past values in `xy_list`\n",
        "\n",
        "    for i in range(n_iters):\n",
        "        fn(xy[0], xy[1]).backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        xy_list.append(xy.detach().clone())\n",
        "\n",
        "    return t.stack(xy_list)\n",
        "\n",
        "\n",
        "points = []\n",
        "\n",
        "optimizer_list = [\n",
        "    (SGD, {\"lr\": 0.03, \"momentum\": 0.99}),\n",
        "    (RMSprop, {\"lr\": 0.02, \"alpha\": 0.99, \"momentum\": 0.8}),\n",
        "    (Adam, {\"lr\": 0.2, \"betas\": (0.99, 0.99), \"weight_decay\": 0.005}),\n",
        "    (AdamW, {\"lr\": 0.2, \"betas\": (0.99, 0.99), \"weight_decay\": 0.005}),\n",
        "]\n",
        "\n",
        "for optimizer_class, params in optimizer_list:\n",
        "    xy = t.tensor([2.5, 2.5], requires_grad=True)\n",
        "    xys = opt_fn(\n",
        "        pathological_curve_loss,\n",
        "        xy=xy,\n",
        "        optimizer_class=optimizer_class,\n",
        "        optimizer_hyperparams=params,\n",
        "    )\n",
        "    points.append((xys, optimizer_class, params))\n",
        "\n",
        "plot_fn_with_points(pathological_curve_loss, min_points=[(0, \"y_min\")], points=points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NW18HKHIT12"
      },
      "source": [
        "Note that the focus shouldn't be on figuring out \"which one is the best optimizer\" - this loss landscape (and other examples we'll give you) were specifically designed to be pathological, and exhibit interesting kinds of behaviours from optimizers. The focus should instead be on understanding how the characteristics of optimizers we discussed in the previous sections are reflected visually in the plots produced on these loss landscapes. Some questions you might want to ask:\n",
        "\n",
        "- We discussed that Adam (and AdamW) center their first and second moments, so that the early values are large - otherwise they start off small and take a long time to grow. Is this reflected in the plots, i.e. with Adam/AdamW taking larger early steps relative to SGD or RMSprop?\n",
        "- The momentum used in SGD and RMSprop causes acceleration until \"terminal velocity\", which is usually a higher cap than Adam and AdamW. Is this reflected in the step size (and the instability) of those optimizers? Do Adam and AdamW seem to adapt slightly faster when they enter new terrain?\n",
        "- Are there any landscapes where weight decay is advantageous, and can you see why it would be?\n",
        "\n",
        "Some more functions you might want to try out (with their minima marked on the plots):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhl_FYJFIT13"
      },
      "outputs": [],
      "source": [
        "def bivariate_gaussian(x, y, x_mean=0.0, y_mean=0.0, x_sig=1.0, y_sig=1.0):\n",
        "    norm = 1 / (2 * np.pi * x_sig * y_sig)\n",
        "    x_exp = 0.5 * ((x - x_mean) ** 2) / (x_sig**2)\n",
        "    y_exp = 0.5 * ((y - y_mean) ** 2) / (y_sig**2)\n",
        "    return norm * t.exp(-x_exp - y_exp)\n",
        "\n",
        "\n",
        "means = [(1.0, -0.5), (-1.0, 0.5), (-0.5, -0.8)]\n",
        "\n",
        "\n",
        "def neg_trimodal_func(x, y):\n",
        "    \"\"\"\n",
        "    This function has 3 global minima, at `means`. Unstable methods can overshoot these minima, and non-adaptive methods\n",
        "    can fail to converge to them in the first place given how shallow the gradients are everywhere except in the close\n",
        "    vicinity of the minima.\n",
        "    \"\"\"\n",
        "    z = -bivariate_gaussian(x, y, x_mean=means[0][0], y_mean=means[0][1], x_sig=0.2, y_sig=0.2)\n",
        "    z -= bivariate_gaussian(x, y, x_mean=means[1][0], y_mean=means[1][1], x_sig=0.2, y_sig=0.2)\n",
        "    z -= bivariate_gaussian(x, y, x_mean=means[2][0], y_mean=means[2][1], x_sig=0.2, y_sig=0.2)\n",
        "    return z\n",
        "\n",
        "\n",
        "plot_fn(neg_trimodal_func, x_range=(-2, 2), y_range=(-2, 2), min_points=means)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSZjqogUIT14"
      },
      "outputs": [],
      "source": [
        "def rosenbrocks_banana_func(x: Tensor, y: Tensor, a=1, b=100) -> Tensor:\n",
        "    \"\"\"\n",
        "    This function has a global minimum at `(a, a)` so in this case `(1, 1)`. It's characterized by a long, narrow,\n",
        "    parabolic valley (parameterized by `y = x**2`). Various gradient descent methods have trouble navigating this\n",
        "    valley because they often oscillate unstably (gradients from the `b`-term dwarf the gradients from the `a`-term).\n",
        "\n",
        "    See more on this function: https://en.wikipedia.org/wiki/Rosenbrock_function.\n",
        "    \"\"\"\n",
        "    return (a - x) ** 2 + b * (y - x**2) ** 2 + 1\n",
        "\n",
        "\n",
        "plot_fn(rosenbrocks_banana_func, x_range=(-2.5, 2.5), y_range=(-2, 4), z_range=(0, 100), min_points=[(1, 1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNVCHizJIT15"
      },
      "source": [
        "<details>\n",
        "<summary>Some example visualizations & observations</summary>\n",
        "\n",
        "Let's start with the negative trimodal function. You should find that weight decay massively helps performance here, but this is for pretty uninteresting reasons - it essentially adds a slope towards the origin, and when the ball rolls towards the origin it will probably also get caught in one of the three minima. So it doesn't tell us much about the actual optimizers.\n",
        "\n",
        "More interestingly, we can compare the optimizers when they have weight decay switched off. You should find that Adam can outperform SGD and RMSprop here, because the way it uses \"momentum\" is better suited to this task. For one thing, the first and second moment centering means it can take larger early steps relative to SGD and RMSprop (which both take a while to accelerate). For another, momentum causes RMSprop step sizes to increase in an unstable way, which is why it will overshoot the minima and get stuck on the other side without careful hyperparameter tuning. SGD is even worse - because of its lack of variance-based scaling, it'll utterly fail to move anywhere unless it starts out very close to one of the three minima.\n",
        "\n",
        "```python\n",
        "optimizer_list = [\n",
        "    (SGD, {\"lr\": 0.1, \"momentum\": 0.5}),\n",
        "    (RMSprop, {\"lr\": 0.1, \"alpha\": 0.99, \"momentum\": 0.5}),\n",
        "    (Adam, {\"lr\": 0.1, \"betas\": (0.9, 0.999)}),\n",
        "]\n",
        "\n",
        "points = []\n",
        "for optimizer_class, params in optimizer_list:\n",
        "    xy = t.tensor([1.0, 1.0], requires_grad=True)\n",
        "    xys = opt_fn(neg_trimodal_func, xy=xy, optimizer_class=optimizer_class, optimizer_hyperparams=params)\n",
        "    points.append((xys, optimizer_class, params))\n",
        "\n",
        "plot_fn_with_points(neg_trimodal_func, points=points, x_range=(-2, 2), y_range=(-2, 2), min_point=means)\n",
        "```\n",
        "\n",
        "<div style=\"text-align: left\"><embed src=\"https://info-arena.github.io/ARENA_img/misc/media-03/0304-points.html\" width=\"1020\" height=\"470\"></div>\n",
        "\n",
        "Next, Rosenbrock's banana. This function has a global minimum at `(1, 1)` inside a long, narrow, parabolic-shaped valley. Basic gradient descent often zigzags back and forth along the valley, making very slow progress. Momentum is absolutely essential to perform well in this task. This is a rare case where SGD plus momentum does converge faster than Adam because the higher terminal velocity enables larger step sizes plus the extreme slope of the loss landscape prevents the kind of instability that usually hinders SGD. However, some caveats: SGD requires a very small step size to prevent unstable oscillations (given how steep the valley is), whereas Adam is much more stable. Furthermore, if we extend the number of iterations, we see that Adam does also converge, and it does so with fewer oscillations than SGD (it stays within the parabolic valley).\n",
        "\n",
        "```python\n",
        "optimizer_list = [\n",
        "    (SGD, {\"lr\": 0.001, \"momentum\": 0.99}),\n",
        "    (Adam, {\"lr\": 0.1, \"betas\": (0.9, 0.999)}),\n",
        "]\n",
        "\n",
        "points = []\n",
        "for optimizer_class, params in optimizer_list:\n",
        "    xy = t.tensor([-1.5, 2.5], requires_grad=True)\n",
        "    xys = opt_fn(\n",
        "        rosenbrocks_banana_func, xy=xy, optimizer_class=optimizer_class, optimizer_hyperparams=params, n_iters=500\n",
        "    )\n",
        "    points.append((xys, optimizer_class, params))\n",
        "\n",
        "plot_fn_with_points(\n",
        "    rosenbrocks_banana_func, x_range=(-2.5, 2.5), y_range=(-2, 4), z_range=(0, 100), min_points=[(1, 1)], points=points\n",
        ")\n",
        "```\n",
        "\n",
        "<div style=\"text-align: left\"><embed src=\"https://info-arena.github.io/ARENA_img/misc/media-03/0305-points.html\" width=\"1020\" height=\"470\"></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8mBiu-hIT16"
      },
      "source": [
        "## Bonus - parameter groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooHJXAn3IT17"
      },
      "source": [
        "> *If you're interested in these exercises then you can go through them, if not then you can move on to the next section (weights and biases).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2nUY9FhIT17"
      },
      "source": [
        "Rather than passing a single iterable of parameters into an optimizer, you have the option to pass a list of parameter groups, each one with different hyperparameters. As an example of how this might work:\n",
        "\n",
        "```python\n",
        "optim.SGD([\n",
        "    {'params': model.base.parameters()},\n",
        "    {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
        "], lr=1e-2, momentum=0.9)\n",
        "```\n",
        "\n",
        "The first argument here is a list of dictionaries, with each dictionary defining a separate parameter group. Each should contain a `params` key, which contains an iterable of parameters belonging to this group. The dictionaries may also contain keyword arguments. If a parameter is not specified in a group, PyTorch uses the value passed as a keyword argument. So the example above is equivalent to:\n",
        "\n",
        "```python\n",
        "optim.SGD([\n",
        "    {'params': model.base.parameters(), 'lr': 1e-2, 'momentum': 0.9},\n",
        "    {'params': model.classifier.parameters(), 'lr': 1e-3, 'momentum': 0.9}\n",
        "])\n",
        "```\n",
        "\n",
        "All parameters have default values, with the exception of `lr` which is why you need to specify it either as a keyword arg to the optimizer or separately in each group.\n",
        "\n",
        "PyTorch optimisers will store all their params and hyperparams in the `param_groups` attribute - this is why when we want to modify an optimizer's learning rate (which we'll do later on in the course), even if we didn't specify any parameter groups we'll still need to use `optimizer.param_groups[0].lr = new_lr`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC6VoUFaIT18"
      },
      "source": [
        "### When to use parameter groups\n",
        "\n",
        "Parameter groups can be useful in several different circumstances. A few examples:\n",
        "\n",
        "* Finetuning a model by freezing earlier layers and only training later layers is an extreme form of parameter grouping. We can use the parameter group syntax to apply a modified form, where the earlier layers have a smaller learning rate. This allows these earlier layers to adapt to the specifics of the problem, while making sure they don't forget all the useful features they've already learned.\n",
        "* Often it's good to treat weights and biases differently, e.g. effects like weight decay are often applied to weights but not biases. PyTorch doesn't differentiate between these two, so you'll have to do this manually using paramter groups.\n",
        "    * This in particular, you might be doing later in the course, if you choose the \"train BERT from scratch\" exercises during the transformers chapter.\n",
        "* On the subject of transformers, weight decay is often *not* applied to embeddings and layernorms in transformer models.\n",
        "\n",
        "More generally, if you're trying to replicate a paper, it's important to be able to use all the same training details that the original authors did, so you can get the same results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pfSE23KIT19"
      },
      "source": [
        "### Exercise - rewrite SGD to use parameter groups\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴🔴⚪\n",
        "> Importance: 🔵⚪⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 30-40 minutes on this exercise.\n",
        "> It's somewhat useful to understand the idea of parameter groups, less so to know how they're actually implemented.\n",
        "> ```\n",
        "\n",
        "You should rewrite the `SGD` optimizer from the earlier exercises, to use `param_groups`. This will involve filling in the 3 methods `__init__`, `zero_grad`, and `step`. Some guidance:\n",
        "\n",
        "- In `__init__` you should create `self.param_groups`, which is a list of dictionaries with each one containing `\"params\"` as well as all the hyperparameters for that group. Remember the hierarchy for hparams: \"specified for group\" > \"specified as a keyword argument\" > \"default value\".\n",
        "- In `zero_grad` and `step` the logic is the same as before, but now you need a double nested for loop: once over the param groups in `self.param_groups`, and once over the params in each group. For the latter, make sure you're using the group-specific hyperparameters (i.e. the ones you hopefully stored in `self.param_groups` in the init method)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH-X1qqDIT1-"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "    def __init__(self, params, **kwargs):\n",
        "        \"\"\"Implements SGD with momentum.\n",
        "\n",
        "        Accepts parameters in groups, or an iterable.\n",
        "\n",
        "        Like the PyTorch version, but assume nesterov=False, maximize=False, and dampening=0\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
        "        \"\"\"\n",
        "        # Deal with case where we didn't supply groups, so we just make it into a single dictionary\n",
        "        if not isinstance(params, (list, tuple)):\n",
        "            params = [{\"params\": params}]\n",
        "\n",
        "        # Make sure each group[\"params\"] is a list of params not a generator (so we don't iterate over & destroy it!)\n",
        "        for p in params:\n",
        "            p[\"params\"] = list(p[\"params\"])\n",
        "\n",
        "        self.param_groups = []\n",
        "\n",
        "        # YOUR CODE HERE - fill in `self.param_groups`\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "tests.test_sgd_param_groups(SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv2huwb2IT1-"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "class SGD:\n",
        "    def __init__(self, params, **kwargs):\n",
        "        \"\"\"Implements SGD with momentum.\n",
        "\n",
        "        Accepts parameters in groups, or an iterable.\n",
        "\n",
        "        Like the PyTorch version, but assume nesterov=False, maximize=False, and dampening=0\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
        "        \"\"\"\n",
        "        # Deal with case where we didn't supply groups, so we just make it into a single dictionary\n",
        "        if not isinstance(params, (list, tuple)):\n",
        "            params = [{\"params\": params}]\n",
        "\n",
        "        # Make sure each group[\"params\"] is a list of params not a generator (so we don't iterate over & destroy it!)\n",
        "        for p in params:\n",
        "            p[\"params\"] = list(p[\"params\"])\n",
        "\n",
        "        self.param_groups = []\n",
        "\n",
        "        for param_group in params:\n",
        "            # Set hyperparameters hierarchically: specified for group > specified as a keyword argument > default value\n",
        "            # We do this via a dictionary merge (right takes precedence over left)\n",
        "            param_group = {\"momentum\": 0.0, \"weight_decay\": 0.0, **kwargs, **param_group}\n",
        "\n",
        "            # Check that \"lr\" is supplied\n",
        "            assert \"lr\" in param_group, \"Error: one of the param groups didn't specify 'lr'\"\n",
        "\n",
        "            # Set \"params\" and \"b\" in our group\n",
        "            param_group[\"b\"] = [t.zeros_like(p) for p in param_group[\"params\"]]\n",
        "\n",
        "            self.param_groups.append(param_group)\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        for param_group in self.param_groups:\n",
        "            for p in param_group[\"params\"]:\n",
        "                p.grad = None\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        # loop through each param group\n",
        "\n",
        "        for param_group in self.param_groups:\n",
        "            # Get hparams for this group\n",
        "            lmda = param_group[\"weight_decay\"]\n",
        "            mu = param_group[\"momentum\"]\n",
        "            lr = param_group[\"lr\"]\n",
        "\n",
        "            # Same code as for SGD implementation before, but using group-specific hparams\n",
        "            for b, theta in zip(param_group[\"b\"], param_group[\"params\"]):\n",
        "                g = theta.grad\n",
        "                if lmda != 0:\n",
        "                    g = g + lmda * theta  # this shouldn't be inplace since we don't want to modify theta.grad\n",
        "                if mu != 0:\n",
        "                    b.copy_(mu * b + g)  # this does need to be inplace, since we're modifying the value in `self.b`\n",
        "                    g = b\n",
        "                theta -= lr * g  # inplace operation, to modify params\n",
        "\n",
        "\n",
        "tests.test_sgd_param_groups(SGD)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gffBGdyIT1_"
      },
      "source": [
        "# 2️⃣ Weights and Biases\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Write modular, extensible code for training models\n",
        "> * Learn what the most important hyperparameters are, and methods for efficiently searching over hyperparameter space\n",
        "> * Learn how to use Weights & Biases for logging your runs\n",
        "> * Adapt your code from yesterday to log training runs to Weights & Biases, and use this service to run **hyperparameter sweeps**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6C1nOzbIT2A"
      },
      "source": [
        "Next, we'll look at methods for choosing hyperparameters effectively. You'll learn how to use **Weights and Biases**, a useful tool for hyperparameter search.\n",
        "\n",
        "The exercises themselves will be based on your ResNet implementations from yesterday, although the principles should carry over to other models you'll build in this course (such as transformers next week).\n",
        "\n",
        "Note, this page only contains a few exercises, and they're relatively short. You're encouraged to spend some time playing around with Weights and Biases, but you should also spend some more time finetuning your ResNet from yesterday (you might want to finetune ResNet during the morning, and look at today's material in the afternoon - you can discuss this with your partner). You should also spend some time reviewing the last three days of material, to make sure there are no large gaps in your understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hygX0I4kIT2B"
      },
      "source": [
        "## Finetuning & feature extraction\n",
        "\n",
        "> We'll start with a brief discussion of the related concepts **finetuning** and **feature extraction**. If you've already gone through yesterday's bonus material on feature extraction then you can skip this section.\n",
        "\n",
        "**Finetuning** can mean slightly different things in different contexts, but broadly speaking it means using the weights of an already trained network as the starting values for training a new network. Because training networks from scratch is very computationally expensive, this is a common practice in ML.\n",
        "\n",
        "The specific type of finetuning we'll be doing here is called **feature extraction**. This is when we freeze most layers of a model except the last few, and perform gradient descent on those. We call this feature extraction because the earlier layers of the model have already learned to identify important features of the data (and these features are also relevant for the new task), so all that we have to do is train a few final layers in the model to extract these features.\n",
        "\n",
        "*Terminology note - sometimes feature extraction and finetuning are defined differently, with finetuning referring to the training of all the weights in a pretrained model (usually with a small or decaying learning rate), and feature extraction referring to the freezing of some layers and training of others. To avoid confusion here, we'll use the term \"feature extraction\" rather than \"finetuning\".*\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/feature_extraction.png\" width=\"400\">\n",
        "\n",
        "The way we implement feature extraction in PyTorch is by **freezing** all but the last few layers of our model, meaning gradients don't propagate back through them (and we don't perform gradient descent updates on them) - more on gradient freezing tomorrow! We've used the `get_resnet_for_feature_extraction` function to do this (the code for this is given to you below so you won't have to write it yourself). This function creates a version of the `ResNet34` model, loads in weights from the PyTorch ResNet34 implementation, freezes all layers, and replaces the final linear layer with an unfrozen randomly initialized linear layer with a certain number of output features (in our case 10 because we're doing feature extraction on CIFAR10 - see next section)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqXEe2gfIT2B"
      },
      "source": [
        "## CIFAR10\n",
        "\n",
        "The benchmark we'll be doing feature extraction on is [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html), which consists of 60000 32x32 colour images in 10 different classes (as opposed to the 1000 different classes that `ResNet34` was originally trained on). Don't peek at what other people online have done for CIFAR10 (it's a common benchmark), because the point is to develop your own process by which you can figure out how to improve your model. Just reading the results of someone else would prevent you from learning how to get the answers. To get an idea of what's possible: using one V100 and a modified ResNet, one entry in the DAWNBench competition was able to achieve 94% test accuracy in 24 epochs and 76 seconds. 94% is approximately [human level performance](http://karpathy.github.io/2011/04/27/manually-classifying-cifar10/).\n",
        "\n",
        "Below is some boilerplate code for downloading and transforming `CIFAR10` data (this shouldn't take more than a minute to run the first time). Note, even though CIFAR10 data is 32x32, we'll resize it to 224x224 like we did for ImageNet yesterday, because ResNet expects 224x224 images as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q188P0T0IT2C"
      },
      "outputs": [],
      "source": [
        "def get_cifar() -> tuple[datasets.CIFAR10, datasets.CIFAR10]:\n",
        "    \"\"\"Returns CIFAR-10 train and test sets.\"\"\"\n",
        "    cifar_trainset = datasets.CIFAR10(exercises_dir / \"data\", train=True, download=True, transform=IMAGENET_TRANSFORM)\n",
        "    cifar_testset = datasets.CIFAR10(exercises_dir / \"data\", train=False, download=True, transform=IMAGENET_TRANSFORM)\n",
        "    return cifar_trainset, cifar_testset\n",
        "\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "IMAGENET_TRANSFORM = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "cifar_trainset, cifar_testset = get_cifar()\n",
        "\n",
        "imshow(\n",
        "    cifar_trainset.data[:15],\n",
        "    facet_col=0,\n",
        "    facet_col_wrap=5,\n",
        "    facet_labels=[cifar_trainset.classes[i] for i in cifar_trainset.targets[:15]],\n",
        "    title=\"CIFAR-10 images\",\n",
        "    height=600,\n",
        "    width=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnUfEGvBIT2D"
      },
      "source": [
        "## Train function (modular)\n",
        "\n",
        "First, let's build on the training function we used yesterday. Previously, we just used a single `train` function which took a dataclass as argument. But this resulted in a very long function with many nested loops and some repeated code. Instead, we'll write our code in the form of a `ResNetFinetuner` class with multiple methods, each one being responsible for a single part of the training process. This will make our code more modular, and easier to read and debug.\n",
        "\n",
        "We've given you the `ResNetFinetuner` class below, as well as a dataclass which contains all the hyperparameters we'll use (again this helps us keep everything organized). You should read this and make sure you understand the role of each method. A brief summary:\n",
        "\n",
        "* `pre_training_setup` defines the model, optimizer, dataset, and objects for logging data. Note that it's good practice to have this logic run in `__init__`, because it's something we only need to do just before actually training (this structural flexibility will prove useful later, when we introduce weights & biases).\n",
        "* `training_step` does a single gradient update step on a single batch of data, and logs & returns the loss.\n",
        "* `evaluate` method computes the total accuracy of the model over the validation set, and logs & returns this accuracy. Note use of the `torch.inference_mode()` decorator, which stops gradients propagating (this is equivalent to using it as a context manager).\n",
        "* `train` combines this all: it performs the pre-training setup, then alternates between training & evaluation for some number of epochs. Note that `model.train()` and `model.eval()` are called before these stages respectively - for why we have to do this, see yesterday's discussion of BatchNorm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "550sEehvIT2E"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ResNetFinetuningArgs:\n",
        "    n_classes: int = 10\n",
        "    batch_size: int = 128\n",
        "    epochs: int = 3\n",
        "    learning_rate: float = 1e-3\n",
        "    weight_decay: float = 0.0\n",
        "\n",
        "\n",
        "class ResNetFinetuner:\n",
        "    def __init__(self, args: ResNetFinetuningArgs):\n",
        "        self.args = args\n",
        "\n",
        "    def pre_training_setup(self):\n",
        "        self.model = get_resnet_for_feature_extraction(self.args.n_classes).to(device)\n",
        "        self.optimizer = AdamW(\n",
        "            self.model.out_layers[-1].parameters(), lr=self.args.learning_rate, weight_decay=self.args.weight_decay\n",
        "        )\n",
        "        self.trainset, self.testset = get_cifar()\n",
        "        self.train_loader = DataLoader(self.trainset, batch_size=self.args.batch_size, shuffle=True)\n",
        "        self.test_loader = DataLoader(self.testset, batch_size=self.args.batch_size, shuffle=False)\n",
        "        self.logged_variables = {\"loss\": [], \"accuracy\": []}\n",
        "        self.examples_seen = 0\n",
        "\n",
        "    def training_step(\n",
        "        self,\n",
        "        imgs: Float[Tensor, \"batch channels height width\"],\n",
        "        labels: Int[Tensor, \"batch\"],\n",
        "    ) -> Float[Tensor, \"\"]:\n",
        "        \"\"\"Perform a gradient update step on a single batch of data.\"\"\"\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        logits = self.model(imgs)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        self.examples_seen += imgs.shape[0]\n",
        "        self.logged_variables[\"loss\"].append(loss.item())\n",
        "        return loss\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def evaluate(self) -> float:\n",
        "        \"\"\"Evaluate the model on the test set and return the accuracy.\"\"\"\n",
        "        self.model.eval()\n",
        "        total_correct, total_samples = 0, 0\n",
        "\n",
        "        for imgs, labels in tqdm(self.test_loader, desc=\"Evaluating\"):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            logits = self.model(imgs)\n",
        "            total_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
        "            total_samples += len(imgs)\n",
        "\n",
        "        accuracy = total_correct / total_samples\n",
        "        self.logged_variables[\"accuracy\"].append(accuracy)\n",
        "        return accuracy\n",
        "\n",
        "    def train(self) -> dict[str, list[float]]:\n",
        "        self.pre_training_setup()\n",
        "\n",
        "        accuracy = self.evaluate()\n",
        "\n",
        "        for epoch in range(self.args.epochs):\n",
        "            self.model.train()\n",
        "\n",
        "            pbar = tqdm(self.train_loader, desc=\"Training\")\n",
        "            for imgs, labels in pbar:\n",
        "                loss = self.training_step(imgs, labels)\n",
        "                pbar.set_postfix(loss=f\"{loss:.3f}\", ex_seen=f\"{self.examples_seen:06}\")\n",
        "\n",
        "            accuracy = self.evaluate()\n",
        "            pbar.set_postfix(loss=f\"{loss:.3f}\", accuracy=f\"{accuracy:.2f}\", ex_seen=f\"{self.examples_seen:06}\")\n",
        "\n",
        "        return self.logged_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3ea324DIT2G"
      },
      "source": [
        "With this class, we can perform feature extraction on our model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxBcxXNTIT2G"
      },
      "outputs": [],
      "source": [
        "args = ResNetFinetuningArgs()\n",
        "trainer = ResNetFinetuner(args)\n",
        "logged_variables = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQugtYVFIT2H"
      },
      "outputs": [],
      "source": [
        "line(\n",
        "    y=[logged_variables[\"loss\"][: 391 * 3 + 1], logged_variables[\"accuracy\"][:4]],\n",
        "    x_max=len(logged_variables[\"loss\"][: 391 * 3 + 1] * args.batch_size),\n",
        "    yaxis2_range=[0, 1],\n",
        "    use_secondary_yaxis=True,\n",
        "    labels={\"x\": \"Examples seen\", \"y1\": \"Cross entropy loss\", \"y2\": \"Test Accuracy\"},\n",
        "    title=\"Feature extraction with ResNet34\",\n",
        "    width=800,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akA6mmYIIT2I"
      },
      "source": [
        "Let's see how well our ResNet performs on the first few inputs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XygwT4h1IT2J"
      },
      "outputs": [],
      "source": [
        "def test_resnet_on_random_input(model: ResNet34, n_inputs: int = 3, seed: int | None = 42):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    indices = np.random.choice(len(cifar_trainset), n_inputs).tolist()\n",
        "    classes = [cifar_trainset.classes[cifar_trainset.targets[i]] for i in indices]\n",
        "    imgs = cifar_trainset.data[indices]\n",
        "    device = next(model.parameters()).device\n",
        "    with t.inference_mode():\n",
        "        x = t.stack(list(map(IMAGENET_TRANSFORM, imgs)))\n",
        "        logits: Tensor = model(x.to(device))\n",
        "    probs = logits.softmax(-1)\n",
        "    if probs.ndim == 1:\n",
        "        probs = probs.unsqueeze(0)\n",
        "    for img, label, prob in zip(imgs, classes, probs):\n",
        "        display(HTML(f\"<h2>Classification probabilities (true class = {label})</h2>\"))\n",
        "        imshow(img, width=200, height=200, margin=0, xaxis_visible=False, yaxis_visible=False)\n",
        "        bar(prob, x=cifar_trainset.classes, width=600, height=400, text_auto=\".2f\", labels={\"x\": \"Class\", \"y\": \"Prob\"})\n",
        "\n",
        "\n",
        "test_resnet_on_random_input(trainer.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTYSwNsNIT2K"
      },
      "source": [
        "## What is Weights and Biases?\n",
        "\n",
        "Weights and Biases is a cloud service that allows you to log data from experiments. Your logged data is shown in graphs during training, and you can easily compare logs across different runs. It also allows you to run **sweeps**, where you can specifiy a distribution over hyperparameters and then start a sequence of test runs which use hyperparameters sampled from this distribution.\n",
        "\n",
        "Before you run any of the code below, you should visit the [Weights and Biases homepage](https://wandb.ai/home), and create your own account.\n",
        "\n",
        "We'll be able to keep the same structure of training loop when using weights and biases, we'll just have to add a few functions. The key functions to know are:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8HjdM0xIT2K"
      },
      "source": [
        "#### `wandb.init`\n",
        "\n",
        "This initialises a training run. It should be called once, at the start of your training loop.\n",
        "\n",
        "A few important arguments are:\n",
        "\n",
        "* `project` - the name of the project where you're sending the new run. For example, this could be `'day3-resnet'` for us. You can have many different runs in each project.\n",
        "* `name` - a display name for this run. By default, if this isn't supplied, wandb generates a random 2-word name for you (e.g. `gentle-sunflower-42`).\n",
        "* `config` - a dictionary containing hyperparameters for this run. If you pass this dictionary, then you can compare different runs' hyperparameters & results in a single table. Alternatively, you can pass a dataclass.\n",
        "\n",
        "#### `wandb.watch`\n",
        "\n",
        "This function tells wandb to watch a model - this means that it will log the gradients and parameters of the model during training. We'll call this function once, after we've created our model. The 3 most important arguments are:\n",
        "\n",
        "* `models` - a module or list of modules (e.g. in our case we might just want to log the weights of the final linear layer, because the others aren't being trained)\n",
        "* `log` - determines what gets tracked, possible values are `'gradients'` (default), `'parameters'` or `'all'`\n",
        "* `log_freq` - the number of batches between each logging step (default is 1000)\n",
        "\n",
        "Why do we log parameters and gradients? Mainly this is [helpful for debugging](https://wandb.ai/wandb_fc/articles/reports/Debugging-Neural-Networks-with-PyTorch-and-W-B-Using-Gradients-and-Visualizations--Vmlldzo1NDQxNTA5), because it helps us identify problems like exploding or vanishing gradients, dead ReLUs, etc.\n",
        "\n",
        "#### `wandb.log`\n",
        "\n",
        "For logging metrics to the wandb dashboard. This is used as `wandb.log(data, step)`, where `step` is an integer (the x-axis on your metric plots) and `data` is a dictionary of metrics (i.e. the keys are metric names, and the values are metric values).\n",
        "\n",
        "#### `wandb.finish`\n",
        "\n",
        "This function should be called at the end of your training loop. It finishes the run, and saves the results to the cloud.\n",
        "\n",
        "If a run terminates early (either because of an error or because you manually terminated it), remember to still run `wandb.finish()` - it will speed things up when you start a new run (otherwise you have to wait for the previous run to be terminated & uploaded)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nVH1I_BIT2L"
      },
      "source": [
        "### Exercise - rewrite training loop with wandb\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵🔵🔵🔵⚪\n",
        ">\n",
        "> You should spend up to 10-25 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "You should now take the training loop from above (i.e. the `ResNetTrainer` class) and rewrite it to use the four `wandb` functions above (in place of the `logged_variables` dictionary, which you can now remove). This will require:\n",
        "\n",
        "- Initializing your run\n",
        "    - Your new `pre_training_setup` method should call `wandb.watch` and `wandb.init` as well as all the stuff it previously did\n",
        "    - For `wandb.init`, you can use the project & name arguments from your new dataclass (see below)\n",
        "    - For `wandb.watch`, be careful of the `log_freq` value - you want to make sure you're logging more than once per epoch\n",
        "- Logging variables to wandb during your run\n",
        "    - i.e. replace updating of `self.logged_variables` with calls to `wandb.log`\n",
        "    - We recommend tracking `self.examples_seen` and passing this as the `step` argument to your logging calls, this way it's easier to compare across different runs with e.g. different batch sizes (more on this later)\n",
        "- Finishing the run\n",
        "    - i.e. calling `wandb.finish` at the end of your training loop\n",
        "\n",
        "This is all you need to do to get wandb working, so the vast majority of the code you write below will be copied and pasted from the previous `ResNetFinetuner` class. We've given you a template for this below, along with a new dataclass. Both the dataclass and the trainer class use inheritance to remove code duplication (e.g. because we don't need to rewrite our `__init__` method, it'll be the same as for `ResNetFinetuner`).\n",
        "\n",
        "Note, we generally recommend keeping progress bars in wandb because they update slightly faster and can give you a better sense of whether something is going wrong in training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkjE6j7NIT2N"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class WandbResNetFinetuningArgs(ResNetFinetuningArgs):\n",
        "    \"\"\"Contains new params for use in wandb.init, as well as all the ResNetFinetuningArgs params.\"\"\"\n",
        "\n",
        "    wandb_project: str | None = \"day3-resnet\"\n",
        "    wandb_name: str | None = None\n",
        "\n",
        "\n",
        "class WandbResNetFinetuner(ResNetFinetuner):\n",
        "    args: WandbResNetFinetuningArgs  # adding this line helps with typechecker!\n",
        "    examples_seen: int = 0  # for tracking the total number of examples seen; used as step argument in wandb.log\n",
        "\n",
        "    def pre_training_setup(self):\n",
        "        \"\"\"Initializes the wandb run using `wandb.init` and `wandb.watch`.\"\"\"\n",
        "        super().pre_training_setup()\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def training_step(\n",
        "        self,\n",
        "        imgs: Float[Tensor, \"batch channels height width\"],\n",
        "        labels: Int[Tensor, \"batch\"],\n",
        "    ) -> Float[Tensor, \"\"]:\n",
        "        \"\"\"Equivalent to ResNetFinetuner.training_step, but logging the loss to wandb.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def evaluate(self) -> float:\n",
        "        \"\"\"Equivalent to ResNetFinetuner.evaluate, but logging the accuracy to wandb.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def train(self) -> None:\n",
        "        \"\"\"Equivalent to ResNetFinetuner.train, but with wandb initialization & calling `wandb.finish` at the end.\"\"\"\n",
        "        self.pre_training_setup()\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "args = WandbResNetFinetuningArgs()\n",
        "trainer = WandbResNetFinetuner(args)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb4KT27BIT2O"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "@dataclass\n",
        "class WandbResNetFinetuningArgs(ResNetFinetuningArgs):\n",
        "    \"\"\"Contains new params for use in wandb.init, as well as all the ResNetFinetuningArgs params.\"\"\"\n",
        "\n",
        "    wandb_project: str | None = \"day3-resnet\"\n",
        "    wandb_name: str | None = None\n",
        "\n",
        "\n",
        "class WandbResNetFinetuner(ResNetFinetuner):\n",
        "    args: WandbResNetFinetuningArgs  # adding this line helps with typechecker!\n",
        "    examples_seen: int = 0  # for tracking the total number of examples seen; used as step argument in wandb.log\n",
        "\n",
        "    def pre_training_setup(self):\n",
        "        \"\"\"Initializes the wandb run using `wandb.init` and `wandb.watch`.\"\"\"\n",
        "        super().pre_training_setup()\n",
        "        wandb.init(project=self.args.wandb_project, name=self.args.wandb_name, config=self.args)\n",
        "        wandb.watch(self.model.out_layers[-1], log=\"all\", log_freq=50)\n",
        "        self.examples_seen = 0\n",
        "\n",
        "    def training_step(\n",
        "        self,\n",
        "        imgs: Float[Tensor, \"batch channels height width\"],\n",
        "        labels: Int[Tensor, \"batch\"],\n",
        "    ) -> Float[Tensor, \"\"]:\n",
        "        \"\"\"Equivalent to ResNetFinetuner.training_step, but logging the loss to wandb.\"\"\"\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        logits = self.model(imgs)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        self.examples_seen += imgs.shape[0]\n",
        "        wandb.log({\"loss\": loss.item()}, step=self.examples_seen)\n",
        "        return loss\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def evaluate(self) -> float:\n",
        "        \"\"\"Equivalent to ResNetFinetuner.evaluate, but logging the accuracy to wandb.\"\"\"\n",
        "        self.model.eval()\n",
        "        total_correct, total_samples = 0, 0\n",
        "\n",
        "        for imgs, labels in tqdm(self.test_loader, desc=\"Evaluating\"):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            logits = self.model(imgs)\n",
        "            total_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
        "            total_samples += len(imgs)\n",
        "\n",
        "        accuracy = total_correct / total_samples\n",
        "        wandb.log({\"accuracy\": accuracy}, step=self.examples_seen)\n",
        "        return accuracy\n",
        "\n",
        "    def train(self) -> None:\n",
        "        \"\"\"Equivalent to ResNetFinetuner.train, but with wandb initialization & calling `wandb.finish` at the end.\"\"\"\n",
        "        self.pre_training_setup()\n",
        "        accuracy = self.evaluate()\n",
        "\n",
        "        for epoch in range(self.args.epochs):\n",
        "            self.model.train()\n",
        "\n",
        "            pbar = tqdm(self.train_loader, desc=\"Training\")\n",
        "            for imgs, labels in pbar:\n",
        "                loss = self.training_step(imgs, labels)\n",
        "                pbar.set_postfix(loss=f\"{loss:.3f}\", ex_seen=f\"{self.examples_seen=:06}\")\n",
        "\n",
        "            accuracy = self.evaluate()\n",
        "            pbar.set_postfix(loss=f\"{loss:.3f}\", accuracy=f\"{accuracy:.2f}\", ex_seen=f\"{self.examples_seen=:06}\")\n",
        "\n",
        "        wandb.finish()\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL5enrhgIT2P"
      },
      "source": [
        "When you run the code for the first time, you'll have to login to Weights and Biases, and paste an API key into VSCode. After this is done, your Weights and Biases training run will start. It'll give you a lot of output text, one line of which will look like:\n",
        "\n",
        "```\n",
        "View run at https://wandb.ai/<USERNAME>/<PROJECT-NAME>/runs/<RUN-NAME>\n",
        "```\n",
        "\n",
        "which you can click on to visit the run page.\n",
        "\n",
        "A nice thing about using Weights and Biases is that you don't need to worry about generating your own plots, that will all be done for you when you visit the page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4nIAIfdIT2P"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/wandb-day3.png\" width=\"900\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryaJk2egIT2Q"
      },
      "source": [
        "### Run & project pages\n",
        "\n",
        "The page you visit will show you a plot of all the variables you've logged, among other things. You can do many things with these plots (e.g. click on the \"edit\" icon for your `train_loss` plot, and apply smoothing & change axis bounds to get a better picture of your loss curve).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/wandb-day3-smoothed.png\" width=\"1000\">\n",
        "\n",
        "The charts are a useful feature of the run page that gets opened when you click on the run page link, but they're not the only feature. You can also navigate to the project page (click on the option to the right of **Projects** on the bar at the top of the Wandb page), and see superimposed plots of all the runs in this project. You can also click on the **Table** icon on the left hand sidebar to see a table of all the runs in this project, which contains useful information (e.g. runtime, the most recent values of any logged variables, etc). However, comparing runs like this becomes especially useful when we start doing hyperparameter search.\n",
        "\n",
        "You can also look at the system tab to inspect things like GPU utilization - this is a good way of checking whether you're saturating your GPU or whether you can afford to increase your batch size more. This tab will be especially useful in the next section, when we move onto distributed training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpzl0XmAIT2R"
      },
      "source": [
        "## Some training heuristics\n",
        "\n",
        "One important skill which every aspiring ML researcher should develop is the ability to play around with hyperparameters and improve a model's training. At times this is more of an art than a science, because frequently rules and heuristics which work most of the time will break down in certain special cases. For example, a common heuristic for number of workers in a `DataLoader` is to set them to be 4 times the number of GPUs you have available (see later sections on distributed computing for more on this). However, setting these values too high can lead to issues where your CPU is bottlenecked by the workers and your epochs take a long time to start - it took me a long time to realize this was happening when I was initially writing these exercises!\n",
        "\n",
        "Sweeping over hyperparameters (which we'll cover shortly) can help remove some of the difficulty here, because you can use sweep methods that guide you towards an optimal set of hyperparameter choices rather than having to manually pick your own. However, here are a few heuristics that you might find useful in a variety of situations:\n",
        "\n",
        "- **Setting batch size**\n",
        "    - Generally you should aim to **saturate your GPU** with data - this means choosing a batch size that's as large as possible without causing memory errors\n",
        "        - You should generally aim for over 70% utilization of your GPU\n",
        "    - Note, this means you should generally try for a larger batch size in your testloader than your trainloader (because evaluation is done without gradients, and so a smaller memory constraint)\n",
        "        - A good starting point is 4x the size, but this will vary between models\n",
        "- **Choosing a learning rate**\n",
        "    - Inspecting loss curves can be a good way of evaluating our learning rate\n",
        "        - If loss is decreasing very slowly & monotonically then this is a sign you should increase the learning rate, whereas very large LR spikes are a sign that you should decrease it\n",
        "    - A common strategy is **warmup**, i.e. having a smaller learning rate for a short period of time at the start of training - we'll do this a lot in later material\n",
        "    - [Jeremy Jordan](https://www.jeremyjordan.me/nn-learning-rate/) has a good blog post on learning rates\n",
        "- **Balancing learning rate and batch size**\n",
        "    - For standard optimizers like `SGD`, it's a good idea to scale the learning rate inversely to the batch size - this way the variance of each parameter step remains the same\n",
        "    - However for **adaptive optimizers** such as `Adam` (where the size of parameter updates automatically adjusts based on the first and second moments of our gradients), this isn't as necessary\n",
        "        - This is why we generally start with default parameters for Adam, and then adjust from there\n",
        "- **Misc. advice**\n",
        "    - If you're training a larger model, it's sometimes a good idea to start with a smaller version of that same model. Good hyperparameters tend to transfer if the architecture & data is the same; the main difference is the larger model may require more regularization to prevent overfitting.\n",
        "    - Bad hyperparameters are usually clearly worse by the end of the first 1-2 epochs. You can manually abort runs that don't look promising (or do it automatically - see discussion of Hyperband in wandb sweeps at the end of this section)\n",
        "    - Overfitting at the start is better than underfitting, because it means your model is capable of learning and has enough capacity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5I7orIdIT2S"
      },
      "source": [
        "## Hyperparameter search\n",
        "\n",
        "One way to search for good hyperparameters is to choose a set of values for each hyperparameter, and then search all combinations of those specific values. This is called **grid search**. The values don't need to be evenly spaced and you can incorporate any knowledge you have about plausible values from similar problems to choose the set of values. Searching the product of sets takes exponential time, so is really only feasible if there are a small number of hyperparameters. I would recommend forgetting about grid search if you have more than 3 hyperparameters, which in deep learning is \"always\".\n",
        "\n",
        "A much better idea is for each hyperparameter, decide on a sampling distribution and then on each trial just sample a random value from that distribution. This is called **random search** and back in 2012, you could get a [publication](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) for this. The diagram below shows the main reason that random search outperforms grid search. Empirically, some hyperparameters matter more than others, and random search benefits from having tried more distinct values in the important dimensions, increasing the chances of finding a \"peak\" between the grid points.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/Fundamentals/main/images/grid_vs_random.png\" width=\"540\">\n",
        "\n",
        "\n",
        "It's worth noting that both of these searches are vastly less efficient than gradient descent at finding optima - imagine if you could only train neural networks by randomly initializing them and checking the loss! Either of these search methods without a dose of human (or eventually AI) judgement is just a great way to turn electricity into a bunch of models that don't perform very well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1gJob71IT2T"
      },
      "source": [
        "## Running hyperparameter sweeps with `wandb`\n",
        "\n",
        "Now we've come to one of the most impressive features of `wandb` - being able to perform hyperparameter sweeps. We do this by defining a `sweep_config` dict which tells us how our hyperparameters will be randomly sampled, then we write a `train` function which takes no arguments and launches a training run with our modified hyperparameters. Lastly we use `wandb.sweep` and `wandb.agent` to run our sweep. We'll go through each step of this below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwxANr4NIT2U"
      },
      "source": [
        "### Sweep config syntax\n",
        "\n",
        "The basic syntax for a sweep config looks like this:\n",
        "\n",
        "```python\n",
        "sweep_config = dict(\n",
        "    method = method, # can be \"grid\", \"random\" or \"bayes\"\n",
        "    metric = dict(\n",
        "        name = metric_name, # name of the metric you're optimising (should be a numeric type logged in `wandb.log`)\n",
        "        goal = goal, # either \"maximize\" or \"minimize\"\n",
        "    )),\n",
        "    parameters = dict(\n",
        "        param_1 = dict(...),\n",
        "        param_2 = dict(...),\n",
        "        ...\n",
        "    ),\n",
        ")\n",
        "```\n",
        "\n",
        "The `method` argument determines how we perform search: `grid` is over all combinations, `random` independently samples each hyperparameter, and `bayes` uses Bayesian optimization to sample hyperparameters. The `metric` dict determines what logged variable we're optimizing, and in what direction. Lastly, `parameters` is a list of parameters we're varying, with each dictionary describing how we want that parameter to be sampled. Possible ways to specify distributions include:\n",
        "\n",
        "```python\n",
        "parameters = dict(\n",
        "    param_1 = dict(values = [...]), # uniformly sample from list of values\n",
        "    param_2 = dict(values = [...], probabilities = [...]), # sample from list with given probabilities\n",
        "    param_3 = dict(min = ..., max = ...), # uniform distribution over [min, max), can either be ints or floats\n",
        "    param_4 = dict(min = ..., max = ..., distribution = \"log_uniform_values\"), # use log-uniform distribution instead\n",
        ")\n",
        "```\n",
        "\n",
        "Note on log uniform distribution - this essentially means we return `value` s.t. `log(value)` is uniformly distributed between `log(min)` and `log(max)`. It can be a useful way to sample hyperparameters which take values in a very large range.\n",
        "\n",
        "You can read more about the syntax [here](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration), but the examples we've given you above should be enough to complete the rest of these exercises.\n",
        "\n",
        "<details>\n",
        "<summary>Note on using YAML files for sweeps (optional)</summary>\n",
        "\n",
        "Rather than using a dictionary, you can alternatively store the `sweep_config` data in a YAML file if you prefer. You will then be able to run a sweep via the following terminal commands:\n",
        "\n",
        "```\n",
        "wandb sweep sweep_config.yaml\n",
        "\n",
        "wandb agent <SWEEP_ID>\n",
        "```\n",
        "\n",
        "where `SWEEP_ID` is the value returned from the first terminal command. You will also need to add another line to the YAML file, specifying the program to be run. For instance, your YAML file might start like this:\n",
        "\n",
        "```yaml\n",
        "program: train.py\n",
        "method: random\n",
        "metric:\n",
        "    name: test_accuracy\n",
        "    goal: maximize\n",
        "```\n",
        "\n",
        "For more, see [this link](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration).\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WhSIXx-IT2V"
      },
      "source": [
        "### Exercise - define a sweep config & update `args`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 10-20 minutes on this exercise.\n",
        "> Learning how to use wandb for sweeps is very useful, so make sure you understand all parts of this code.\n",
        "> ```\n",
        "\n",
        "You should define a dictionary `sweep_config`, which has the following rules for hyperparameter sweeps:\n",
        "\n",
        "* Hyperparameters are chosen **randomly**, according to the distributions given in the dictionary\n",
        "* Your goal is to **maximize** the **accuracy** metric\n",
        "* The hyperparameters you vary are:\n",
        "    * Learning rate - a log-uniform distribution between 1e-4 and 1e-1\n",
        "    * Batch size - randomly chosen from (32, 64, 128, 256)\n",
        "    * Weight decay - with 50% probability set to 0, and with 50% probability log-uniform between 1e-4 and 1e-2\n",
        "\n",
        "Each time your sweep launches a training run it will sample the specified hyperparameters and give them to you in the form of a dictionary (keys = hyperparameter names, values = sampled values), so we also need a way to update `args` based on these sampled values. You should fill in the `update_args` function below, to do exactly this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcODLntVIT2W"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE - fill `sweep_config` so it has the requested behaviour\n",
        "sweep_config = dict(\n",
        "    method = ...,\n",
        "    metric = ...,\n",
        "    parameters = ...,\n",
        ")\n",
        "\n",
        "\n",
        "def update_args(args: WandbResNetFinetuningArgs, sampled_parameters: dict) -> WandbResNetFinetuningArgs:\n",
        "    \"\"\"\n",
        "    Returns a new args object with modified values. The dictionary `sampled_parameters` will have the same keys as\n",
        "    your `sweep_config[\"parameters\"]` dict, and values equal to the sampled values of those hyperparameters.\n",
        "    \"\"\"\n",
        "    assert sampled_parameters.keys() == sweep_config[\"parameters\"].keys()\n",
        "\n",
        "    # YOUR CODE HERE - update `args` based on `sampled_parameters`\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "tests.test_sweep_config(sweep_config)\n",
        "tests.test_update_args(update_args, sweep_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkqr5w5IT2W"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I'm not sure how to implement the weight decay distribution that was requested.</summary>\n",
        "\n",
        "The easiest option is to include 2 parameters: one is a boolean and determines whether to use weight decay, one is log-uniform and gives you the value in the cases where it's non-zero. Both parameters are used to set the final value in `args`.\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "sweep_config = dict(\n",
        "    method=\"random\",\n",
        "    metric=dict(name=\"accuracy\", goal=\"maximize\"),\n",
        "    parameters=dict(\n",
        "        lr=dict(min=1e-4, max=1e-1, distribution=\"log_uniform_values\"),\n",
        "        batch_size=dict(values=[32, 64, 128, 256]),\n",
        "        weight_decay=dict(min=1e-4, max=1e-2, distribution=\"log_uniform_values\"),\n",
        "        weight_decay_bool=dict(values=[True, False]),\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "def update_args(args: WandbResNetFinetuningArgs, sampled_parameters: dict) -> WandbResNetFinetuningArgs:\n",
        "    \"\"\"\n",
        "    Returns a new args object with modified values. The dictionary `sampled_parameters` will have the same keys as\n",
        "    your `sweep_config[\"parameters\"]` dict, and values equal to the sampled values of those hyperparameters.\n",
        "    \"\"\"\n",
        "    assert sampled_parameters.keys() == sweep_config[\"parameters\"].keys()\n",
        "    args.learning_rate = sampled_parameters[\"lr\"]\n",
        "    args.batch_size = sampled_parameters[\"batch_size\"]\n",
        "    args.weight_decay = sampled_parameters[\"weight_decay\"] if sampled_parameters[\"weight_decay_bool\"] else 0.0\n",
        "    return args\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwFGfbjtIT2X"
      },
      "source": [
        "Now we've done this, we'll define a `train` function that takes no arguments and launches a training run with our modified hyperparameters. This is done in the following way:\n",
        "\n",
        "- The train function calls `wandb.init`\n",
        "- Our sampled hyperparameters are now available in `wandb.config`, so we use this object to update `args`\n",
        "- We then launch a training run based on these new hyperparameters\n",
        "\n",
        "The line `sweep_id = wandb.sweep(...)` initializes a hyperparameter sweep (giving it an ID) and the line `wandb.agent(...)` starts an agent that runs the training script `train` 3 times, with different randomly sampled sets of hyperparameters each time.\n",
        "\n",
        "Note that we pass `reinit=False` into our `wandb.init` call - this is so we ignore the second `wandb.init` call that takes place in our pretraining setup when we run `trainer.train()` (so we can avoid the hassle of having to rewrite this method to remove that line)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwSxpKEZIT2Y"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    # Define args & initialize wandb\n",
        "    args = WandbResNetFinetuningArgs()\n",
        "    wandb.init(project=args.wandb_project, name=args.wandb_name, reinit=False)\n",
        "\n",
        "    # After initializing wandb, we can update args using `wandb.config`\n",
        "    args = update_args(args, wandb.config)\n",
        "\n",
        "    # Train the model with these new hyperparameters (the second `wandb.init` call will be ignored)\n",
        "    trainer = WandbResNetFinetuner(args)\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project=\"day3-resnet-sweep\")\n",
        "wandb.agent(sweep_id=sweep_id, function=train, count=3)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OgiL7C8IT2Y"
      },
      "source": [
        "When you run this code, you should click on the link which looks like:\n",
        "\n",
        "```\n",
        "View sweep at https://wandb.ai/<USERNAME>/<PROJECT-NAME>/sweeps/<SWEEP-NAME>\n",
        "```\n",
        "\n",
        "This link will bring you to a page comparing each of your sweeps. You'll be able to see overlaid graphs of each of their training loss and test accuracy, as well as a bunch of other cool things like:\n",
        "\n",
        "* Bar charts of the [importance](https://docs.wandb.ai/ref/app/features/panels/parameter-importance) (and correlation) of each hyperparameter wrt the target metric. Note that only looking at the correlation could be misleading - something can have a correlation of 1, but still have a very small effect on the metric.\n",
        "* A [parallel coordinates plot](https://docs.wandb.ai/ref/app/features/panels/parallel-coordinates), which summarises the relationship between the hyperparameters in your config and the model metric you're optimising.\n",
        "\n",
        "What can you infer from these results? Are there any hyperparameters which are especially correlated / anticorrelated with the target metric? Are there any results which suggest the model is being undertrained?\n",
        "\n",
        "You might also want to play around with Bayesian hyperparameter search, if you get the time! Note that wandb sweeps also offer [early termination](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration/#early_terminate) of runs that don't look promising, based on the [Hyperband](https://www.jmlr.org/papers/volume18/16-558/16-558.pdf) algorithm.\n",
        "\n",
        "To conclude - `wandb` is an incredibly useful tool when training models, and you should find yourself using it a fair amount throughout this program. You can always return to this page of exercises if you forget how any part of it works!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a22v6I2KIT2Z"
      },
      "source": [
        "# 3️⃣ Distributed Training\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Understand the different kinds of parallelization used in deep learning (data, pipeline, tensor)\n",
        "> * Understand how primitive operations in `torch.distributed` work, and how they come together to enable distributed training\n",
        "> * Launch and benchmark your own distributed training runs, to train your implementation of `ResNet34` from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hYc2Gh8IT2Z"
      },
      "source": [
        "## Intro to distributed training\n",
        "\n",
        "Distributed training is a model training paradigm that involves spreading training workload across multiple worker nodes, therefore significantly improving the speed of training and model accuracy. While distributed training can be used for any type of ML model training, it is most beneficial to use it for large models and compute demanding tasks as deep learning.\n",
        "\n",
        "There are 2 main families of distributed training methods: **data parallelism** and **model parallelism**. In data parallelism, we split batches of data across different processes, run forward & backward passes on each separately, and accumulate the gradients to update the model parameters. In model parallelism, the model is segmented into different parts that can run concurrently in different nodes, and each one runs on the same data. Model parallelism further splits into horizontal and vertical parallelism depending on whether we're splitting the model up into sequential or parallel parts. Most often horizontal parallelism is called **tensor parallelism** (because it involves splitting up the weights in a single layer across multiple GPUs, into what we commonly call **sharded weights**), and vertical parallelism is called **pipeline parallelism**.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/parallelism.png\" width=\"1100\">\n",
        "\n",
        "Data & model parallelism are both widely used, and can be more or less appropriate in different circumstances (e.g. some kind of model parallelism is necessary when your model is too large to fit on a single GPU). However it is possible to create hybrid forms of parallelism by combining these; this is especially common when training large models like current SOTA LLMs. In these exercises, we'll focus on just data parallelism, although we'll suggest a few bonus exercises that explore model parallelism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqLjD8LjIT2a"
      },
      "source": [
        "### Summary of exercises\n",
        "\n",
        "The exercises below will take you through **data parallelism**. You'll start by learning how to use the basic send and receive functions in PyTorch's distributed training module `torch.distributed` to transfer tensors between different processes (and different GPUs). Note that **you'll need multiple GPUs for these exercises** - we've included instructions in a dropdown below.\n",
        "\n",
        "<details>\n",
        "<summary>Getting multiple GPUs</summary>\n",
        "\n",
        "The instructions for booting up a machine from vastai can already be found on the Streamlit homepage (i.e. navigate to \"Home\" on the sidebar, then to the section \"Virtual Machines\"). The only extra thing you'll need to do here is filter for an appropriate machine type.\n",
        "\n",
        "We recommend filtering for \"Disk Space To Allocate\" (i.e. the primary slider on the top of the filter menu) of at least 40GB, not for the model (which is actually quite small) but for installing the ARENA dependencies. You should also filter for number of GPUs: we recommend 4x or 8x. You can do this using the options menu at the very top of the list of machines. Lastly, we recommend filtering for a decent PCIE Bandwidth (e.g. at least 20GB/s) - this is important for efficient gradient sychronization between GPUs. We're training a small model today: approx 22m parameters, which translates to ~88MB total size of weights, and so we'll transfer 88MB of data between GPUs per process (since we're transferring the model's gradients, which have the same size as the weights). We don't want this to be a bottleneck, which is why we should filter for this bandwidth.\n",
        "\n",
        "Once you've filtered for this, we recommend picking an RTX 3090 or 4090 machine. These won't be as powerful as an A100, but the purpose today is more to illustrate the basic ideas behind distributed training than to push your model training to its limits. Note that if you were using an A100 then you should filter for a high NVLink Bandwidth rather than PCIE (since A100s use NVLink instead of PCIE).\n",
        "\n",
        "</details>\n",
        "\n",
        "Once you've done this, you'll use those 2 primitive point-to-point functions to build up some more advanced functions: `broadcast` (which gets a tensor from one process to all others), `gather` (which gathers all tensors from different devices to a single device) and `all_reduce` (which combines both `broadcast` and `gather` to make aggregate tensor values across all processes). These functions (`all_reduce` in particular) are key parts of how distributed computing works.\n",
        "\n",
        "Lastly, you'll learn how to use these functions to build a distributed training loop, which will be adapted from the `ResNetTrainer` code from your previous exercises. We also explain how you can use `DistributedDataParallel` to abstract away these low-level operations, which you might find useful later on (although you will benefit from building these components up from scratch, and understanding how they work under the hood)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Tl_jtpIT2b"
      },
      "source": [
        "### Running these exercises\n",
        "\n",
        "> These exercises can't all be run in a notebook or Colab, because distributed training typically requires spawning multiple processes and Jupyter notebooks run in a single interactive process - they're not designed for this kind of use-case.\n",
        "\n",
        "You have 2 different options:\n",
        "\n",
        "1. **Do everything in a Python file** (either `# %%`-separated cells or [execute on selection](https://stackoverflow.com/questions/38952053/how-can-i-run-text-selected-in-the-active-editor-in-vs-codes-integrated-termina)), but make sure to wrap any execution code in `if __name__ == \"__main__\":` blocks. This makes sure that when you launch multiple processes they don't recursively launch their own processes, and they'll only execute the code you want them to.\n",
        "2. **Write your functions in a Python file, then import & run them in a notebook**. For example in the example code below, you could define the `send_receive` function in a Python file, then import this function & pass it into the `mp.spawn()` call.\n",
        "\n",
        "In either case, make sure when you run `mp.spawn` you're passing in the most updated version of your function. This means saving the Python file after you make changes, and also using something like `importlib.reload()` if you're running the code in a notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_GCLBPOIT2b"
      },
      "outputs": [],
      "source": [
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "assert not IN_COLAB, \"Should be doing these exercises in VS Code\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNSn4GK3IT2c"
      },
      "source": [
        "## Basic send & receiving\n",
        "\n",
        "The code below is a simplified example that demonstrates distributed communication between multiple processes.\n",
        "\n",
        "At the highest level, `mp.spawn()` launches multiple worker processes, each with a unique rank. For each worker, we create a new Python interpreter (called a \"child process\") which will execute the function passed to `mp.spawn` (which in this case is `send_receive`). The function has to have the type signature `fn(rank, *args)` where `args` is the tuple we pass into `mp.spawn()`. The total number of processes is determined by `world_size`. Note that this isn't the same as the number of GPUs - in fact, in the code below we've not moved any of our data to GPUs, we're just using the distributed API to sync data across multiple processes. We'll introduce GPUs in the code below this!\n",
        "\n",
        "We require the environment variables `MASTER_ADDR` and `MASTER_PORT` to be set before launching & communicating between processes. The former specifies the IP address or hostname of the machine that will act as the central coordinator (the \"master\" node) for setting up and managing the distributed environment, while the latter specifies the port number that the master node will use for communication. In our case we're running all our processes from a single node, so all we need is for this to be an unused port on our machine.\n",
        "\n",
        "Now, breaking down the `send_receive` function line by line:\n",
        "\n",
        "- `dist.init_process_group` initializes each process with a common address and port, and a communication backend. It also gives each process a unique rank, so they know who is sending & receiving data.\n",
        "- If the function is being run by rank 0, then we create a tensor of zeros and send it using `dist.send`.\n",
        "- If the function is being run by rank 1, then we create a tensor of ones and wait to receive a tensor from rank 0 using `dist.recv`. This will overwrite the data in the original tensor that we created, i.e. so we're just left with a tensor of zeros.\n",
        "- `dist.destroy_process_group()` is called at the end of the function to destroy the process group and release resources.\n",
        "\n",
        "The functions `dist.send` and `dist.recv` are the basic primitives for point-to-point communication between processes (we'll look at the primitives for collective communication later on). Each `recv` for a given source process `src` will wait until it receives a `send` from that source to continue, and likewise each `send` to a given destination process `dst` will wait until it receives a `recv` from that process to continue. We call these **blocking operations** (later on we'll look at non-blocking operations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbgGaO3cIT2c"
      },
      "outputs": [],
      "source": [
        "WORLD_SIZE = t.cuda.device_count()\n",
        "\n",
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = \"12345\"\n",
        "\n",
        "\n",
        "def send_receive(rank, world_size):\n",
        "    dist.init_process_group(backend=\"gloo\", rank=rank, world_size=world_size)\n",
        "\n",
        "    if rank == 0:\n",
        "        # Send tensor to rank 1\n",
        "        sending_tensor = t.zeros(1)\n",
        "        print(f\"{rank=}, sending {sending_tensor=}\")\n",
        "        dist.send(tensor=sending_tensor, dst=1)\n",
        "    elif rank == 1:\n",
        "        # Receive tensor from rank 0\n",
        "        received_tensor = t.ones(1)\n",
        "        print(f\"{rank=}, creating {received_tensor=}\")\n",
        "        dist.recv(received_tensor, src=0)  # this line overwrites the tensor's data with our `sending_tensor`\n",
        "        print(f\"{rank=}, received {received_tensor=}\")\n",
        "\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    world_size = 2  # simulate 2 processes\n",
        "    mp.spawn(send_receive, args=(world_size,), nprocs=world_size, join=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60KJ-W7nIT2d"
      },
      "source": [
        "Now, let's adapt this toy example to work with our multiple GPUs! You can check how many GPUs you have access to using `torch.cuda.device_count()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beOzv4udIT2d"
      },
      "outputs": [],
      "source": [
        "assert t.cuda.is_available()\n",
        "assert t.cuda.device_count() > 1, \"This example requires at least 2 GPUs per machine\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA5sUoyxIT2e"
      },
      "source": [
        "Before writing our new code, let's first return to the `backend` argument for `dist.init_process_group`. There are 3 main backends for distributed training: MPI, GLOO and NCCL. The first two are more general-purpose and support both CPU & GPU tensor communication, while NCCL is a GPU-only protocol optimized specifically for NVIDIA GPUs. It provides better bandwidth and lower latency for GPU-GPU communication, and so we'll be using it for subsequent exercises.\n",
        "\n",
        "When sending & receiving tensors between GPUs with a NCCL backend, there are 3 important things to remember:\n",
        "\n",
        "1. Send & received tensors should be of the same datatype.\n",
        "2. Tensors need to be moved to the GPU before sending or receiving.\n",
        "3. No two processes should be using the same GPU.\n",
        "\n",
        "Because of this third point, each process `rank` will be using the GPU with index `rank` - hence we'll sometimes refer to the process rank and its corresponding GPU index interchangeably. However it's worth emphasizing that this only applies to our specific data parallelism & NCCL backend example, and so this correspondence doesn't have to exist in general.\n",
        "\n",
        "The code below is a slightly modified version of the prior code; all we're doing is changing the backend to NCCL & moving the tensors to the appropriate device before sending or receiving.\n",
        "\n",
        "Note - if at any point during this section you get errors related to the socket, then you can kill the processes by running `kill -9 <pid>` where `<pid>` is the process ID. If the process ID isn't given in the error message, you can find it using `lsof -i :<port>` where `<port>` is the port number specified in `os.environ[\"MASTER_PORT\"]` (note you might have to `sudo apt-get install lsof` before you can run this). If your code is still failing, try changing the port in `os.environ[\"MASTER_PORT\"]` and running it again.\n",
        "\n",
        "<!-- Note - an alternative to explicitly defining the device here is to run the line `torch.cuda.set_device(rank)`, then code like `tensor.cuda()` will automatically send the tensor to the correct device. Which one you use is a matter of preference, however for the solutions & demo code we'll stick with the explicit device definition. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DwpbRWtIT2f"
      },
      "outputs": [],
      "source": [
        "def send_receive_nccl(rank, world_size):\n",
        "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "    device = t.device(f\"cuda:{rank}\")\n",
        "\n",
        "    if rank == 0:\n",
        "        # Create a tensor, send it to rank 1\n",
        "        sending_tensor = t.tensor([rank], device=device)\n",
        "        print(f\"{rank=}, {device=}, sending {sending_tensor=}\")\n",
        "        dist.send(sending_tensor, dst=1)  # Send tensor to CPU before sending\n",
        "    elif rank == 1:\n",
        "        # Receive tensor from rank 0 (it needs to be on the CPU before receiving)\n",
        "        received_tensor = t.tensor([rank], device=device)\n",
        "        print(f\"{rank=}, {device=}, creating {received_tensor=}\")\n",
        "        dist.recv(received_tensor, src=0)  # this line overwrites the tensor's data with our `sending_tensor`\n",
        "        print(f\"{rank=}, {device=}, received {received_tensor=}\")\n",
        "\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    world_size = 2  # simulate 2 processes\n",
        "    mp.spawn(send_receive_nccl, args=(world_size,), nprocs=world_size, join=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LHoGMMcIT2f"
      },
      "source": [
        "## Collective communication primitives\n",
        "\n",
        "We'll now move from basic point-to-point communication to **collective communication**. This refers to operations that synchronize data across multiple processes, rather than just between a single sender and receiver. There are 3 important kinds of collective communication functions:\n",
        "\n",
        "- **Broadcast**: send a tensor from one process to all other processes\n",
        "- **Gather**: collect tensors from all processes and concatenates them into a single tensor\n",
        "- **Reduce**: like gather, but perform a reduction operation (e.g. sum, mean) rather than concatenation\n",
        "\n",
        "The latter 2 functions have different variants depending on whether you want the final result to be in just a single destination process or in all of them: for example `dist.gather` will gather data to a single destination process, while `dist.all_gather` will make sure every process ends up with all the data.\n",
        "\n",
        "The functions we're most interested in building are `broadcast` and `all_reduce` - the former for making sure all processes have the same initial model parameters, and the latter for aggregating gradients across all processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_O9jXZaIT2g"
      },
      "source": [
        "### Exercise - implement `broadcast`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 10-20 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Below, you should implement `broadcast`. If you have tensor $T_i$ on process $i$ for each index, then after running this function you should have $T_s$ on all processes, where $s$ is the source process. If you're confused, you can see exactly what is expected of you by reading the test code in `tests.py`. Again, remember that you should be running tests either from the command line or in the Python interactive terminal, not in a notebook cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ_hV5ExIT2h"
      },
      "outputs": [],
      "source": [
        "def broadcast(tensor: Tensor, rank: int, world_size: int, src: int = 0):\n",
        "    \"\"\"\n",
        "    Broadcast averaged gradients from rank 0 to all other ranks.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    tests.test_broadcast(broadcast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM6qFJFlIT2i"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def broadcast(tensor: Tensor, rank: int, world_size: int, src: int = 0):\n",
        "    \"\"\"\n",
        "    Broadcast averaged gradients from rank 0 to all other ranks.\n",
        "    \"\"\"\n",
        "    if rank == src:\n",
        "        for other_rank in range(world_size):\n",
        "            if other_rank != src:\n",
        "                dist.send(tensor, dst=other_rank)\n",
        "    else:\n",
        "        received_tensor = t.zeros_like(tensor)\n",
        "        dist.recv(received_tensor, src=src)\n",
        "        tensor.copy_(received_tensor)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w2r4HHZIT2j"
      },
      "source": [
        "### Exercise - implement `all_reduce`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 10-20 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "You should now implement `reduce` and `all_reduce`. The former will aggregate the tensors at some destination process (either sum or mean), and the latter will do the same but then broadcast the result to all processes.\n",
        "\n",
        "Note, more complicated allreduce algorithms exist than this naive one, and you'll be able to look at some of them in the bonus material."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkh_GkQmIT2j"
      },
      "outputs": [],
      "source": [
        "def reduce(tensor, rank, world_size, dst=0, op: Literal[\"sum\", \"mean\"] = \"sum\"):\n",
        "    \"\"\"\n",
        "    Reduces gradients to rank `dst`, so this process contains the sum or mean of all tensors across processes.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "def all_reduce(tensor, rank, world_size, op: Literal[\"sum\", \"mean\"] = \"sum\"):\n",
        "    \"\"\"\n",
        "    Allreduce the tensor across all ranks, using 0 as the initial gathering rank.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    tests.test_reduce(reduce)\n",
        "    tests.test_all_reduce(all_reduce)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iaMmyYTIT2k"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def reduce(tensor, rank, world_size, dst=0, op: Literal[\"sum\", \"mean\"] = \"sum\"):\n",
        "    \"\"\"\n",
        "    Reduces gradients to rank `dst`, so this process contains the sum or mean of all tensors across processes.\n",
        "    \"\"\"\n",
        "    if rank != dst:\n",
        "        dist.send(tensor, dst=dst)\n",
        "    else:\n",
        "        for other_rank in range(world_size):\n",
        "            if other_rank != dst:\n",
        "                received_tensor = t.zeros_like(tensor)\n",
        "                dist.recv(received_tensor, src=other_rank)\n",
        "                tensor += received_tensor\n",
        "    if op == \"mean\":\n",
        "        tensor /= world_size\n",
        "\n",
        "\n",
        "def all_reduce(tensor, rank, world_size, op: Literal[\"sum\", \"mean\"] = \"sum\"):\n",
        "    \"\"\"\n",
        "    Allreduce the tensor across all ranks, using 0 as the initial gathering rank.\n",
        "    \"\"\"\n",
        "    reduce(tensor, rank, world_size, dst=0, op=op)\n",
        "    broadcast(tensor, rank, world_size, src=0)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJHANO9XIT2l"
      },
      "source": [
        "<!-- <pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running reduce on dst=0, with initial tensors: [0, 0], [1, 2], [10, 20]\n",
        "Rank 1, op='sum', expected non-reduced tensor([1., 2.]), got tensor([1., 2.])\n",
        "Rank 1, op='mean', expected non-reduced tensor([0.3333, 0.6667]), got tensor([0.3333, 0.6667])\n",
        "Rank 0, op='sum', expected reduced tensor([11., 22.]), got tensor([11., 22.])\n",
        "Rank 2, op='sum', expected non-reduced tensor([10., 20.]), got tensor([10., 20.])\n",
        "Rank 0, op='mean', expected reduced tensor([3.6667, 7.3333]), got tensor([3.6667, 7.3333])\n",
        "Rank 2, op='mean', expected non-reduced tensor([3.3333, 6.6667]), got tensor([3.3333, 6.6667])\n",
        "All tests in `test_reduce` passed!\n",
        "\n",
        "Running all_reduce, with initial tensors: [0, 0], [1, 2], [10, 20]\n",
        "Rank 1, op='sum', expected non-reduced tensor([11., 22.]), got tensor([11., 22.])\n",
        "Rank 2, op='sum', expected non-reduced tensor([11., 22.]), got tensor([11., 22.])\n",
        "Rank 0, op='sum', expected non-reduced tensor([11., 22.]), got tensor([11., 22.])\n",
        "Rank 1, op='mean', expected non-reduced tensor([3.6667, 7.3333]), got tensor([3.6667, 7.3333])\n",
        "Rank 2, op='mean', expected non-reduced tensor([3.6667, 7.3333]), got tensor([3.6667, 7.3333])\n",
        "Rank 0, op='mean', expected non-reduced tensor([3.6667, 7.3333]), got tensor([3.6667, 7.3333])\n",
        "All tests in `test_all_reduce` passed!</pre> -->\n",
        "\n",
        "Once you've passed these tests, you can run the code below to see how this works for a toy example of model training. In this case our model just has a single parameter and we're performing gradient descent using the squared error between its parameters and the input data as our loss function (in other words we're training the model's parameters to equal the mean of the input data).\n",
        "\n",
        "The data in the example below is the same as the rank index, i.e. `r = 0, 1`. For initial parameter `x = 2` this gives us errors of `(x-r).pow(2) = 4, 2` respectively, and gradients of `2x(x-r) = 8, 4`. Averaging these gives us a gradient of `6`, so after a single optimization step with learning rate `lr=0.1` we get our gradients changing to `2.0 - 0.6 = 1.4`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5GCXTRHIT2m"
      },
      "outputs": [],
      "source": [
        "class SimpleModel(t.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.param = t.nn.Parameter(t.tensor([2.0]))\n",
        "\n",
        "    def forward(self, x: t.Tensor):\n",
        "        return x - self.param\n",
        "\n",
        "\n",
        "def run_simple_model(rank, world_size):\n",
        "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "    device = t.device(f\"cuda:{rank}\")\n",
        "    model = SimpleModel().to(device)  # Move the model to the device corresponding to this process\n",
        "    optimizer = t.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "    input = t.tensor([rank], dtype=t.float32, device=device)\n",
        "    output = model(input)\n",
        "    loss = output.pow(2).sum()\n",
        "    loss.backward()  # Each rank has separate gradients at this point\n",
        "\n",
        "    print(f\"Rank {rank}, before all_reduce, grads: {model.param.grad=}\")\n",
        "    all_reduce(model.param.grad, rank, world_size)  # Synchronize gradients\n",
        "    print(f\"Rank {rank}, after all_reduce, synced grads (summed over processes): {model.param.grad=}\")\n",
        "\n",
        "    optimizer.step()  # Step with the optimizer (this will update all models the same way)\n",
        "    print(f\"Rank {rank}, new param: {model.param.data}\")\n",
        "\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "\n",
        "world_size = 2\n",
        "mp.spawn(run_simple_model, args=(world_size,), nprocs=world_size, join=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kabMX_lgIT2m"
      },
      "source": [
        "## Full training loop\n",
        "\n",
        "We'll now use everything we've learned to put together a full training loop! Rather than finetuning it which we've been doing so far, you'll be training your resnet from scratch (although still using the same CIFAR10 dataset). We've given you a function `get_untrained_resnet` which uses the `ResNet34` class from yesterday's solutions, although you're encouraged to replace this function with your implementation if you've completed those exercises.\n",
        "\n",
        "There are 4 key elements you'll need to change from the non-distributed version of training:\n",
        "\n",
        "1. **Weight broadcasting at initialization**\n",
        "    - For each process you'll need to initialize your model and move it onto the corresponding GPU, but you also want to make sure each process is working with the same model. You do this by **broadcasting weights in the `__init__` method**, e.g. using process 0 as the shared source process.\n",
        "    - Note - you may find you'll have to brodcast `param.data` rather than `param` when you iterate through the model's parameters, because broadcasting only works for tensors not parameters. Parameters are a special class wrapping around and extending standard PyTorch tensors - we'll look at this in more detail tomorrow!\n",
        "2. **Dataloader sampling at each epoch**\n",
        "    - Distributed training works by splitting each batch of data across all the running processes, and so we need to implement this by splitting each batch randomly across our GPUs.\n",
        "    - Some sample code for this is given below - we recommend you start with this (although you're welcome to play around with some of the parameters here like `num_workers` and `pin_memory`).\n",
        "3. **Parameter syncing after each training step**\n",
        "    - After each `loss.backward()` call but before stepping with the optimizer, you'll need to use `all_reduce` to sync gradients across each parameter in the model.\n",
        "    - Just like in the example we gave above, calling `all_reduce` on `param.grad` should work, because `.grad` is a standard PyTorch tensor.\n",
        "4. **Aggregating correct predictions after each evaluation step**\\*\n",
        "    - We can also split the evaluation step across GPUs - we use `all_reduce` at the end of the `evaluate` method to sum the total number of correct predictions across GPUs.\n",
        "    - This is optional, and often it's not implemented because the evaluation step isn't a bottleneck compared to training, however we've included it in our solutions for completeness.\n",
        "\n",
        "<details>\n",
        "<summary>Dataloader sampling example code</summary>\n",
        "\n",
        "```python\n",
        "self.train_sampler = t.utils.data.DistributedSampler(\n",
        "    self.trainset,\n",
        "    num_replicas=args.world_size, # we'll divide each batch up into this many random sub-batches\n",
        "    rank=self.rank, # this determines which sub-batch this process gets\n",
        ")\n",
        "self.train_loader = t.utils.data.DataLoader(\n",
        "    self.trainset,\n",
        "    self.args.batch_size, # this is the sub-batch size, i.e. the batch size that each GPU gets\n",
        "    sampler=self.train_sampler,\n",
        "    num_workers=2,  # setting this low so as not to risk bottlenecking CPU resources\n",
        "    pin_memory=True,  # this can improve data transfer speed between CPU and GPU\n",
        ")\n",
        "\n",
        "for epoch in range(self.args.epochs):\n",
        "self.train_sampler.set_epoch(epoch)\n",
        "for imgs, labels in self.train_loader:\n",
        "    ...\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZOP1W-fIT2n"
      },
      "source": [
        "### Exercise - complete `DistResNetTrainer`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴🔴🔴\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 30-60 minutes on this exercise.\n",
        "> If you get stuck on specific bits, you're encouraged to look at the solutions for guidance.\n",
        "> ```\n",
        "\n",
        "We've given you the function `dist_train_resnet_from_scratch` which you'll be able to pass into `mp.spawn` just like the examples above, and we've given you a very light template for the `DistResNetTrainer` class which you should fill in. Your job is just to make the 4 adjustments described above. We recommend not using inheritance for this, because there are lots of minor modifications you'll need to make to the previous code and so you won't be reducing code duplication by very much.\n",
        "\n",
        "A few last tips before we get started:\n",
        "\n",
        "- If your code is running slowly, we recommend you also `wandb.log` the duration of each stage of the training step from the rank 0 process (fwd pass, bwd pass, and `all_reduce` for parameter syncing), as well as logging the duration of the training & evaluation phases across the epoch. These kinds of logs are generally very helpful for debugging slow code.\n",
        "- Since running this code won't directly return your model as output, it's good practice to save your model at the end of training using `torch.save`.\n",
        "- We recommend you increment `examples_seen` by the total number of examples across processes, i.e. `len(input) * world_size`. This will help when you're comparing across different runs with different world sizes (it's convenient for them to have a consistent x-axis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhPL4JGaIT2o"
      },
      "outputs": [],
      "source": [
        "def get_untrained_resnet(n_classes: int) -> ResNet34:\n",
        "    \"\"\"Gets untrained resnet using code from part2_cnns.solutions (you can replace this with your implementation).\"\"\"\n",
        "    resnet = ResNet34()\n",
        "    resnet.out_layers[-1] = Linear(resnet.out_features_per_group[-1], n_classes)\n",
        "    return resnet\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DistResNetTrainingArgs(WandbResNetFinetuningArgs):\n",
        "    world_size: int = 1\n",
        "    wandb_project: str | None = \"day3-resnet-dist-training\"\n",
        "\n",
        "\n",
        "class DistResNetTrainer:\n",
        "    args: DistResNetTrainingArgs\n",
        "\n",
        "    def __init__(self, args: DistResNetTrainingArgs, rank: int):\n",
        "        self.args = args\n",
        "        self.rank = rank\n",
        "        self.device = t.device(f\"cuda:{rank}\")\n",
        "\n",
        "    def pre_training_setup(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def training_step(self, imgs: Tensor, labels: Tensor) -> Tensor:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def evaluate(self) -> float:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def train(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "def dist_train_resnet_from_scratch(rank, world_size):\n",
        "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
        "    args = DistResNetTrainingArgs(world_size=world_size)\n",
        "    trainer = DistResNetTrainer(args, rank)\n",
        "    trainer.train()\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    world_size = t.cuda.device_count()\n",
        "    mp.spawn(dist_train_resnet_from_scratch, args=(world_size,), nprocs=world_size, join=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiMdqzrgIT2o"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def get_untrained_resnet(n_classes: int) -> ResNet34:\n",
        "    \"\"\"Gets untrained resnet using code from part2_cnns.solutions (you can replace this with your implementation).\"\"\"\n",
        "    resnet = ResNet34()\n",
        "    resnet.out_layers[-1] = Linear(resnet.out_features_per_group[-1], n_classes)\n",
        "    return resnet\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DistResNetTrainingArgs(WandbResNetFinetuningArgs):\n",
        "    world_size: int = 1\n",
        "    wandb_project: str | None = \"day3-resnet-dist-training\"\n",
        "\n",
        "\n",
        "class DistResNetTrainer:\n",
        "    args: DistResNetTrainingArgs\n",
        "\n",
        "    def __init__(self, args: DistResNetTrainingArgs, rank: int):\n",
        "        self.args = args\n",
        "        self.rank = rank\n",
        "        self.device = t.device(f\"cuda:{rank}\")\n",
        "\n",
        "    def pre_training_setup(self):\n",
        "        self.model = get_untrained_resnet(self.args.n_classes).to(self.device)\n",
        "        if self.args.world_size > 1:\n",
        "            for param in self.model.parameters():\n",
        "                broadcast(param.data, self.rank, self.args.world_size, src=0)\n",
        "                # dist.broadcast(param.data, src=0)\n",
        "\n",
        "        self.optimizer = t.optim.AdamW(\n",
        "            self.model.parameters(), lr=self.args.learning_rate, weight_decay=self.args.weight_decay\n",
        "        )\n",
        "\n",
        "        self.trainset, self.testset = get_cifar()\n",
        "        self.train_sampler = self.test_sampler = None\n",
        "        if self.args.world_size > 1:\n",
        "            self.train_sampler = DistributedSampler(self.trainset, num_replicas=self.args.world_size, rank=self.rank)\n",
        "            self.test_sampler = DistributedSampler(self.testset, num_replicas=self.args.world_size, rank=self.rank)\n",
        "        dataloader_shared_kwargs = dict(batch_size=self.args.batch_size, num_workers=2, pin_memory=True)\n",
        "        self.train_loader = DataLoader(self.trainset, sampler=self.train_sampler, **dataloader_shared_kwargs)\n",
        "        self.test_loader = DataLoader(self.testset, sampler=self.test_sampler, **dataloader_shared_kwargs)\n",
        "        self.examples_seen = 0\n",
        "\n",
        "        if self.rank == 0:\n",
        "            wandb.init(project=self.args.wandb_project, name=self.args.wandb_name, config=self.args)\n",
        "\n",
        "    def training_step(self, imgs: Tensor, labels: Tensor) -> Tensor:\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Forward pass\n",
        "        imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
        "        logits = self.model(imgs)\n",
        "        t1 = time.time()\n",
        "\n",
        "        # Backward pass\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        t2 = time.time()\n",
        "\n",
        "        # Gradient sychronization\n",
        "        if self.args.world_size > 1:\n",
        "            for param in self.model.parameters():\n",
        "                all_reduce(param.grad, self.rank, self.args.world_size, op=\"mean\")\n",
        "                # dist.all_reduce(param.grad, op=dist.ReduceOp.SUM); param.grad /= self.args.world_size\n",
        "        t3 = time.time()\n",
        "\n",
        "        # Optimizer step, update examples seen & log data\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "        self.examples_seen += imgs.shape[0] * self.args.world_size\n",
        "        if self.rank == 0:\n",
        "            wandb.log(\n",
        "                {\"loss\": loss.item(), \"fwd_time\": (t1 - t0), \"bwd_time\": (t2 - t1), \"dist_time\": (t3 - t2)},\n",
        "                step=self.examples_seen,\n",
        "            )\n",
        "        return loss\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def evaluate(self) -> float:\n",
        "        self.model.eval()\n",
        "        total_correct, total_samples = 0, 0\n",
        "\n",
        "        for imgs, labels in tqdm(self.test_loader, desc=\"Evaluating\", disable=self.rank != 0):\n",
        "            imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
        "            logits = self.model(imgs)\n",
        "            total_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
        "            total_samples += len(imgs)\n",
        "\n",
        "        # Turn total_correct & total_samples into a tensor, so we can use all_reduce to sum them across processes\n",
        "        tensor = t.tensor([total_correct, total_samples], device=self.device)\n",
        "        all_reduce(tensor, self.rank, self.args.world_size, op=\"sum\")\n",
        "        total_correct, total_samples = tensor.tolist()\n",
        "\n",
        "        accuracy = total_correct / total_samples\n",
        "        if self.rank == 0:\n",
        "            wandb.log({\"accuracy\": accuracy}, step=self.examples_seen)\n",
        "        return accuracy\n",
        "\n",
        "    def train(self):\n",
        "        self.pre_training_setup()\n",
        "\n",
        "        accuracy = self.evaluate()  # our evaluate method is the same as parent class\n",
        "\n",
        "        for epoch in range(self.args.epochs):\n",
        "            t0 = time.time()\n",
        "\n",
        "            if self.args.world_size > 1:\n",
        "                self.train_sampler.set_epoch(epoch)\n",
        "                self.test_sampler.set_epoch(epoch)\n",
        "\n",
        "            self.model.train()\n",
        "\n",
        "            pbar = tqdm(self.train_loader, desc=\"Training\", disable=self.rank != 0)\n",
        "            for imgs, labels in pbar:\n",
        "                loss = self.training_step(imgs, labels)\n",
        "                pbar.set_postfix(loss=f\"{loss:.3f}\", ex_seen=f\"{self.examples_seen=:06}\")\n",
        "\n",
        "            accuracy = self.evaluate()\n",
        "\n",
        "            if self.rank == 0:\n",
        "                wandb.log({\"epoch_duration\": time.time() - t0}, step=self.examples_seen)\n",
        "                pbar.set_postfix(loss=f\"{loss:.3f}\", accuracy=f\"{accuracy:.3f}\", ex_seen=f\"{self.examples_seen=:06}\")\n",
        "\n",
        "        if self.rank == 0:\n",
        "            wandb.finish()\n",
        "            t.save(self.model.state_dict(), f\"resnet_{self.rank}.pth\")\n",
        "\n",
        "\n",
        "def dist_train_resnet_from_scratch(rank, world_size):\n",
        "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
        "    args = DistResNetTrainingArgs(world_size=world_size)\n",
        "    trainer = DistResNetTrainer(args, rank)\n",
        "    trainer.train()\n",
        "    dist.destroy_process_group()\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWuyns1MIT2p"
      },
      "source": [
        "## Bonus - DDP\n",
        "\n",
        "In practice, the most convenient way to use DDP is to wrap your model in `torch.nn.parallel.DistributedDataParallel`, which removes the need for explicitly calling `broadcast` at the start and `all_reduce` at the end of each training step. When you define a model in this way, it will automatically broadcast its weights to all processes, and the gradients will sync after each `loss.backward()` call. Here's the example `SimpleModel` code from above, rewritten to use these features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnPC7FzxIT2q"
      },
      "outputs": [],
      "source": [
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "\n",
        "def run(rank: int, world_size: int):\n",
        "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "    device = t.device(f\"cuda:{rank}\")\n",
        "    model = DDP(SimpleModel().to(device), device_ids=[rank])  # Wrap the model with DDP\n",
        "    optimizer = t.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "    input = t.tensor([rank], dtype=t.float32, device=device)\n",
        "    output = model(input)\n",
        "    loss = output.pow(2).sum()\n",
        "    loss.backward()  # DDP handles gradient synchronization\n",
        "\n",
        "    optimizer.step()\n",
        "    print(f\"Rank {rank}, new param: {model.module.param.data}\")\n",
        "\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    world_size = 2\n",
        "    mp.spawn(run, args=(world_size,), nprocs=world_size, join=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OwJRtgTIT2r"
      },
      "source": [
        "Can you use these features to rewrite your ResNet training code? Can you compare it to the code you wrote and see how much faster the built-in DDP version is? Note, you won't be able to separate the time taken for backward passes and gradient synchronization since these happen in the same line, but you can assume that the time taken for the backward pass is approximately unchanged and so any speedup you see is due to the better gradient synchronization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSt9lbIkIT2s"
      },
      "source": [
        "## Bonus - ring operations\n",
        "\n",
        "Our all reduce operation would scale quite badly when we have a large number of models. It chooses a single process as the source process to receive then send out all data, and so this process risks becoming a bottleneck. One of the most popular alternatives is **ring all-reduce**. Broadly speaking, ring-based algorithms work by sending data in a cyclic pattern (i.e. worker `n` sends it to worker `n+1 % N` where `N` is the total number of workers). After each sending round, we perform a reduction operation to the data that was just sent. [This blog post](https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/) illustrates the ring all-reduce algorithm for the sum operation.\n",
        "\n",
        "Can you implement the ring all-reduce algorithm by filling in the function below & passing tests? Once you've implemented it, you can compare the speed of your ring all-reduce vs the all-reduce we implemented earlier - is it faster? Do you expect it to be faster in this particular case?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFQKcEs7IT2s"
      },
      "outputs": [],
      "source": [
        "def ring_all_reduce(tensor: Tensor, rank, world_size, op: Literal[\"sum\", \"mean\"] = \"sum\") -> None:\n",
        "    \"\"\"\n",
        "    Ring all_reduce implementation using non-blocking send/recv to avoid deadlock.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    tests.test_all_reduce(ring_all_reduce)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt_HkzfOIT2t"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def ring_all_reduce(tensor: Tensor, rank, world_size, op: Literal[\"sum\", \"mean\"] = \"sum\") -> None:\n",
        "    \"\"\"\n",
        "    Ring all_reduce implementation using non-blocking send/recv to avoid deadlock.\n",
        "    \"\"\"\n",
        "    # Clone the tensor as the \"send_chunk\" for initial accumulation\n",
        "    send_chunk = tensor.clone()\n",
        "\n",
        "    # Step 1: Reduce-Scatter phase\n",
        "    for _ in range(world_size - 1):\n",
        "        # Compute the ranks involved in this round of sending/receiving\n",
        "        send_to = (rank + 1) % world_size\n",
        "        recv_from = (rank - 1 + world_size) % world_size\n",
        "\n",
        "        # Prepare a buffer for the received chunk\n",
        "        recv_chunk = t.zeros_like(send_chunk)\n",
        "\n",
        "        # Non-blocking send and receive\n",
        "        send_req = dist.isend(send_chunk, dst=send_to)\n",
        "        recv_req = dist.irecv(recv_chunk, src=recv_from)\n",
        "        send_req.wait()\n",
        "        recv_req.wait()\n",
        "\n",
        "        # Accumulate the received chunk into the tensor\n",
        "        tensor += recv_chunk\n",
        "\n",
        "        # Update send_chunk for the next iteration\n",
        "        send_chunk = recv_chunk\n",
        "\n",
        "    # Step 2: All-Gather phase\n",
        "    send_chunk = tensor.clone()\n",
        "    for _ in range(world_size - 1):\n",
        "        # Compute the ranks involved in this round of sending/receiving\n",
        "        send_to = (rank + 1) % world_size\n",
        "        recv_from = (rank - 1 + world_size) % world_size\n",
        "\n",
        "        # Prepare a buffer for the received chunk\n",
        "        recv_chunk = t.zeros_like(send_chunk)\n",
        "\n",
        "        # Non-blocking send and receive, and wait for completion\n",
        "        send_req = dist.isend(send_chunk, dst=send_to)\n",
        "        recv_req = dist.irecv(recv_chunk, src=recv_from)\n",
        "        send_req.wait()\n",
        "        recv_req.wait()\n",
        "\n",
        "        # Update the tensor with received data\n",
        "        tensor.copy_(recv_chunk)\n",
        "\n",
        "        # Update send_chunk for the next iteration\n",
        "        send_chunk = recv_chunk\n",
        "\n",
        "    # Step 3: Average the final result\n",
        "    if op == \"mean\":\n",
        "        tensor /= world_size\n",
        "```\n",
        "\n",
        "We should expect this algorithm to be better when we scale up the number of GPUs, but it won't always be faster in small-world settings like ours, because the naive allreduce algorithm requires fewer individual communication steps and this could outweigh the benefits brought by the ring-based allreduce.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xnye7FxIT2u"
      },
      "source": [
        "# ☆ Bonus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrVxchrDIT2u"
      },
      "source": [
        "Congratulations for getting to the end of the main content! This section gives some suggestions for more features of Weights and Biases to explore, or some other experiments you can run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mo0H_pUIT2v"
      },
      "source": [
        "## Scaling Laws"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "631udIwrIT2v"
      },
      "source": [
        "These bonus exercises are taken directly from Jacob Hilton's [online deep learning curriculum](https://github.com/jacobhilton/deep_learning_curriculum/blob/master/2-Scaling-Laws.md) (which is what the original version of the ARENA course was based on).\n",
        "\n",
        "First, you can start by reading the [Chinchilla paper](https://arxiv.org/abs/2203.15556). This is a correction to the original scaling laws paper: parameter count scales linearly with token budget for compute-optimal models, not ~quadratically. The difference comes from using a separately-tuned learning rate schedule for each token budget, rather than using a single training run to measure performance for every token budget. This highlights the importance of hyperparameter tuning for measuring scaling law exponents.\n",
        "\n",
        "You don't have to read the entire paper, just skim the graphs. Don't worry if they don't all make sense yet (it will be more illuminating when we study LLMs next week). Note that, although it specifically applies to language models, the key underlying ideas of tradeoffs between optimal dataset size and model size are generally applicable.\n",
        "\n",
        "### Suggested exercise\n",
        "\n",
        "Perform your own study of scaling laws for MNIST.\n",
        "\n",
        "- Write a script to train a small CNN on MNIST, or find one you have written previously.\n",
        "- Training for a single epoch only, vary the model size and dataset size. For the model size, multiply the width by powers of sqrt(2) (rounding if necessary - the idea is to vary the amount of compute used per forward pass by powers of 2). For the dataset size, multiply the fraction of the full dataset used by powers of 2 (i.e. 1, 1/2, 1/4, ...). To reduce noise, use a few random seeds and always use the full validation set.\n",
        "- The learning rate will need to vary with model size. Either tune it carefully for each model size, or use the rule of thumb that for Adam, the learning rate should be proportional to the initialization scale, i.e. `1/sqrt(fan_in)` for the standard Kaiming He initialization (which is what PyTorch generally uses by default).\n",
        "    - Note - `fan_in` refers to the variable $N_{in}$, which is `in_features` for a linear layer, and `in_channels * kernel_size * kernel_size` for a convolutional layer - in other words, the number of input parameters/activations we take a sumproduct over to get each output activation.\n",
        "- Plot the amount of compute used (on a log scale) against validation loss. The compute-efficient frontier should follow an approximate power law (straight line on a log scale).\n",
        "How does validation accuracy behave?\n",
        "- Study how the compute-efficient model size varies with compute. This should also follow an approximate power law. Try to estimate its exponent.\n",
        "- Repeat your entire experiment with 20% [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) to see how this affects the scaling exponents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtWmwcYjIT2w"
      },
      "source": [
        "## Other WandB features\n",
        "\n",
        "Here are a few more Weights & Biases features you might also want to play around with:\n",
        "\n",
        "* [Logging media and objects in experiments](https://docs.wandb.ai/guides/track/log?fbclid=IwAR3NxKsGpEjZwq3vSwYkohZllMpBwxHgOCc_k0ByuD9XGUsi_Scf5ELvGsQ) - you'll be doing this during the RL week, and it's useful when you're training generative image models like VAEs and diffusion models.\n",
        "* [Code saving](https://docs.wandb.ai/guides/app/features/panels/code?fbclid=IwAR2BkaXbRf7cqEH8kc1VzqH_kOJWGxqjUb_JCBq_SCnXOx1oF-Rt-hHydb4) - this captures all python source code files in the current director and all subdirectories. It's great for reproducibility, and also for sharing your code with others.\n",
        "* [Saving and loading PyTorch models](https://wandb.ai/wandb/common-ml-errors/reports/How-to-Save-and-Load-Models-in-PyTorch--VmlldzozMjg0MTE?fbclid=IwAR1Y9MzFTxIiVBJG06b4ppitwKWR4H5_ncKyT2F_rR5Z_IHawmpBTKskPcQ) - you can do this easily using `torch.save`, but it's also possible to do this directly through Weights and Biases as an **artifact**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poJyY2JRIT2w"
      },
      "source": [
        "## The Optimizer's Curse\n",
        "\n",
        "The [optimizer's curse](https://www.lesswrong.com/posts/5gQLrJr2yhPzMCcni/the-optimizer-s-curse-and-how-to-beat-it) applies to tuning hyperparameters. The main take-aways are:\n",
        "\n",
        "- You can expect your best hyperparameter combination to actually underperform in the future. You chose it because it was the best on some metric, but that metric has an element of noise/luck, and the more combinations you test the larger this effect is.\n",
        "- Look at the overall trends and correlations in context and try to make sense of the values you're seeing. Just because you ran a long search process doesn't mean your best output is really the best.\n",
        "\n",
        "For more on this, see [Preventing \"Overfitting\" of Cross-Validation Data](https://ai.stanford.edu/~ang/papers/cv-final.pdf) by Andrew Ng."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}